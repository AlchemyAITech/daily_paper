|     | paper_title                                                                                                                                          | paper_first_author             | publish_time   | categories                              | paper_summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | paper_url                         |
|----:|:-----------------------------------------------------------------------------------------------------------------------------------------------------|:-------------------------------|:---------------|:----------------------------------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------------------|
|   0 | MMVP: Motion-Matrix-based Video Prediction                                                                                                           | Yiqi Zhong                     | 2023-08-30     | cs.CV                                   | A central challenge of video prediction lies where the system has to reasonthe objects' future motions from image frames while simultaneously maintainingthe consistency of their appearances across frames. This work introduces anend-to-end trainable two-stream video prediction framework, Motion-Matrix-basedVideo Prediction (MMVP), to tackle this challenge. Unlike previous methods thatusually handle motion prediction and appearance maintenance within the same setof modules, MMVP decouples motion and appearance information by constructingappearance-agnostic motion matrices. The motion matrices represent the temporalsimilarity of each and every pair of feature patches in the input frames, andare the sole input of the motion prediction module in MMVP. This designimproves video prediction in both accuracy and efficiency, and reduces themodel size. Results of extensive experiments demonstrate that MMVP outperformsstate-of-the-art systems on public data sets by non-negligible large margins(about 1 db in PSNR, UCF Sports) in significantly smaller model sizes (84% thesize or smaller). Please refer tohttps://github.com/Kay1794/MMVP-motion-matrix-based-video-prediction for theofficial code and the datasets used in this paper.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.16154v1 |
|   1 | Learned Image Reasoning Prior Penetrates Deep Unfolding Network for Panchromatic and Multi-Spectral Image Fusion                                     | Man Zhou                       | 2023-08-30     | cs.CV, eess.IV                          | The success of deep neural networks for pan-sharpening is commonly in a formof black box, lacking transparency and interpretability. To alleviate thisissue, we propose a novel model-driven deep unfolding framework with imagereasoning prior tailored for the pan-sharpening task. Different from existingunfolding solutions that deliver the proximal operator networks as theuncertain and vague priors, our framework is motivated by the content reasoningability of masked autoencoders (MAE) with insightful designs. Specifically, thepre-trained MAE with spatial masking strategy, acting as intrinsic reasoningprior, is embedded into unfolding architecture. Meanwhile, the pre-trained MAEwith spatial-spectral masking strategy is treated as the regularization termwithin loss function to constrain the spatial-spectral consistency. Suchdesigns penetrate the image reasoning prior into deep unfolding networks whileimproving its interpretability and representation capability. The uniqueness ofour framework is that the holistic learning process is explicitly integratedwith the inherent physical mechanism underlying the pan-sharpening task.Extensive experiments on multiple satellite datasets demonstrate thesuperiority of our method over the existing state-of-the-art approaches. Codewill be released at \url{https://manman1995.github.io/}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.16083v1 |
|   2 | Fusing Pseudo Labels with Weak Supervision for Dynamic Traffic Scenarios                                                                             | Harshith Mohan Kumar           | 2023-08-30     | cs.CV                                   | Advanced Driver Assistance Systems (ADAS) have made significant strides,capitalizing on computer vision to enhance perception and decision-makingcapabilities. Nonetheless, the adaptation of these systems to diverse trafficscenarios poses challenges due to shifts in data distribution stemming fromfactors such as location, weather, and road infrastructure. To tackle this, weintroduce a weakly-supervised label unification pipeline that amalgamatespseudo labels from a multitude of object detection models trained onheterogeneous datasets. Our pipeline engenders a unified label space throughthe amalgamation of labels from disparate datasets, rectifying bias andenhancing generalization. We fine-tune multiple object detection models onindividual datasets, subsequently crafting a unified dataset featuring pseudolabels, meticulously validated for precision. Following this, we retrain asolitary object detection model using the merged label space, culminating in aresilient model proficient in dynamic traffic scenarios. We put forth acomprehensive evaluation of our approach, employing diverse datasetsoriginating from varied Asian countries, effectively demonstrating its efficacyin challenging road conditions. Notably, our method yields substantialenhancements in object detection performance, culminating in a model withheightened resistance against domain shifts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.15960v1 |
|   3 | Introducing Language Guidance in Prompt-based Continual Learning                                                                                     | Muhammad Gul Zain Ali Khan     | 2023-08-30     | cs.CV                                   | Continual Learning aims to learn a single model on a sequence of taskswithout having access to data from previous tasks. The biggest challenge in thedomain still remains catastrophic forgetting: a loss in performance on seenclasses of earlier tasks. Some existing methods rely on an expensive replaybuffer to store a chunk of data from previous tasks. This, while promising,becomes expensive when the number of tasks becomes large or data can not bestored for privacy reasons. As an alternative, prompt-based methods have beenproposed that store the task information in a learnable prompt pool. Thisprompt pool instructs a frozen image encoder on how to solve each task. Whilethe model faces a disjoint set of classes in each task in this setting, weargue that these classes can be encoded to the same embedding space of apre-trained language encoder. In this work, we propose Language Guidance forPrompt-based Continual Learning (LGCL) as a plug-in for prompt-based methods.LGCL is model agnostic and introduces language guidance at the task level inthe prompt pool and at the class level on the output feature of the visionencoder. We show with extensive experimentation that LGCL consistently improvesthe performance of prompt-based continual learning methods to set a newstate-of-the art. LGCL achieves these performance improvements without needingany additional learnable parameters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.15827v1 |
|   4 | Canonical Factors for Hybrid Neural Fields                                                                                                           | Brent Yi                       | 2023-08-29     | cs.CV, cs.LG, math.OC                   | Factored feature volumes offer a simple way to build more compact, efficient,and intepretable neural fields, but also introduce biases that are notnecessarily beneficial for real-world data. In this work, we (1) characterizethe undesirable biases that these architectures have for axis-aligned signals-- they can lead to radiance field reconstruction differences of as high as 2PSNR -- and (2) explore how learning a set of canonicalizing transformationscan improve representations by removing these biases. We prove in atwo-dimensional model problem that simultaneously learning thesetransformations together with scene appearance succeeds with drasticallyimproved efficiency. We validate the resulting architectures, which we callTILTED, using image, signed distance, and radiance field reconstruction tasks,where we observe improvements across quality, robustness, compactness, andruntime. Results demonstrate that TILTED can enable capabilities comparable tobaselines that are 2x larger, while highlighting weaknesses of neural fieldevaluation procedures.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.15461v1 |
|   5 | Shatter and Gather: Learning Referring Image Segmentation with Text Supervision                                                                      | Dongwon Kim                    | 2023-08-29     | cs.CV                                   | Referring image segmentation, the task of segmenting any arbitrary entitiesdescribed in free-form texts, opens up a variety of vision applications.However, manual labeling of training data for this task is prohibitivelycostly, leading to lack of labeled data for training. We address this issue bya weakly supervised learning approach using text descriptions of trainingimages as the only source of supervision. To this end, we first present a newmodel that discovers semantic entities in input image and then combines suchentities relevant to text query to predict the mask of the referent. We alsopresent a new loss function that allows the model to be trained without anyfurther supervision. Our method was evaluated on four public benchmarks forreferring image segmentation, where it clearly outperformed the existing methodfor the same task and recent open-vocabulary segmentation models on all thebenchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.15512v1 |
|   6 | Efficient Model Personalization in Federated Learning via Client-Specific Prompt Generation                                                          | Fu-En Yang                     | 2023-08-29     | cs.CV, cs.AI, cs.LG                     | Federated learning (FL) emerges as a decentralized learning framework whichtrains models from multiple distributed clients without sharing their data topreserve privacy. Recently, large-scale pre-trained models (e.g., VisionTransformer) have shown a strong capability of deriving robust representations.However, the data heterogeneity among clients, the limited computationresources, and the communication bandwidth restrict the deployment oflarge-scale models in FL frameworks. To leverage robust representations fromlarge-scale models while enabling efficient model personalization forheterogeneous clients, we propose a novel personalized FL framework ofclient-specific Prompt Generation (pFedPG), which learns to deploy apersonalized prompt generator at the server for producing client-specificvisual prompts that efficiently adapts frozen backbones to local datadistributions. Our proposed framework jointly optimizes the stages ofpersonalized prompt adaptation locally and personalized prompt generationglobally. The former aims to train visual prompts that adapt foundation modelsto each client, while the latter observes local optimization directions togenerate personalized prompts for all clients. Through extensive experiments onbenchmark datasets, we show that our pFedPG is favorable againststate-of-the-art personalized FL methods under various types of dataheterogeneity, allowing computation and communication efficient modelpersonalization.                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.15367v1 |
|   7 | CLIPTrans: Transferring Visual Knowledge with Pre-trained Models for Multimodal Machine Translation                                                  | Devaansh Gupta                 | 2023-08-29     | cs.CV, cs.AI, cs.CL                     | There has been a growing interest in developing multimodal machinetranslation (MMT) systems that enhance neural machine translation (NMT) withvisual knowledge. This problem setup involves using images as auxiliaryinformation during training, and more recently, eliminating their use duringinference. Towards this end, previous works face a challenge in trainingpowerful MMT models from scratch due to the scarcity of annotated multilingualvision-language data, especially for low-resource languages. Simultaneously,there has been an influx of multilingual pre-trained models for NMT andmultimodal pre-trained models for vision-language tasks, primarily in English,which have shown exceptional generalisation ability. However, these are notdirectly applicable to MMT since they do not provide aligned multimodalmultilingual features for generative tasks. To alleviate this issue, instead ofdesigning complex modules for MMT, we propose CLIPTrans, which simply adaptsthe independently pre-trained multimodal M-CLIP and the multilingual mBART. Inorder to align their embedding spaces, mBART is conditioned on the M-CLIPfeatures by a prefix sequence generated through a lightweight mapping network.We train this in a two-stage pipeline which warms up the model with imagecaptioning before the actual translation task. Through experiments, wedemonstrate the merits of this framework and consequently push forward thestate-of-the-art across standard benchmarks by an average of +2.67 BLEU. Thecode can be found at www.github.com/devaansh100/CLIPTrans.                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.15226v1 |
|   8 | Learning to Upsample by Learning to Sample                                                                                                           | Wenze Liu                      | 2023-08-29     | cs.CV                                   | We present DySample, an ultra-lightweight and effective dynamic upsampler.While impressive performance gains have been witnessed from recent kernel-baseddynamic upsamplers such as CARAFE, FADE, and SAPA, they introduce muchworkload, mostly due to the time-consuming dynamic convolution and theadditional sub-network used to generate dynamic kernels. Further, the need forhigh-res feature guidance of FADE and SAPA somehow limits their applicationscenarios. To address these concerns, we bypass dynamic convolution andformulate upsampling from the perspective of point sampling, which is moreresource-efficient and can be easily implemented with the standard built-infunction in PyTorch. We first showcase a naive design, and then demonstrate howto strengthen its upsampling behavior step by step towards our new upsampler,DySample. Compared with former kernel-based dynamic upsamplers, DySamplerequires no customized CUDA package and has much fewer parameters, FLOPs, GPUmemory, and latency. Besides the light-weight characteristics, DySampleoutperforms other upsamplers across five dense prediction tasks, includingsemantic segmentation, object detection, instance segmentation, panopticsegmentation, and monocular depth estimation. Code is available athttps://github.com/tiny-smart/dysample.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.15085v1 |
|   9 | Class Prior-Free Positive-Unlabeled Learning with Taylor Variational Loss for Hyperspectral Remote Sensing Imagery                                   | Hengwei Zhao                   | 2023-08-29     | cs.CV                                   | Positive-unlabeled learning (PU learning) in hyperspectral remote sensingimagery (HSI) is aimed at learning a binary classifier from positive andunlabeled data, which has broad prospects in various earth vision applications.However, when PU learning meets limited labeled HSI, the unlabeled data maydominate the optimization process, which makes the neural networks overfit theunlabeled data. In this paper, a Taylor variational loss is proposed for HSI PUlearning, which reduces the weight of the gradient of the unlabeled data byTaylor series expansion to enable the network to find a balance betweenoverfitting and underfitting. In addition, the self-calibrated optimizationstrategy is designed to stabilize the training process. Experiments on 7benchmark datasets (21 tasks in total) validate the effectiveness of theproposed method. Code is at: https://github.com/Hengwei-Zhao96/T-HOneCls.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.15081v1 |
|  10 | Exploring Model Transferability through the Lens of Potential Energy                                                                                 | Xiaotong Li                    | 2023-08-29     | cs.CV, cs.LG                            | Transfer learning has become crucial in computer vision tasks due to the vastavailability of pre-trained deep learning models. However, selecting theoptimal pre-trained model from a diverse pool for a specific downstream taskremains a challenge. Existing methods for measuring the transferability ofpre-trained models rely on statistical correlations between encoded staticfeatures and task labels, but they overlook the impact of underlyingrepresentation dynamics during fine-tuning, leading to unreliable results,especially for self-supervised models. In this paper, we present an insightfulphysics-inspired approach named PED to address these challenges. We reframe thechallenge of model selection through the lens of potential energy and directlymodel the interaction forces that influence fine-tuning dynamics. By capturingthe motion of dynamic representations to decline the potential energy within aforce-driven physical model, we can acquire an enhanced and more stableobservation for estimating transferability. The experimental results on 10downstream tasks and 12 self-supervised models demonstrate that our approachcan seamlessly integrate into existing ranking techniques and enhance theirperformances, revealing its effectiveness for the model selection task and itspotential for understanding the mechanism in transfer learning. Code will beavailable at https://github.com/lixiaotong97/PED.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.15074v1 |
|  11 | NSF: Neural Surface Fields for Human Modeling from Monocular Depth                                                                                   | Yuxuan Xue                     | 2023-08-28     | cs.CV                                   | Obtaining personalized 3D animatable avatars from a monocular camera hasseveral real world applications in gaming, virtual try-on, animation, andVR/XR, etc. However, it is very challenging to model dynamic and fine-grainedclothing deformations from such sparse data. Existing methods for modeling 3Dhumans from depth data have limitations in terms of computational efficiency,mesh coherency, and flexibility in resolution and topology. For instance,reconstructing shapes using implicit functions and extracting explicit meshesper frame is computationally expensive and cannot ensure coherent meshes acrossframes. Moreover, predicting per-vertex deformations on a pre-designed humantemplate with a discrete surface lacks flexibility in resolution and topology.To overcome these limitations, we propose a novel method `\keyfeature: NeuralSurface Fields' for modeling 3D clothed humans from monocular depth. NSFdefines a neural field solely on the base surface which models a continuous andflexible displacement field. NSF can be adapted to the base surface withdifferent resolution and topology without retraining at inference time.Compared to existing approaches, our method eliminates the expensive per-framesurface extraction while maintaining mesh coherency, and is capable ofreconstructing meshes with arbitrary resolution without retraining. To fosterresearch in this direction, we release our code in project page at:https://yuxuan-xue.com/nsf.                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.14847v2 |
|  12 | CLNeRF: Continual Learning Meets NeRF                                                                                                                | Zhipeng Cai                    | 2023-08-28     | cs.CV                                   | Novel view synthesis aims to render unseen views given a set of calibratedimages. In practical applications, the coverage, appearance or geometry of thescene may change over time, with new images continuously being captured.Efficiently incorporating such continuous change is an open challenge. StandardNeRF benchmarks only involve scene coverage expansion. To study other practicalscene changes, we propose a new dataset, World Across Time (WAT), consisting ofscenes that change in appearance and geometry over time. We also propose asimple yet effective method, CLNeRF, which introduces continual learning (CL)to Neural Radiance Fields (NeRFs). CLNeRF combines generative replay and theInstant Neural Graphics Primitives (NGP) architecture to effectively preventcatastrophic forgetting and efficiently update the model when new data arrives.We also add trainable appearance and geometry embeddings to NGP, allowing asingle compact model to handle complex scene changes. Without the need to storehistorical images, CLNeRF trained sequentially over multiple scans of achanging scene performs on-par with the upper bound model trained on all scansat once. Compared to other CL baselines CLNeRF performs much better acrossstandard benchmarks and WAT. The source code, and the WAT dataset are availableat https://github.com/IntelLabs/CLNeRF. Video presentation is available at:https://youtu.be/nLRt6OoDGq0?si=8yD6k-8MMBJInQPs                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.14816v1 |
|  13 | Efficient Discovery and Effective Evaluation of Visual Perceptual Similarity: A Benchmark and Beyond                                                 | Oren Barkan                    | 2023-08-28     | cs.CV, cs.LG                            | Visual similarities discovery (VSD) is an important task with broade-commerce applications. Given an image of a certain object, the goal of VSD isto retrieve images of different objects with high perceptual visual similarity.Although being a highly addressed problem, the evaluation of proposed methodsfor VSD is often based on a proxy of an identification-retrieval task,evaluating the ability of a model to retrieve different images of the sameobject. We posit that evaluating VSD methods based on identification tasks islimited, and faithful evaluation must rely on expert annotations. In thispaper, we introduce the first large-scale fashion visual similarity benchmarkdataset, consisting of more than 110K expert-annotated image pairs. Besidesthis major contribution, we share insight from the challenges we faced whilecurating this dataset. Based on these insights, we propose a novel andefficient labeling procedure that can be applied to any dataset. Our analysisexamines its limitations and inductive biases, and based on these findings, wepropose metrics to mitigate those limitations. Though our primary focus lies onvisual similarity, the methodologies we present have broader applications fordiscovering and evaluating perceptual similarity across various domains.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.14753v1 |
|  14 | R3D3: Dense 3D Reconstruction of Dynamic Scenes from Multiple Cameras                                                                                | Aron Schmied                   | 2023-08-28     | cs.CV                                   | Dense 3D reconstruction and ego-motion estimation are key challenges inautonomous driving and robotics. Compared to the complex, multi-modal systemsdeployed today, multi-camera systems provide a simpler, low-cost alternative.However, camera-based 3D reconstruction of complex dynamic scenes has provenextremely difficult, as existing solutions often produce incomplete orincoherent results. We propose R3D3, a multi-camera system for dense 3Dreconstruction and ego-motion estimation. Our approach iterates betweengeometric estimation that exploits spatial-temporal information from multiplecameras, and monocular depth refinement. We integrate multi-camera featurecorrelation and dense bundle adjustment operators that yield robust geometricdepth and pose estimates. To improve reconstruction where geometric depth isunreliable, e.g. for moving objects or low-textured regions, we introducelearnable scene priors via a depth refinement network. We show that this designenables a dense, consistent 3D reconstruction of challenging, dynamic outdoorenvironments. Consequently, we achieve state-of-the-art dense depth predictionon the DDAD and NuScenes benchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.14713v1 |
|  15 | S-TREK: Sequential Translation and Rotation Equivariant Keypoints for local feature extraction                                                       | Emanuele Santellani            | 2023-08-28     | cs.CV                                   | In this work we introduce S-TREK, a novel local feature extractor thatcombines a deep keypoint detector, which is both translation and rotationequivariant by design, with a lightweight deep descriptor extractor. We trainthe S-TREK keypoint detector within a framework inspired by reinforcementlearning, where we leverage a sequential procedure to maximize a rewarddirectly related to keypoint repeatability. Our descriptor network is trainedfollowing a "detect, then describe" approach, where the descriptor loss isevaluated only at those locations where keypoints have been selected by thealready trained detector. Extensive experiments on multiple benchmarks confirmthe effectiveness of our proposed method, with S-TREK often outperforming otherstate-of-the-art methods in terms of repeatability and quality of the recoveredposes, especially when dealing with in-plane rotations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.14598v1 |
|  16 | Referring Image Segmentation Using Text Supervision                                                                                                  | Fang Liu                       | 2023-08-28     | cs.CV                                   | Existing Referring Image Segmentation (RIS) methods typically requireexpensive pixel-level or box-level annotations for supervision. In this paper,we observe that the referring texts used in RIS already provide sufficientinformation to localize the target object. Hence, we propose a novelweakly-supervised RIS framework to formulate the target localization problem asa classification process to differentiate between positive and negative textexpressions. While the referring text expressions for an image are used aspositive expressions, the referring text expressions from other images can beused as negative expressions for this image. Our framework has three mainnovelties. First, we propose a bilateral prompt method to facilitate theclassification process, by harmonizing the domain discrepancy between visualand linguistic features. Second, we propose a calibration method to reducenoisy background information and improve the correctness of the response mapsfor target object localization. Third, we propose a positive response mapselection strategy to generate high-quality pseudo-labels from the enhancedresponse maps, for training a segmentation network for RIS inference. Forevaluation, we propose a new metric to measure localization accuracy.Experiments on four benchmarks show that our framework achieves promisingperformances to existing fully-supervised RIS methods while outperformingstate-of-the-art weakly-supervised methods adapted from related areas. Code isavailable at https://github.com/fawnliu/TRIS.                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.14575v1 |
|  17 | LAC -- Latent Action Composition for Skeleton-based Action Segmentation                                                                              | Di Yang                        | 2023-08-28     | cs.CV                                   | Skeleton-based action segmentation requires recognizing composable actions inuntrimmed videos. Current approaches decouple this problem by first extractinglocal visual features from skeleton sequences and then processing them by atemporal model to classify frame-wise actions. However, their performancesremain limited as the visual features cannot sufficiently express composableactions. In this context, we propose Latent Action Composition (LAC), a novelself-supervised framework aiming at learning from synthesized composablemotions for skeleton-based action segmentation. LAC is composed of a novelgeneration module towards synthesizing new sequences. Specifically, we design alinear latent space in the generator to represent primitive motion. Newcomposed motions can be synthesized by simply performing arithmetic operationson latent representations of multiple input skeleton sequences. LAC leveragessuch synthesized sequences, which have large diversity and complexity, forlearning visual representations of skeletons in both sequence and frame spacesvia contrastive learning. The resulting visual encoder has a high expressivepower and can be effectively transferred onto action segmentation tasks byend-to-end fine-tuning without the need for additional temporal models. Weconduct a study focusing on transfer-learning and we show that representationslearned from pre-trained LAC outperform the state-of-the-art by a large marginon TSU, Charades, PKU-MMD datasets.                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.14500v2 |
|  18 | Graph-based Asynchronous Event Processing for Rapid Object Recognition                                                                               | Yijin Li                       | 2023-08-28     | cs.CV                                   | Different from traditional video cameras, event cameras capture asynchronousevents stream in which each event encodes pixel location, trigger time, and thepolarity of the brightness changes. In this paper, we introduce a novelgraph-based framework for event cameras, namely SlideGCN. Unlike some recentgraph-based methods that use groups of events as input, our approach canefficiently process data event-by-event, unlock the low latency nature ofevents data while still maintaining the graph's structure internally. For fastgraph construction, we develop a radius search algorithm, which better exploitsthe partial regular structure of event cloud against k-d tree based genericmethods. Experiments show that our method reduces the computational complexityup to 100 times with respect to current graph-based methods while keepingstate-of-the-art performance on object recognition. Moreover, we verify thesuperiority of event-wise processing with our method. When the state becomesstable, we can give a prediction with high confidence, thus making an earlyrecognition. Project page: \url{https://zju3dv.github.io/slide_gcn/}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.14419v1 |
|  19 | Multi-Modal Neural Radiance Field for Monocular Dense SLAM with a Light-Weight ToF Sensor                                                            | Xinyang Liu                    | 2023-08-28     | cs.CV                                   | Light-weight time-of-flight (ToF) depth sensors are compact andcost-efficient, and thus widely used on mobile devices for tasks such asautofocus and obstacle detection. However, due to the sparse and noisy depthmeasurements, these sensors have rarely been considered for dense geometryreconstruction. In this work, we present the first dense SLAM system with amonocular camera and a light-weight ToF sensor. Specifically, we propose amulti-modal implicit scene representation that supports rendering both thesignals from the RGB camera and light-weight ToF sensor which drives theoptimization by comparing with the raw sensor inputs. Moreover, in order toguarantee successful pose tracking and reconstruction, we exploit a predicteddepth as an intermediate supervision and develop a coarse-to-fine optimizationstrategy for efficient learning of the implicit representation. At last, thetemporal information is explicitly exploited to deal with the noisy signalsfrom light-weight ToF sensors to improve the accuracy and robustness of thesystem. Experiments demonstrate that our system well exploits the signals oflight-weight ToF sensors and achieves competitive results both on cameratracking and dense scene reconstruction. Project page:\url{https://zju3dv.github.io/tof_slam/}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.14383v1 |
|  20 | Online Continual Learning on Hierarchical Label Expansion                                                                                            | Byung Hyun Lee                 | 2023-08-28     | cs.LG                                   | Continual learning (CL) enables models to adapt to new tasks and environmentswithout forgetting previously learned knowledge. While current CL setups haveignored the relationship between labels in the past task and the new task withor without small task overlaps, real-world scenarios often involve hierarchicalrelationships between old and new tasks, posing another challenge fortraditional CL approaches. To address this challenge, we propose a novelmulti-level hierarchical class incremental task configuration with an onlinelearning constraint, called hierarchical label expansion (HLE). Ourconfiguration allows a network to first learn coarse-grained classes, with datalabels continually expanding to more fine-grained classes in various hierarchydepths. To tackle this new setup, we propose a rehearsal-based method thatutilizes hierarchy-aware pseudo-labeling to incorporate hierarchical classinformation. Additionally, we propose a simple yet effective memory managementand sampling strategy that selectively adopts samples of newly encounteredclasses. Our experiments demonstrate that our proposed method can effectivelyuse hierarchy on our HLE setup to improve classification accuracy across alllevels of hierarchies, regardless of depth and class imbalance ratio,outperforming prior state-of-the-art works by significant margins while alsooutperforming them on the conventional disjoint, blurry and i-Blurry CL setups.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.14374v1 |
|  21 | HoloFusion: Towards Photo-realistic 3D Generative Modeling                                                                                           | Animesh Karnewar               | 2023-08-28     | cs.CV, cs.GR                            | Diffusion-based image generators can now produce high-quality and diversesamples, but their success has yet to fully translate to 3D generation:existing diffusion methods can either generate low-resolution but 3D consistentoutputs, or detailed 2D views of 3D objects but with potential structuraldefects and lacking view consistency or realism. We present HoloFusion, amethod that combines the best of these approaches to produce high-fidelity,plausible, and diverse 3D samples while learning from a collection ofmulti-view 2D images only. The method first generates coarse 3D samples using avariant of the recently proposed HoloDiffusion generator. Then, itindependently renders and upsamples a large number of views of the coarse 3Dmodel, super-resolves them to add detail, and distills those into a single,high-fidelity implicit 3D representation, which also ensures view consistencyof the final renders. The super-resolution network is trained as an integralpart of HoloFusion, end-to-end, and the final distillation uses a new samplingscheme to capture the space of super-resolved signals. We compare our methodagainst existing baselines, including DreamFusion, Get3D, EG3D, andHoloDiffusion, and achieve, to the best of our knowledge, the most realisticresults on the challenging CO3Dv2 dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.14244v1 |
|  22 | Towards Vision-Language Mechanistic Interpretability: A Causal Tracing Tool for BLIP                                                                 | Vedant Palit                   | 2023-08-27     | cs.CL, cs.AI, cs.CV                     | Mechanistic interpretability seeks to understand the neural mechanisms thatenable specific behaviors in Large Language Models (LLMs) by leveragingcausality-based methods. While these approaches have identified neural circuitsthat copy spans of text, capture factual knowledge, and more, they remainunusable for multimodal models since adapting these tools to thevision-language domain requires considerable architectural changes. In thiswork, we adapt a unimodal causal tracing tool to BLIP to enable the study ofthe neural mechanisms underlying image-conditioned text generation. Wedemonstrate our approach on a visual question answering dataset, highlightingthe causal relevance of later layer representations for all tokens.Furthermore, we release our BLIP causal tracing tool as open source to enablefurther experimentation in vision-language mechanistic interpretability by thecommunity. Our code is available athttps://github.com/vedantpalit/Towards-Vision-Language-Mechanistic-Interpretability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.14179v1 |
|  23 | Sparse Sampling Transformer with Uncertainty-Driven Ranking for Unified Removal of Raindrops and Rain Streaks                                        | Sixiang Chen                   | 2023-08-27     | cs.CV                                   | In the real world, image degradations caused by rain often exhibit acombination of rain streaks and raindrops, thereby increasing the challenges ofrecovering the underlying clean image. Note that the rain streaks and raindropshave diverse shapes, sizes, and locations in the captured image, and thusmodeling the correlation relationship between irregular degradations caused byrain artifacts is a necessary prerequisite for image deraining. This paper aimsto present an efficient and flexible mechanism to learn and model degradationrelationships in a global view, thereby achieving a unified removal ofintricate rain scenes. To do so, we propose a Sparse Sampling Transformer basedon Uncertainty-Driven Ranking, dubbed UDR-S2Former. Compared to previousmethods, our UDR-S2Former has three merits. First, it can adaptively samplerelevant image degradation information to model underlying degradationrelationships. Second, explicit application of the uncertainty-driven rankingstrategy can facilitate the network to attend to degradation features andunderstand the reconstruction process. Finally, experimental results show thatour UDR-S2Former clearly outperforms state-of-the-art methods for allbenchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.14153v1 |
|  24 | Unaligned 2D to 3D Translation with Conditional Vector-Quantized Code Diffusion using Transformers                                                   | Abril Corona-Figueroa          | 2023-08-27     | cs.CV                                   | Generating 3D images of complex objects conditionally from a few 2D views isa difficult synthesis problem, compounded by issues such as domain gap andgeometric misalignment. For instance, a unified framework such as GenerativeAdversarial Networks cannot achieve this unless they explicitly define both adomain-invariant and geometric-invariant joint latent distribution, whereasNeural Radiance Fields are generally unable to handle both issues as theyoptimize at the pixel level. By contrast, we propose a simple and novel 2D to3D synthesis approach based on conditional diffusion with vector-quantizedcodes. Operating in an information-rich code space enables high-resolution 3Dsynthesis via full-coverage attention across the views. Specifically, wegenerate the 3D codes (e.g. for CT images) conditional on previously generated3D codes and the entire codebook of two 2D views (e.g. 2D X-rays). Qualitativeand quantitative results demonstrate state-of-the-art performance overspecialized methods across varied evaluation criteria, including fidelitymetrics such as density, coverage, and distortion metrics for two complexvolumetric imagery datasets from in real-world scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.14152v1 |
|  25 | Multi-model fusion for Aerial Vision and Dialog Navigation based on human attention aids                                                             | Xinyi Wang                     | 2023-08-27     | cs.CV                                   | Drones have been widely used in many areas of our daily lives. It relievespeople of the burden of holding a controller all the time and makes dronecontrol easier to use for people with disabilities or occupied hands. However,the control of aerial robots is more complicated compared to normal robots dueto factors such as uncontrollable height. Therefore, it is crucial to developan intelligent UAV that has the ability to talk to humans and follow naturallanguage commands. In this report, we present an aerial navigation task for the2023 ICCV Conversation History. Based on the AVDN dataset containing more than3k recorded navigation trajectories and asynchronous human-robot conversations,we propose an effective method of fusion training of Human Attention AidedTransformer model (HAA-Transformer) and Human Attention Aided LSTM (HAA-LSTM)model, which achieves the prediction of the navigation routing points and humanattention. The method not only achieves high SR and SPL metrics, but also showsa 7% improvement in GP metrics compared to the baseline model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.14064v1 |
|  26 | Hierarchical Contrastive Learning for Pattern-Generalizable Image Corruption Detection                                                               | Xin Feng                       | 2023-08-27     | cs.CV                                   | Effective image restoration with large-size corruptions, such as blind imageinpainting, entails precise detection of corruption region masks which remainsextremely challenging due to diverse shapes and patterns of corruptions. Inthis work, we present a novel method for automatic corruption detection, whichallows for blind corruption restoration without known corruption masks.Specifically, we develop a hierarchical contrastive learning framework todetect corrupted regions by capturing the intrinsic semantic distinctionsbetween corrupted and uncorrupted regions. In particular, our model detects thecorrupted mask in a coarse-to-fine manner by first predicting a coarse mask bycontrastive learning in low-resolution feature space and then refines theuncertain area of the mask by high-resolution contrastive learning. Aspecialized hierarchical interaction mechanism is designed to facilitate theknowledge propagation of contrastive learning in different scales, boosting themodeling performance substantially. The detected multi-scale corruption masksare then leveraged to guide the corruption restoration. Detecting corruptedregions by learning the contrastive distinctions rather than the semanticpatterns of corruptions, our model has well generalization ability acrossdifferent corruption patterns. Extensive experiments demonstrate followingmerits of our model: 1) the superior performance over other methods on bothcorruption detection and various image restoration tasks including blindinpainting and watermark removal, and 2) strong generalization across differentcorruption patterns such as graffiti, random noise or other image content.Codes and trained weights are available at https://github.com/xyfJASON/HCL .                                                                                                                                                                | http://arxiv.org/abs/2308.14061v1 |
|  27 | MB-TaylorFormer: Multi-branch Efficient Transformer Expanded by Taylor Formula for Image Dehazing                                                    | Yuwei Qiu                      | 2023-08-27     | cs.CV                                   | In recent years, Transformer networks are beginning to replace pureconvolutional neural networks (CNNs) in the field of computer vision due totheir global receptive field and adaptability to input. However, the quadraticcomputational complexity of softmax-attention limits the wide application inimage dehazing task, especially for high-resolution images. To address thisissue, we propose a new Transformer variant, which applies the Taylor expansionto approximate the softmax-attention and achieves linear computationalcomplexity. A multi-scale attention refinement module is proposed as acomplement to correct the error of the Taylor expansion. Furthermore, weintroduce a multi-branch architecture with multi-scale patch embedding to theproposed Transformer, which embeds features by overlapping deformableconvolution of different scales. The design of multi-scale patch embedding isbased on three key ideas: 1) various sizes of the receptive field; 2)multi-level semantic information; 3) flexible shapes of the receptive field.Our model, named Multi-branch Transformer expanded by Taylor formula(MB-TaylorFormer), can embed coarse to fine features more flexibly at the patchembedding stage and capture long-distance pixel interactions with limitedcomputational cost. Experimental results on several dehazing benchmarks showthat MB-TaylorFormer achieves state-of-the-art (SOTA) performance with a lightcomputational burden. The source code and pre-trained models are available athttps://github.com/FVL2020/ICCV-2023-MB-TaylorFormer.                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.14036v2 |
|  28 | Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation                                                                           | Sunandini Sanyal               | 2023-08-27     | cs.CV                                   | Conventional Domain Adaptation (DA) methods aim to learn domain-invariantfeature representations to improve the target adaptation performance. However,we motivate that domain-specificity is equally important since in-domaintrained models hold crucial domain-specific properties that are beneficial foradaptation. Hence, we propose to build a framework that supportsdisentanglement and learning of domain-specific factors and task-specificfactors in a unified model. Motivated by the success of vision transformers inseveral multi-modal vision problems, we find that queries could be leveraged toextract the domain-specific factors. Hence, we propose a novelDomain-specificity-inducing Transformer (DSiT) framework for disentangling andlearning both domain-specific and task-specific factors. To achievedisentanglement, we propose to construct novel Domain-Representative Inputs(DRI) with domain-specific information to train a domain classifier with anovel domain token. We are the first to utilize vision transformers for domainadaptation in a privacy-oriented source-free setting, and our approach achievesstate-of-the-art performance on single-source, multi-source, and multi-targetbenchmarks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.14023v1 |
|  29 | Calibrating Panoramic Depth Estimation for Practical Localization and Mapping                                                                        | Junho Kim                      | 2023-08-27     | cs.CV                                   | The absolute depth values of surrounding environments provide crucial cuesfor various assistive technologies, such as localization, navigation, and 3Dstructure estimation. We propose that accurate depth estimated from panoramicimages can serve as a powerful and light-weight input for a wide range ofdownstream tasks requiring 3D information. While panoramic images can easilycapture the surrounding context from commodity devices, the estimated depthshares the limitations of conventional image-based depth estimation; theperformance deteriorates under large domain shifts and the absolute values arestill ambiguous to infer from 2D observations. By taking advantage of theholistic view, we mitigate such effects in a self-supervised way and fine-tunethe network with geometric consistency during the test phase. Specifically, weconstruct a 3D point cloud from the current depth prediction and project thepoint cloud at various viewpoints or apply stretches on the current input imageto generate synthetic panoramas. Then we minimize the discrepancy of the 3Dstructure estimated from synthetic images without collecting additional data.We empirically evaluate our method in robot navigation and map-freelocalization where our method shows large performance enhancements. Ourcalibration method can therefore widen the applicability under various externalconditions, serving as a key component for practical panorama-based machinevision systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.14005v1 |
|  30 | LDL: Line Distance Functions for Panoramic Localization                                                                                              | Junho Kim                      | 2023-08-27     | cs.CV                                   | We introduce LDL, a fast and robust algorithm that localizes a panorama to a3D map using line segments. LDL focuses on the sparse structural information oflines in the scene, which is robust to illumination changes and can potentiallyenable efficient computation. While previous line-based localization approachestend to sacrifice accuracy or computation time, our method effectively observesthe holistic distribution of lines within panoramic images and 3D maps.Specifically, LDL matches the distribution of lines with 2D and 3D linedistance functions, which are further decomposed along principal directions oflines to increase the expressiveness. The distance functions provide coarsepose estimates by comparing the distributional information, where the poses arefurther optimized using conventional local feature matching. As our pipelinesolely leverages line geometry and local features, it does not require costlyadditional training of line-specific features or correspondence matching.Nevertheless, our method demonstrates robust performance on challengingscenarios including object layout changes, illumination shifts, and large-scalescenes, while exhibiting fast pose search terminating within a matter ofmilliseconds. We thus expect our method to serve as a practical solution forline-based localization, and complement the well-established point-basedparadigm. The code for LDL is available through the following link:https://github.com/82magnolia/panoramic-localization.                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.13989v1 |
|  31 | Prior-guided Source-free Domain Adaptation for Human Pose Estimation                                                                                 | Dripta S. Raychaudhuri         | 2023-08-26     | cs.CV                                   | Domain adaptation methods for 2D human pose estimation typically requirecontinuous access to the source data during adaptation, which can bechallenging due to privacy, memory, or computational constraints. To addressthis limitation, we focus on the task of source-free domain adaptation for poseestimation, where a source model must adapt to a new target domain using onlyunlabeled target data. Although recent advances have introduced source-freemethods for classification tasks, extending them to the regression task of poseestimation is non-trivial. In this paper, we present Prior-guided Self-training(POST), a pseudo-labeling approach that builds on the popular Mean Teacherframework to compensate for the distribution shift. POST leveragesprediction-level and feature-level consistency between a student and teachermodel against certain image transformations. In the absence of source data,POST utilizes a human pose prior that regularizes the adaptation process bydirecting the model to generate more accurate and anatomically plausible posepseudo-labels. Despite being simple and intuitive, our framework can deliversignificant performance gains compared to applying the source model directly tothe target data, as demonstrated in our extensive experiments and ablationstudies. In fact, our approach achieves comparable performance to recentstate-of-the-art methods that use source data for adaptation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.13954v1 |
|  32 | Disjoint Pose and Shape for 3D Face Reconstruction                                                                                                   | Raja Kumar                     | 2023-08-26     | cs.CV                                   | Existing methods for 3D face reconstruction from a few casually capturedimages employ deep learning based models along with a 3D Morphable Model(3DMM)as face geometry prior. Structure From Motion(SFM), followed by Multi-ViewStereo (MVS), on the other hand, uses dozens of high-resolution images toreconstruct accurate 3D faces.However, it produces noisy and stretched-outresults with only two views available. In this paper, taking inspiration fromboth these methods, we propose an end-to-end pipeline that disjointly solvesfor pose and shape to make the optimization stable and accurate. We use a faceshape prior to estimate face pose and use stereo matching followed by a 3DMM tosolve for the shape. The proposed method achieves end-to-end topologicalconsistency, enables iterative face pose refinement procedure, and showremarkable improvement on both quantitative and qualitative results overexisting state-of-the-art methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.13903v1 |
|  33 | Late Stopping: Avoiding Confidently Learning from Mislabeled Examples                                                                                | Suqin Yuan                     | 2023-08-26     | cs.LG, cs.CV                            | Sample selection is a prevalent method in learning with noisy labels, wheresmall-loss data are typically considered as correctly labeled data. However,this method may not effectively identify clean hard examples with large losses,which are critical for achieving the model's close-to-optimal generalizationperformance. In this paper, we propose a new framework, Late Stopping, whichleverages the intrinsic robust learning ability of DNNs through a prolongedtraining process. Specifically, Late Stopping gradually shrinks the noisydataset by removing high-probability mislabeled examples while retaining themajority of clean hard examples in the training set throughout the learningprocess. We empirically observe that mislabeled and clean examples exhibitdifferences in the number of epochs required for them to be consistently andcorrectly classified, and thus high-probability mislabeled examples can beremoved. Experimental results on benchmark-simulated and real-world noisydatasets demonstrate that the proposed method outperforms state-of-the-artcounterparts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.13862v1 |
|  34 | Beyond One-to-One: Rethinking the Referring Image Segmentation                                                                                       | Yutao Hu                       | 2023-08-26     | cs.CV                                   | Referring image segmentation aims to segment the target object referred by anatural language expression. However, previous methods rely on the strongassumption that one sentence must describe one target in the image, which isoften not the case in real-world applications. As a result, such methods failwhen the expressions refer to either no objects or multiple objects. In thispaper, we address this issue from two perspectives. First, we propose a DualMulti-Modal Interaction (DMMI) Network, which contains two decoder branches andenables information flow in two directions. In the text-to-image decoder, textembedding is utilized to query the visual feature and localize thecorresponding target. Meanwhile, the image-to-text decoder is implemented toreconstruct the erased entity-phrase conditioned on the visual feature. In thisway, visual features are encouraged to contain the critical semanticinformation about target entity, which supports the accurate segmentation inthe text-to-image decoder in turn. Secondly, we collect a new challenging butrealistic dataset called Ref-ZOM, which includes image-text pairs underdifferent settings. Extensive experiments demonstrate our method achievesstate-of-the-art performance on different datasets, and the Ref-ZOM-trainedmodel performs well on various types of text inputs. Codes and datasets areavailable at https://github.com/toggle1995/RIS-DMMI.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.13853v1 |
|  35 | Point-Query Quadtree for Crowd Counting, Localization, and More                                                                                      | Chengxin Liu                   | 2023-08-26     | cs.CV                                   | We show that crowd counting can be viewed as a decomposable point queryingprocess. This formulation enables arbitrary points as input and jointly reasonswhether the points are crowd and where they locate. The querying processing,however, raises an underlying problem on the number of necessary queryingpoints. Too few imply underestimation; too many increase computationaloverhead. To address this dilemma, we introduce a decomposable structure, i.e.,the point-query quadtree, and propose a new counting model, termed Point quEryTransformer (PET). PET implements decomposable point querying viadata-dependent quadtree splitting, where each querying point could split intofour new points when necessary, thus enabling dynamic processing of sparse anddense regions. Such a querying process yields an intuitive, universal modelingof crowd as both the input and output are interpretable and steerable. Wedemonstrate the applications of PET on a number of crowd-related tasks,including fully-supervised crowd counting and localization, partial annotationlearning, and point annotation refinement, and also report state-of-the-artperformance. For the first time, we show that a single counting model canaddress multiple crowd-related tasks across different learning paradigms. Codeis available at https://github.com/cxliu0/PET.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.13814v1 |
|  36 | Generalized Lightness Adaptation with Channel Selective Normalization                                                                                | Mingde Yao                     | 2023-08-26     | cs.CV                                   | Lightness adaptation is vital to the success of image processing to avoidunexpected visual deterioration, which covers multiple aspects, e.g., low-lightimage enhancement, image retouching, and inverse tone mapping. Existing methodstypically work well on their trained lightness conditions but perform poorly inunknown ones due to their limited generalization ability. To address thislimitation, we propose a novel generalized lightness adaptation algorithm thatextends conventional normalization techniques through a channel filteringdesign, dubbed Channel Selective Normalization (CSNorm). The proposed CSNormpurposely normalizes the statistics of lightness-relevant channels and keepsother channels unchanged, so as to improve feature generalization anddiscrimination. To optimize CSNorm, we propose an alternating training strategythat effectively identifies lightness-relevant channels. The model equippedwith our CSNorm only needs to be trained on one lightness condition and can bewell generalized to unknown lightness conditions. Experimental results onmultiple benchmark datasets demonstrate the effectiveness of CSNorm inenhancing the generalization ability for the existing lightness adaptationmethods. Code is available at https://github.com/mdyao/CSNorm.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.13783v1 |
|  37 | MST-compression: Compressing and Accelerating Binary Neural Networks with Minimum Spanning Tree                                                      | Quang Hieu Vo                  | 2023-08-26     | cs.CV                                   | Binary neural networks (BNNs) have been widely adopted to reduce thecomputational cost and memory storage on edge-computing devices by usingone-bit representation for activations and weights. However, as neural networksbecome wider/deeper to improve accuracy and meet practical requirements, thecomputational burden remains a significant challenge even on the binaryversion. To address these issues, this paper proposes a novel method calledMinimum Spanning Tree (MST) compression that learns to compress and accelerateBNNs. The proposed architecture leverages an observation from previous worksthat an output channel in a binary convolution can be computed using anotheroutput channel and XNOR operations with weights that differ from the weights ofthe reused channel. We first construct a fully connected graph with verticescorresponding to output channels, where the distance between two vertices isthe number of different values between the weight sets used for these outputs.Then, the MST of the graph with the minimum depth is proposed to reorder outputcalculations, aiming to reduce computational cost and latency. Moreover, wepropose a new learning algorithm to reduce the total MST distance duringtraining. Experimental results on benchmark models demonstrate that our methodachieves significant compression ratios with negligible accuracy drops, makingit a promising approach for resource-constrained edge-computing devices.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.13735v1 |
|  38 | Eventful Transformers: Leveraging Temporal Redundancy in Vision Transformers                                                                         | Matthew Dutson                 | 2023-08-25     | cs.CV                                   | Vision Transformers achieve impressive accuracy across a range of visualrecognition tasks. Unfortunately, their accuracy frequently comes with highcomputational costs. This is a particular issue in video recognition, wheremodels are often applied repeatedly across frames or temporal chunks. In thiswork, we exploit temporal redundancy between subsequent inputs to reduce thecost of Transformers for video processing. We describe a method for identifyingand re-processing only those tokens that have changed significantly over time.Our proposed family of models, Eventful Transformers, can be converted fromexisting Transformers (often without any re-training) and give adaptive controlover the compute cost at runtime. We evaluate our method on large-scaledatasets for video object detection (ImageNet VID) and action recognition(EPIC-Kitchens 100). Our approach leads to significant computational savings(on the order of 2-4x) with only minor reductions in accuracy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.13494v1 |
|  39 | Harvard Glaucoma Detection and Progression: A Multimodal Multitask Dataset and Generalization-Reinforced Semi-Supervised Learning                    | Yan Luo                        | 2023-08-25     | cs.CV                                   | Glaucoma is the number one cause of irreversible blindness globally. A majorchallenge for accurate glaucoma detection and progression forecasting is thebottleneck of limited labeled patients with the state-of-the-art (SOTA) 3Dretinal imaging data of optical coherence tomography (OCT). To address the datascarcity issue, this paper proposes two solutions. First, we develop a novelgeneralization-reinforced semi-supervised learning (SSL) model called pseudosupervisor to optimally utilize unlabeled data. Compared with SOTA models, theproposed pseudo supervisor optimizes the policy of predicting pseudo labelswith unlabeled samples to improve empirical generalization. Our pseudosupervisor model is evaluated with two clinical tasks consisting of glaucomadetection and progression forecasting. The progression forecasting task isevaluated both unimodally and multimodally. Our pseudo supervisor modeldemonstrates superior performance than SOTA SSL comparison models. Moreover,our model also achieves the best results on the publicly available LAG fundusdataset. Second, we introduce the Harvard Glaucoma Detection and Progression(Harvard-GDP) Dataset, a multimodal multitask dataset that includes data from1,000 patients with OCT imaging data, as well as labels for glaucoma detectionand progression. This is the largest glaucoma detection dataset with 3D OCTimaging data and the first glaucoma progression forecasting dataset that ispublicly available. Detailed sex and racial analysis are provided, which can beused by interested researchers for fairness learning studies. Our releaseddataset is benchmarked with several SOTA supervised CNN and transformer deeplearning models. The dataset and code are made publicly available via\url{https://ophai.hms.harvard.edu/datasets/harvard-gdp1000}.                                                                                            | http://arxiv.org/abs/2308.13411v1 |
|  40 | Distribution-Aligned Diffusion for Human Mesh Recovery                                                                                               | Lin Geng Foo                   | 2023-08-25     | cs.CV                                   | Recovering a 3D human mesh from a single RGB image is a challenging task dueto depth ambiguity and self-occlusion, resulting in a high degree ofuncertainty. Meanwhile, diffusion models have recently seen much success ingenerating high-quality outputs by progressively denoising noisy inputs.Inspired by their capability, we explore a diffusion-based approach for humanmesh recovery, and propose a Human Mesh Diffusion (HMDiff) framework whichframes mesh recovery as a reverse diffusion process. We also propose aDistribution Alignment Technique (DAT) that injects input-specific distributioninformation into the diffusion process, and provides useful prior knowledge tosimplify the mesh recovery task. Our method achieves state-of-the-artperformance on three widely used datasets. Project page:https://gongjia0208.github.io/HMDiff/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.13369v1 |
|  41 | ConSlide: Asynchronous Hierarchical Interaction Transformer with Breakup-Reorganize Rehearsal for Continual Whole Slide Image Analysis               | Yanyan Huang                   | 2023-08-25     | cs.CV                                   | Whole slide image (WSI) analysis has become increasingly important in themedical imaging community, enabling automated and objective diagnosis,prognosis, and therapeutic-response prediction. However, in clinical practice,the ever-evolving environment hamper the utility of WSI analysis models. Inthis paper, we propose the FIRST continual learning framework for WSI analysis,named ConSlide, to tackle the challenges of enormous image size, utilization ofhierarchical structure, and catastrophic forgetting by progressive modelupdating on multiple sequential datasets. Our framework contains three keycomponents. The Hierarchical Interaction Transformer (HIT) is proposed to modeland utilize the hierarchical structural knowledge of WSI. TheBreakup-Reorganize (BuRo) rehearsal method is developed for WSI data replaywith efficient region storing buffer and WSI reorganizing operation. Theasynchronous updating mechanism is devised to encourage the network to learngeneric and specific knowledge respectively during the replay stage, based on anested cross-scale similarity learning (CSSL) module. We evaluated the proposedConSlide on four public WSI datasets from TCGA projects. It performs best overother state-of-the-art methods with a fair WSI-based continual learning settingand achieves a better trade-off of the overall performance and forgetting onprevious task                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.13324v1 |
|  42 | GridPull: Towards Scalability in Learning Implicit Representations from 3D Point Clouds                                                              | Chao Chen                      | 2023-08-25     | cs.CV                                   | Learning implicit representations has been a widely used solution for surfacereconstruction from 3D point clouds. The latest methods infer a distance oroccupancy field by overfitting a neural network on a single point cloud.However, these methods suffer from a slow inference due to the slow convergenceof neural networks and the extensive calculation of distances to surfacepoints, which limits them to small scale points. To resolve the scalabilityissue in surface reconstruction, we propose GridPull to improve the efficiencyof learning implicit representations from large scale point clouds. Our noveltylies in the fast inference of a discrete distance field defined on gridswithout using any neural components. To remedy the lack of continuousnessbrought by neural networks, we introduce a loss function to encouragecontinuous distances and consistent gradients in the field during pullingqueries onto the surface in grids near to the surface. We use uniform grids fora fast grid search to localize sampled queries, and organize surface points ina tree structure to speed up the calculation of distances to the surface. We donot rely on learning priors or normal supervision during optimization, andachieve superiority over the latest methods in terms of complexity andaccuracy. We evaluate our method on shape and scene benchmarks, and reportnumerical and visual comparisons with the latest methods to justify oureffectiveness and superiority. The code is available athttps://github.com/chenchao15/GridPull.                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.13175v1 |
|  43 | IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization                                                   | Zekun Li                       | 2023-08-25     | cs.CV, cs.LG                            | Semi-supervised learning (SSL) aims to leverage massive unlabeled data whenlabels are expensive to obtain. Unfortunately, in many real-world applications,the collected unlabeled data will inevitably contain unseen-class outliers notbelonging to any of the labeled classes. To deal with the challenging open-setSSL task, the mainstream methods tend to first detect outliers and then filterthem out. However, we observe a surprising fact that such approach could resultin more severe performance degradation when labels are extremely scarce, as theunreliable outlier detector may wrongly exclude a considerable portion ofvaluable inliers. To tackle with this issue, we introduce a novel open-set SSLframework, IOMatch, which can jointly utilize inliers and outliers, even whenit is difficult to distinguish exactly between them. Specifically, we proposeto employ a multi-binary classifier in combination with the standard closed-setclassifier for producing unified open-set classification targets, which regardall outliers as a single new class. By adopting these targets as open-setpseudo-labels, we optimize an open-set classifier with all unlabeled samplesincluding both inliers and outliers. Extensive experiments have shown thatIOMatch significantly outperforms the baseline methods across differentbenchmark datasets and different settings despite its remarkable simplicity.Our code and models are available at https://github.com/nukezil/IOMatch.                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.13168v1 |
|  44 | Diff-Retinex: Rethinking Low-light Image Enhancement with A Generative Diffusion Model                                                               | Xunpeng Yi                     | 2023-08-25     | cs.CV, eess.IV                          | In this paper, we rethink the low-light image enhancement task and propose aphysically explainable and generative diffusion model for low-light imageenhancement, termed as Diff-Retinex. We aim to integrate the advantages of thephysical model and the generative network. Furthermore, we hope to supplementand even deduce the information missing in the low-light image through thegenerative network. Therefore, Diff-Retinex formulates the low-light imageenhancement problem into Retinex decomposition and conditional imagegeneration. In the Retinex decomposition, we integrate the superiority ofattention in Transformer and meticulously design a Retinex Transformerdecomposition network (TDN) to decompose the image into illumination andreflectance maps. Then, we design multi-path generative diffusion networks toreconstruct the normal-light Retinex probability distribution and solve thevarious degradations in these components respectively, including darkillumination, noise, color deviation, loss of scene contents, etc. Owing togenerative diffusion model, Diff-Retinex puts the restoration of low-lightsubtle detail into practice. Extensive experiments conducted on real-worldlow-light datasets qualitatively and quantitatively demonstrate theeffectiveness, superiority, and generalization of the proposed method.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.13164v1 |
|  45 | Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models                                                     | Jan Warchocki                  | 2023-08-24     | cs.CV                                   | In temporal action localization, given an input video, the goal is to predictwhich actions it contains, where they begin, and where they end. Training andtesting current state-of-the-art deep learning models requires access to largeamounts of data and computational power. However, gathering such data ischallenging and computational resources might be limited. This work exploresand measures how current deep temporal action localization models perform insettings constrained by the amount of data or computational power. We measuredata efficiency by training each model on a subset of the training set. We findthat TemporalMaxer outperforms other models in data-limited settings.Furthermore, we recommend TriDet when training time is limited. To test theefficiency of the models during inference, we pass videos of different lengthsthrough each model. We find that TemporalMaxer requires the least computationalresources, likely due to its simple architecture.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.13082v1 |
|  46 | Preserving Modality Structure Improves Multi-Modal Learning                                                                                          | Swetha Sirnam                  | 2023-08-24     | cs.CV                                   | Self-supervised learning on large-scale multi-modal datasets allows learningsemantically meaningful embeddings in a joint multi-modal representation spacewithout relying on human annotations. These joint embeddings enable zero-shotcross-modal tasks like retrieval and classification. However, these methodsoften struggle to generalize well on out-of-domain data as they ignore thesemantic structure present in modality-specific embeddings. In this context, wepropose a novel Semantic-Structure-Preserving Consistency approach to improvegeneralizability by preserving the modality-specific relationships in the jointembedding space. To capture modality-specific semantic relationships betweensamples, we propose to learn multiple anchors and represent the multifacetedrelationship between samples with respect to their relationship with theseanchors. To assign multiple anchors to each sample, we propose a novelMulti-Assignment Sinkhorn-Knopp algorithm. Our experimentation demonstratesthat our proposed approach learns semantically meaningful anchors in aself-supervised manner. Furthermore, our evaluation on MSR-VTT and YouCook2datasets demonstrates that our proposed multi-anchor assignment based solutionachieves state-of-the-art performance and generalizes to both inandout-of-domain datasets. Code: https://github.com/Swetha5/Multi_Sinkhorn_Knopp                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.13077v1 |
|  47 | NeO 360: Neural Fields for Sparse View Synthesis of Outdoor Scenes                                                                                   | Muhammad Zubair Irshad         | 2023-08-24     | cs.CV, cs.AI, cs.LG                     | Recent implicit neural representations have shown great results for novelview synthesis. However, existing methods require expensive per-sceneoptimization from many views hence limiting their application to real-worldunbounded urban settings where the objects of interest or backgrounds areobserved from very few views. To mitigate this challenge, we introduce a newapproach called NeO 360, Neural fields for sparse view synthesis of outdoorscenes. NeO 360 is a generalizable method that reconstructs 360{\deg} scenesfrom a single or a few posed RGB images. The essence of our approach is incapturing the distribution of complex real-world outdoor 3D scenes and using ahybrid image-conditional triplanar representation that can be queried from anyworld point. Our representation combines the best of both voxel-based andbird's-eye-view (BEV) representations and is more effective and expressive thaneach. NeO 360's representation allows us to learn from a large collection ofunbounded 3D scenes while offering generalizability to new views and novelscenes from as few as a single image during inference. We demonstrate ourapproach on the proposed challenging 360{\deg} unbounded dataset, called NeRDS360, and show that NeO 360 outperforms state-of-the-art generalizable methodsfor novel view synthesis while also offering editing and compositioncapabilities. Project page:https://zubair-irshad.github.io/projects/neo360.html                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.12967v1 |
|  48 | Scenimefy: Learning to Craft Anime Scene via Semi-Supervised Image-to-Image Translation                                                              | Yuxin Jiang                    | 2023-08-24     | cs.CV, cs.LG                            | Automatic high-quality rendering of anime scenes from complex real-worldimages is of significant practical value. The challenges of this task lie inthe complexity of the scenes, the unique features of anime style, and the lackof high-quality datasets to bridge the domain gap. Despite promising attempts,previous efforts are still incompetent in achieving satisfactory results withconsistent semantic preservation, evident stylization, and fine details. Inthis study, we propose Scenimefy, a novel semi-supervised image-to-imagetranslation framework that addresses these challenges. Our approach guides thelearning with structure-consistent pseudo paired data, simplifying the pureunsupervised setting. The pseudo data are derived uniquely from asemantic-constrained StyleGAN leveraging rich model priors like CLIP. Wefurther apply segmentation-guided data selection to obtain high-quality pseudosupervision. A patch-wise contrastive style loss is introduced to improvestylization and fine details. Besides, we contribute a high-resolution animescene dataset to facilitate future research. Our extensive experimentsdemonstrate the superiority of our method over state-of-the-art baselines interms of both perceptual quality and quantitative performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.12968v1 |
|  49 | Motion-Guided Masking for Spatiotemporal Representation Learning                                                                                     | David Fan                      | 2023-08-24     | cs.CV                                   | Several recent works have directly extended the image masked autoencoder(MAE) with random masking into video domain, achieving promising results.However, unlike images, both spatial and temporal information are important forvideo understanding. This suggests that the random masking strategy that isinherited from the image MAE is less effective for video MAE. This motivatesthe design of a novel masking algorithm that can more efficiently make use ofvideo saliency. Specifically, we propose a motion-guided masking algorithm(MGM) which leverages motion vectors to guide the position of each mask overtime. Crucially, these motion-based correspondences can be directly obtainedfrom information stored in the compressed format of the video, which makes ourmethod efficient and scalable. On two challenging large-scale video benchmarks(Kinetics-400 and Something-Something V2), we equip video MAE with our MGM andachieve up to +$1.3\%$ improvement compared to previous state-of-the-artmethods. Additionally, our MGM achieves equivalent performance to previousvideo MAE using up to $66\%$ fewer training epochs. Lastly, we show that MGMgeneralizes better to downstream transfer learning and domain adaptation taskson the UCF101, HMDB51, and Diving48 datasets, achieving up to +$4.9\%$improvement compared to baseline methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.12962v1 |
|  50 | On Offline Evaluation of 3D Object Detection for Autonomous Driving                                                                                  | Tim Schreier                   | 2023-08-24     | cs.CV, cs.RO                            | Prior work in 3D object detection evaluates models using offline metrics likeaverage precision since closed-loop online evaluation on the downstream drivingtask is costly. However, it is unclear how indicative offline results are ofdriving performance. In this work, we perform the first empirical evaluationmeasuring how predictive different detection metrics are of driving performancewhen detectors are integrated into a full self-driving stack. We conductextensive experiments on urban driving in the CARLA simulator using 16 objectdetection models. We find that the nuScenes Detection Score has a highercorrelation to driving performance than the widely used average precisionmetric. In addition, our results call for caution on the exclusive reliance onthe emerging class of `planner-centric' metrics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.12779v1 |
|  51 | LISTER: Neighbor Decoding for Length-Insensitive Scene Text Recognition                                                                              | Changxu Cheng                  | 2023-08-24     | cs.CV                                   | The diversity in length constitutes a significant characteristic of text. Dueto the long-tail distribution of text lengths, most existing methods for scenetext recognition (STR) only work well on short or seen-length text, lacking thecapability of recognizing longer text or performing length extrapolation. Thisis a crucial issue, since the lengths of the text to be recognized are usuallynot given in advance in real-world applications, but it has not been adequatelyinvestigated in previous works. Therefore, we propose in this paper a methodcalled Length-Insensitive Scene TExt Recognizer (LISTER), which remedies thelimitation regarding the robustness to various text lengths. Specifically, aNeighbor Decoder is proposed to obtain accurate character attention maps withthe assistance of a novel neighbor matrix regardless of the text lengths.Besides, a Feature Enhancement Module is devised to model the long-rangedependency with low computation cost, which is able to perform iterations withthe neighbor decoder to enhance the feature map progressively. To the best ofour knowledge, we are the first to achieve effective length-insensitive scenetext recognition. Extensive experiments demonstrate that the proposed LISTERalgorithm exhibits obvious superiority on long text recognition and the abilityfor length extrapolation, while comparing favourably with the previousstate-of-the-art methods on standard benchmarks for STR (mainly short text).                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.12774v1 |
|  52 | Towards Hierarchical Regional Transformer-based Multiple Instance Learning                                                                           | Josef Cersovsky                | 2023-08-24     | cs.CV, cs.AI, cs.LG                     | The classification of gigapixel histopathology images with deep multipleinstance learning models has become a critical task in digital pathology andprecision medicine. In this work, we propose a Transformer-based multipleinstance learning approach that replaces the traditional learned attentionmechanism with a regional, Vision Transformer inspired self-attentionmechanism. We present a method that fuses regional patch information to deriveslide-level predictions and show how this regional aggregation can be stackedto hierarchically process features on different distance levels. To increasepredictive accuracy, especially for datasets with small, local morphologicalfeatures, we introduce a method to focus the image processing on high attentionregions during inference. Our approach is able to significantly improveperformance over the baseline on two histopathology datasets and points towardspromising directions for further research.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.12634v1 |
|  53 | Logic-induced Diagnostic Reasoning for Semi-supervised Semantic Segmentation                                                                         | Chen Liang                     | 2023-08-24     | cs.CV                                   | Recent advances in semi-supervised semantic segmentation have been heavilyreliant on pseudo labeling to compensate for limited labeled data, disregardingthe valuable relational knowledge among semantic concepts. To bridge this gap,we devise LogicDiag, a brand new neural-logic semi-supervised learningframework. Our key insight is that conflicts within pseudo labels, identifiedthrough symbolic knowledge, can serve as strong yet commonly ignored learningsignals. LogicDiag resolves such conflicts via reasoning with logic-induceddiagnoses, enabling the recovery of (potentially) erroneous pseudo labels,ultimately alleviating the notorious error accumulation problem. We showcasethe practical application of LogicDiag in the data-hungry segmentationscenario, where we formalize the structured abstraction of semantic concepts asa set of logic rules. Extensive experiments on three standard semi-supervisedsemantic segmentation benchmarks demonstrate the effectiveness and generalityof LogicDiag. Moreover, LogicDiag highlights the promising opportunitiesarising from the systematic integration of symbolic reasoning into theprevalent statistical, neural learning approaches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.12595v1 |
|  54 | Self-supervised Learning of Implicit Shape Representation with Dense Correspondence for Deformable Objects                                           | Baowen Zhang                   | 2023-08-24     | cs.CV                                   | Learning 3D shape representation with dense correspondence for deformableobjects is a fundamental problem in computer vision. Existing approaches oftenneed additional annotations of specific semantic domain, e.g., skeleton posesfor human bodies or animals, which require extra annotation effort and sufferfrom error accumulation, and they are limited to specific domain. In thispaper, we propose a novel self-supervised approach to learn neural implicitshape representation for deformable objects, which can represent shapes with atemplate shape and dense correspondence in 3D. Our method does not require thepriors of skeleton and skinning weight, and only requires a collection ofshapes represented in signed distance fields. To handle the large deformation,we constrain the learned template shape in the same latent space with thetraining shapes, design a new formulation of local rigid constraint thatenforces rigid transformation in local region and addresses local reflectionissue, and present a new hierarchical rigid constraint to reduce the ambiguitydue to the joint learning of template shape and correspondences. Extensiveexperiments show that our model can represent shapes with large deformations.We also show that our shape representation can support two typicalapplications, such as texture transfer and shape editing, with competitiveperformance. The code and models are available athttps://iscas3dv.github.io/deformshape                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.12590v1 |
|  55 | Grounded Entity-Landmark Adaptive Pre-training for Vision-and-Language Navigation                                                                    | Yibo Cui                       | 2023-08-24     | cs.CV                                   | Cross-modal alignment is one key challenge for Vision-and-Language Navigation(VLN). Most existing studies concentrate on mapping the global instruction orsingle sub-instruction to the corresponding trajectory. However, anothercritical problem of achieving fine-grained alignment at the entity level isseldom considered. To address this problem, we propose a novel GroundedEntity-Landmark Adaptive (GELA) pre-training paradigm for VLN tasks. To achievethe adaptive pre-training paradigm, we first introduce grounded entity-landmarkhuman annotations into the Room-to-Room (R2R) dataset, named GEL-R2R.Additionally, we adopt three grounded entity-landmark adaptive pre-trainingobjectives: 1) entity phrase prediction, 2) landmark bounding box prediction,and 3) entity-landmark semantic alignment, which explicitly supervise thelearning of fine-grained cross-modal alignment between entity phrases andenvironment landmarks. Finally, we validate our model on two downstreambenchmarks: VLN with descriptive instructions (R2R) and dialogue instructions(CVDN). The comprehensive experiments show that our GELA model achievesstate-of-the-art results on both tasks, demonstrating its effectiveness andgeneralizability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.12587v1 |
|  56 | LORD: Leveraging Open-Set Recognition with Unknown Data                                                                                              | Tobias Koch                    | 2023-08-24     | cs.CV, cs.LG                            | Handling entirely unknown data is a challenge for any deployed classifier.Classification models are typically trained on a static pre-defined dataset andare kept in the dark for the open unassigned feature space. As a result, theystruggle to deal with out-of-distribution data during inference. Addressingthis task on the class-level is termed open-set recognition (OSR). However,most OSR methods are inherently limited, as they train closed-set classifiersand only adapt the downstream predictions to OSR. This work presents LORD, aframework to Leverage Open-set Recognition by exploiting unknown Data. LORDexplicitly models open space during classifier training and provides asystematic evaluation for such approaches. We identify three model-agnostictraining strategies that exploit background data and applied them towell-established classifiers. Due to LORD's extensive evaluation protocol, weconsistently demonstrate improved recognition of unknown data. The benchmarksfacilitate in-depth analysis across various requirement levels. To mitigatedependency on extensive and costly background datasets, we explore mixup as anoff-the-shelf data generation technique. Our experiments highlight mixup'seffectiveness as a substitute for background datasets. Lightweight constraintson mixup synthesis further improve OSR performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.12584v1 |
|  57 | NOVA: NOvel View Augmentation for Neural Composition of Dynamic Objects                                                                              | Dakshit Agrawal                | 2023-08-24     | cs.CV                                   | We propose a novel-view augmentation (NOVA) strategy to train NeRFs forphoto-realistic 3D composition of dynamic objects in a static scene. Comparedto prior work, our framework significantly reduces blending artifacts wheninserting multiple dynamic objects into a 3D scene at novel views and times;achieves comparable PSNR without the need for additional ground truthmodalities like optical flow; and overall provides ease, flexibility, andscalability in neural composition. Our codebase is on GitHub.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.12560v1 |
|  58 | Hyperbolic Audio-visual Zero-shot Learning                                                                                                           | Jie Hong                       | 2023-08-24     | cs.CV                                   | Audio-visual zero-shot learning aims to classify samples consisting of a pairof corresponding audio and video sequences from classes that are not presentduring training. An analysis of the audio-visual data reveals a large degree ofhyperbolicity, indicating the potential benefit of using a hyperbolictransformation to achieve curvature-aware geometric learning, with the aim ofexploring more complex hierarchical data structures for this task. The proposedapproach employs a novel loss function that incorporates cross-modalityalignment between video and audio features in the hyperbolic space.Additionally, we explore the use of multiple adaptive curvatures for hyperbolicprojections. The experimental results on this very challenging task demonstratethat our proposed hyperbolic approach for zero-shot learning outperforms theSOTA method on three datasets: VGGSound-GZSL, UCF-GZSL, and ActivityNet-GZSLachieving a harmonic mean (HM) improvement of around 3.0%, 7.0%, and 5.3%,respectively.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.12558v1 |
|  59 | Synchronize Feature Extracting and Matching: A Single Branch Framework for 3D Object Tracking                                                        | Teli Ma                        | 2023-08-24     | cs.CV, cs.AI                            | Siamese network has been a de facto benchmark framework for 3D LiDAR objecttracking with a shared-parametric encoder extracting features from template andsearch region, respectively. This paradigm relies heavily on an additionalmatching network to model the cross-correlation/similarity of the template andsearch region. In this paper, we forsake the conventional Siamese paradigm andpropose a novel single-branch framework, SyncTrack, synchronizing the featureextracting and matching to avoid forwarding encoder twice for template andsearch region as well as introducing extra parameters of matching network. Thesynchronization mechanism is based on the dynamic affinity of the Transformer,and an in-depth analysis of the relevance is provided theoretically. Moreover,based on the synchronization, we introduce a novel Attentive Points-Samplingstrategy into the Transformer layers (APST), replacing the random/FarthestPoints Sampling (FPS) method with sampling under the supervision of attentiverelations between the template and search region. It implies connectingpoint-wise sampling with the feature learning, beneficial to aggregating moredistinctive and geometric features for tracking with sparse points. Extensiveexperiments on two benchmark datasets (KITTI and NuScenes) show that SyncTrackachieves state-of-the-art performance in real-time tracking.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.12549v1 |
|  60 | Masked Autoencoders are Efficient Class Incremental Learners                                                                                         | Jiang-Tian Zhai                | 2023-08-24     | cs.CV, cs.AI, cs.LG                     | Class Incremental Learning (CIL) aims to sequentially learn new classes whileavoiding catastrophic forgetting of previous knowledge. We propose to useMasked Autoencoders (MAEs) as efficient learners for CIL. MAEs were originallydesigned to learn useful representations through reconstructive unsupervisedlearning, and they can be easily integrated with a supervised loss forclassification. Moreover, MAEs can reliably reconstruct original input imagesfrom randomly selected patches, which we use to store exemplars from past tasksmore efficiently for CIL. We also propose a bilateral MAE framework to learnfrom image-level and embedding-level fusion, which produces better-qualityreconstructed images and more stable representations. Our experiments confirmthat our approach performs better than the state-of-the-art on CIFAR-100,ImageNet-Subset, and ImageNet-Full. The code is available athttps://github.com/scok30/MAE-CIL .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.12510v1 |
|  61 | MOFA: A Model Simplification Roadmap for Image Restoration on Mobile Devices                                                                         | Xiangyu Chen                   | 2023-08-24     | cs.CV, eess.IV                          | Image restoration aims to restore high-quality images from degradedcounterparts and has seen significant advancements through deep learningtechniques. The technique has been widely applied to mobile devices for taskssuch as mobile photography. Given the resource limitations on mobile devices,such as memory constraints and runtime requirements, the efficiency of modelsduring deployment becomes paramount. Nevertheless, most previous works haveprimarily concentrated on analyzing the efficiency of single modules andimproving them individually. This paper examines the efficiency acrossdifferent layers. We propose a roadmap that can be applied to furtheraccelerate image restoration models prior to deployment while simultaneouslyincreasing PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural SimilarityIndex). The roadmap first increases the model capacity by adding moreparameters to partial convolutions on FLOPs non-sensitive layers. Then, itapplies partial depthwise convolution coupled with decouplingupsampling/downsampling layers to accelerate the model speed. Extensiveexperiments demonstrate that our approach decreases runtime by up to 13% andreduces the number of parameters by up to 23%, while increasing PSNR and SSIMon several image restoration datasets. Source Code of our method is availableat \href{https://github.com/xiangyu8/MOFA}{https://github.com/xiangyu8/MOFA}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.12494v1 |
|  62 | With a Little Help from your own Past: Prototypical Memory Networks for Image Captioning                                                             | Manuele Barraco                | 2023-08-23     | cs.CV, cs.AI, cs.CL, cs.MM              | Image captioning, like many tasks involving vision and language, currentlyrelies on Transformer-based architectures for extracting the semantics in animage and translating it into linguistically coherent descriptions. Althoughsuccessful, the attention operator only considers a weighted summation ofprojections of the current input sample, therefore ignoring the relevantsemantic information which can come from the joint observation of othersamples. In this paper, we devise a network which can perform attention overactivations obtained while processing other training samples, through aprototypical memory model. Our memory models the distribution of past keys andvalues through the definition of prototype vectors which are bothdiscriminative and compact. Experimentally, we assess the performance of theproposed model on the COCO dataset, in comparison with carefully designedbaselines and state-of-the-art approaches, and by investigating the role ofeach of the proposed components. We demonstrate that our proposal can increasethe performance of an encoder-decoder Transformer by 3.7 CIDEr points both whentraining in cross-entropy only and when fine-tuning with self-critical sequencetraining. Source code and trained models are available at:https://github.com/aimagelab/PMA-Net.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.12383v1 |
|  63 | Vision Transformer Adapters for Generalizable Multitask Learning                                                                                     | Deblina Bhattacharjee          | 2023-08-23     | cs.CV, cs.CL                            | We introduce the first multitasking vision transformer adapters that learngeneralizable task affinities which can be applied to novel tasks and domains.Integrated into an off-the-shelf vision transformer backbone, our adapters cansimultaneously solve multiple dense vision tasks in a parameter-efficientmanner, unlike existing multitasking transformers that are parametricallyexpensive. In contrast to concurrent methods, we do not require retraining orfine-tuning whenever a new task or domain is added. We introduce a task-adaptedattention mechanism within our adapter framework that combines gradient-basedtask similarities with attention-based ones. The learned task affinitiesgeneralize to the following settings: zero-shot task transfer, unsuperviseddomain adaptation, and generalization without fine-tuning to novel domains. Wedemonstrate that our approach outperforms not only the existing convolutionalneural network-based multitasking methods but also the vision transformer-basedones. Our project page is at \url{https://ivrl.github.io/VTAGML}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.12372v1 |
|  64 | AdVerb: Visually Guided Audio Dereverberation                                                                                                        | Sanjoy Chowdhury               | 2023-08-23     | cs.CV, cs.MM, cs.SD, eess.AS            | We present AdVerb, a novel audio-visual dereverberation framework that usesvisual cues in addition to the reverberant sound to estimate clean audio.Although audio-only dereverberation is a well-studied problem, our approachincorporates the complementary visual modality to perform audiodereverberation. Given an image of the environment where the reverberated soundsignal has been recorded, AdVerb employs a novel geometry-aware cross-modaltransformer architecture that captures scene geometry and audio-visualcross-modal relationship to generate a complex ideal ratio mask, which, whenapplied to the reverberant audio predicts the clean sound. The effectiveness ofour method is demonstrated through extensive quantitative and qualitativeevaluations. Our approach significantly outperforms traditional audio-only andaudio-visual baselines on three downstream tasks: speech enhancement, speechrecognition, and speaker verification, with relative improvements in the rangeof 18% - 82% on the LibriSpeech test-clean set. We also achieve highlysatisfactory RT60 error scores on the AVSpeech dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.12370v1 |
|  65 | Continual Zero-Shot Learning through Semantically Guided Generative Random Walks                                                                     | Wenxuan Zhang                  | 2023-08-23     | cs.CV                                   | Learning novel concepts, remembering previous knowledge, and adapting it tofuture tasks occur simultaneously throughout a human's lifetime. To model suchcomprehensive abilities, continual zero-shot learning (CZSL) has recently beenintroduced. However, most existing methods overused unseen semantic informationthat may not be continually accessible in realistic settings. In this paper, weaddress the challenge of continual zero-shot learning where unseen informationis not provided during training, by leveraging generative modeling. The heartof the generative-based methods is to learn quality representations from seenclasses to improve the generative understanding of the unseen visual space.Motivated by this, we introduce generalization-bound tools and provide thefirst theoretical explanation for the benefits of generative modeling to CZSLtasks. Guided by the theoretical analysis, we then propose our learningalgorithm that employs a novel semantically guided Generative Random Walk (GRW)loss. The GRW loss augments the training by continually encouraging the modelto generate realistic and characterized samples to represent the unseen space.Our algorithm achieves state-of-the-art performance on AWA1, AWA2, CUB, and SUNdatasets, surpassing existing CZSL methods by 3-7\%. The code has been madeavailable here \url{https://github.com/wx-zhang/IGCZSL}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.12366v1 |
|  66 | CHORUS: Learning Canonicalized 3D Human-Object Spatial Relations from Unbounded Synthesized Images                                                   | Sookwan Han                    | 2023-08-23     | cs.CV, cs.AI                            | We present a method for teaching machines to understand and model theunderlying spatial common sense of diverse human-object interactions in 3D in aself-supervised way. This is a challenging task, as there exist specificmanifolds of the interactions that can be considered human-like and natural,but the human pose and the geometry of objects can vary even for similarinteractions. Such diversity makes the annotating task of 3D interactionsdifficult and hard to scale, which limits the potential to reason about that ina supervised way. One way of learning the 3D spatial relationship betweenhumans and objects during interaction is by showing multiple 2D images capturedfrom different viewpoints when humans interact with the same type of objects.The core idea of our method is to leverage a generative model that produceshigh-quality 2D images from an arbitrary text prompt input as an "unbounded"data generator with effective controllability and view diversity. Despite itsimperfection of the image quality over real images, we demonstrate that thesynthesized images are sufficient to learn the 3D human-object spatialrelations. We present multiple strategies to leverage the synthesized images,including (1) the first method to leverage a generative image model for 3Dhuman-object spatial relation learning; (2) a framework to reason about the 3Dspatial relations from inconsistent 2D cues in a self-supervised manner via 3Doccupancy reasoning with pose canonicalization; (3) semantic clustering todisambiguate different types of interactions with the same object types; and(4) a novel metric to assess the quality of 3D spatial learning of interaction.Project Page: https://jellyheadandrew.github.io/projects/chorus                                                                                                                                                                     | http://arxiv.org/abs/2308.12288v1 |
|  67 | SG-Former: Self-guided Transformer with Evolving Token Reallocation                                                                                  | Sucheng Ren                    | 2023-08-23     | cs.CV                                   | Vision Transformer has demonstrated impressive success across various visiontasks. However, its heavy computation cost, which grows quadratically withrespect to the token sequence length, largely limits its power in handlinglarge feature maps. To alleviate the computation cost, previous works rely oneither fine-grained self-attentions restricted to local small regions, orglobal self-attentions but to shorten the sequence length resulting in coarsegranularity. In this paper, we propose a novel model, termed as Self-guidedTransformer~(SG-Former), towards effective global self-attention with adaptivefine granularity. At the heart of our approach is to utilize a significancemap, which is estimated through hybrid-scale self-attention and evolves itselfduring training, to reallocate tokens based on the significance of each region.Intuitively, we assign more tokens to the salient regions for achievingfine-grained attention, while allocating fewer tokens to the minor regions inexchange for efficiency and global receptive fields. The proposed SG-Formerachieves performance superior to state of the art: our base size model achieves\textbf{84.7\%} Top-1 accuracy on ImageNet-1K, \textbf{51.2mAP} bbAP on CoCo,\textbf{52.7mIoU} on ADE20K surpassing the Swin Transformer by \textbf{+1.3\% /+2.7 mAP/ +3 mIoU}, with lower computation costs and fewer parameters. The codeis available at\href{https://github.com/OliverRensu/SG-Former}{https://github.com/OliverRensu/SG-Former}                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.12216v1 |
|  68 | CLIPN for Zero-Shot OOD Detection: Teaching CLIP to Say No                                                                                           | Hualiang Wang                  | 2023-08-23     | cs.CV, cs.AI, 68T45, I.4.9              | Out-of-distribution (OOD) detection refers to training the model on anin-distribution (ID) dataset to classify whether the input images come fromunknown classes. Considerable effort has been invested in designing various OODdetection methods based on either convolutional neural networks ortransformers. However, zero-shot OOD detection methods driven by CLIP, whichonly require class names for ID, have received less attention. This paperpresents a novel method, namely CLIP saying no (CLIPN), which empowers thelogic of saying no within CLIP. Our key motivation is to equip CLIP with thecapability of distinguishing OOD and ID samples using positive-semantic promptsand negation-semantic prompts. Specifically, we design a novel learnable noprompt and a no text encoder to capture negation semantics within images.Subsequently, we introduce two loss functions: the image-text binary-oppositeloss and the text semantic-opposite loss, which we use to teach CLIPN toassociate images with no prompts, thereby enabling it to identify unknownsamples. Furthermore, we propose two threshold-free inference algorithms toperform OOD detection by utilizing negation semantics from no prompts and thetext encoder. Experimental results on 9 benchmark datasets (3 ID datasets and 6OOD datasets) for the OOD detection task demonstrate that CLIPN, based onViT-B-16, outperforms 7 well-used algorithms by at least 2.34% and 11.64% interms of AUROC and FPR95 for zero-shot OOD detection on ImageNet-1K. Our CLIPNcan serve as a solid foundation for effectively leveraging CLIP in downstreamOOD tasks. The code is available on https://github.com/xmed-lab/CLIPN.                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.12213v2 |
|  69 | Sign Language Translation with Iterative Prototype                                                                                                   | Huijie Yao                     | 2023-08-23     | cs.CV                                   | This paper presents IP-SLT, a simple yet effective framework for signlanguage translation (SLT). Our IP-SLT adopts a recurrent structure andenhances the semantic representation (prototype) of the input sign languagevideo via an iterative refinement manner. Our idea mimics the behavior of humanreading, where a sentence can be digested repeatedly, till reaching accurateunderstanding. Technically, IP-SLT consists of feature extraction, prototypeinitialization, and iterative prototype refinement. The initialization modulegenerates the initial prototype based on the visual feature extracted by thefeature extraction module. Then, the iterative refinement module leverages thecross-attention mechanism to polish the previous prototype by aggregating itwith the original video feature. Through repeated refinement, the prototypefinally converges to a more stable and accurate state, leading to a fluent andappropriate translation. In addition, to leverage the sequential dependence ofprototypes, we further propose an iterative distillation loss to compress theknowledge of the final iteration into previous ones. As the autoregressivedecoding process is executed only once in inference, our IP-SLT is ready toimprove various SLT systems with acceptable overhead. Extensive experiments areconducted on public benchmarks to demonstrate the effectiveness of the IP-SLT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.12191v1 |
|  70 | The TYC Dataset for Understanding Instance-Level Semantics and Motions of Cells in Microstructures                                                   | Christoph Reich                | 2023-08-23     | cs.CV                                   | Segmenting cells and tracking their motion over time is a common task inbiomedical applications. However, predicting accurate instance-wisesegmentation and cell motions from microscopy imagery remains a challengingtask. Using microstructured environments for analyzing single cells in aconstant flow of media adds additional complexity. While large-scale labeledmicroscopy datasets are available, we are not aware of any large-scale dataset,including both cells and microstructures. In this paper, we introduce thetrapped yeast cell (TYC) dataset, a novel dataset for understandinginstance-level semantics and motions of cells in microstructures. We release$105$ dense annotated high-resolution brightfield microscopy images, includingabout $19$k instance masks. We also release $261$ curated video clips composedof $1293$ high-resolution microscopy images to facilitate unsupervisedunderstanding of cell motions and morphology. TYC offers ten times moreinstance annotations than the previously largest dataset, including cells andmicrostructures. Our effort also exceeds previous attempts in terms ofmicrostructure variability, resolution, complexity, and capturing device(microscopy) variability. We facilitate a unified comparison on our noveldataset by introducing a standardized evaluation strategy. TYC and evaluationcode are publicly available under CC BY 4.0 license.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.12116v1 |
|  71 | SILT: Shadow-aware Iterative Label Tuning for Learning to Detect Shadows from Noisy Labels                                                           | Han Yang                       | 2023-08-23     | cs.CV                                   | Existing shadow detection datasets often contain missing or mislabeledshadows, which can hinder the performance of deep learning models traineddirectly on such data. To address this issue, we propose SILT, the Shadow-awareIterative Label Tuning framework, which explicitly considers noise in shadowlabels and trains the deep model in a self-training manner. Specifically, weincorporate strong data augmentations with shadow counterfeiting to help thenetwork better recognize non-shadow regions and alleviate overfitting. We alsodevise a simple yet effective label tuning strategy with global-local fusionand shadow-aware filtering to encourage the network to make significantrefinements on the noisy labels. We evaluate the performance of SILT byrelabeling the test set of the SBU dataset and conducting various experiments.Our results show that even a simple U-Net trained with SILT can outperform allstate-of-the-art methods by a large margin. When trained on SBU / UCF / ISTD,our network can successfully reduce the Balanced Error Rate by 25.2% / 36.9% /21.3% over the best state-of-the-art method.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.12064v1 |
|  72 | DR-Tune: Improving Fine-tuning of Pretrained Visual Models by Distribution Regularization with Semantic Calibration                                  | Nan Zhou                       | 2023-08-23     | cs.CV                                   | The visual models pretrained on large-scale benchmarks encode generalknowledge and prove effective in building more powerful representations fordownstream tasks. Most existing approaches follow the fine-tuning paradigm,either by initializing or regularizing the downstream model based on thepretrained one. The former fails to retain the knowledge in the successivefine-tuning phase, thereby prone to be over-fitting, and the latter imposesstrong constraints to the weights or feature maps of the downstream modelwithout considering semantic drift, often incurring insufficient optimization.To deal with these issues, we propose a novel fine-tuning framework, namelydistribution regularization with semantic calibration (DR-Tune). It employsdistribution regularization by enforcing the downstream task head to decreaseits classification error on the pretrained feature distribution, which preventsit from over-fitting while enabling sufficient training of downstream encoders.Furthermore, to alleviate the interference by semantic drift, we develop thesemantic calibration (SC) module to align the global shape and class centers ofthe pretrained and downstream feature distributions. Extensive experiments onwidely used image classification datasets show that DR-Tune consistentlyimproves the performance when combing with various backbones under differentpretraining strategies. Code is available at:https://github.com/weeknan/DR-Tune.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.12058v1 |
|  73 | RankMixup: Ranking-Based Mixup Training for Network Calibration                                                                                      | Jongyoun Noh                   | 2023-08-23     | cs.CV                                   | Network calibration aims to accurately estimate the level of confidences,which is particularly important for employing deep neural networks inreal-world systems. Recent approaches leverage mixup to calibrate the network'spredictions during training. However, they do not consider the problem thatmixtures of labels in mixup may not accurately represent the actualdistribution of augmented samples. In this paper, we present RankMixup, a novelmixup-based framework alleviating the problem of the mixture of labels fornetwork calibration. To this end, we propose to use an ordinal rankingrelationship between raw and mixup-augmented samples as an alternativesupervisory signal to the label mixtures for network calibration. Wehypothesize that the network should estimate a higher level of confidence forthe raw samples than the augmented ones (Fig.1). To implement this idea, weintroduce a mixup-based ranking loss (MRL) that encourages lower confidencesfor augmented samples compared to raw ones, maintaining the rankingrelationship. We also propose to leverage the ranking relationship amongmultiple mixup-augmented samples to further improve the calibration capability.Augmented samples with larger mixing coefficients are expected to have higherconfidences and vice versa (Fig.1). That is, the order of confidences should bealigned with that of mixing coefficients. To this end, we introduce a novelloss, M-NDCG, in order to reduce the number of misaligned pairs of thecoefficients and confidences. Extensive experimental results on standardbenchmarks for network calibration demonstrate the effectiveness of RankMixup.                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.11990v1 |
|  74 | Blending-NeRF: Text-Driven Localized Editing in Neural Radiance Fields                                                                               | Hyeonseop Song                 | 2023-08-23     | cs.CV, cs.AI, cs.GR                     | Text-driven localized editing of 3D objects is particularly difficult aslocally mixing the original 3D object with the intended new object and styleeffects without distorting the object's form is not a straightforward process.To address this issue, we propose a novel NeRF-based model, Blending-NeRF,which consists of two NeRF networks: pretrained NeRF and editable NeRF.Additionally, we introduce new blending operations that allow Blending-NeRF toproperly edit target regions which are localized by text. By using a pretrainedvision-language aligned model, CLIP, we guide Blending-NeRF to add new objectswith varying colors and densities, modify textures, and remove parts of theoriginal object. Our extensive experiments demonstrate that Blending-NeRFproduces naturally and locally edited 3D objects from various text prompts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.11974v1 |
|  75 | LFS-GAN: Lifelong Few-Shot Image Generation                                                                                                          | Juwon Seo                      | 2023-08-23     | cs.CV, cs.AI                            | We address a challenging lifelong few-shot image generation task for thefirst time. In this situation, a generative model learns a sequence of tasksusing only a few samples per task. Consequently, the learned model encountersboth catastrophic forgetting and overfitting problems at a time. Existingstudies on lifelong GANs have proposed modulation-based methods to preventcatastrophic forgetting. However, they require considerable additionalparameters and cannot generate high-fidelity and diverse images from limiteddata. On the other hand, the existing few-shot GANs suffer from severecatastrophic forgetting when learning multiple tasks. To alleviate theseissues, we propose a framework called Lifelong Few-Shot GAN (LFS-GAN) that cangenerate high-quality and diverse images in lifelong few-shot image generationtask. Our proposed framework learns each task using an efficient task-specificmodulator - Learnable Factorized Tensor (LeFT). LeFT is rank-constrained andhas a rich representation ability due to its unique reconstruction technique.Furthermore, we propose a novel mode seeking loss to improve the diversity ofour model in low-data circumstances. Extensive experiments demonstrate that theproposed LFS-GAN can generate high-fidelity and diverse images without anyforgetting and mode collapse in various domains, achieving state-of-the-art inlifelong few-shot image generation task. Surprisingly, we find that our LFS-GANeven outperforms the existing few-shot GANs in the few-shot image generationtask. The code is available at Github.                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.11917v1 |
|  76 | Semantic-Aware Implicit Template Learning via Part Deformation Consistency                                                                           | Sihyeon Kim                    | 2023-08-23     | cs.CV                                   | Learning implicit templates as neural fields has recently shown impressiveperformance in unsupervised shape correspondence. Despite the success, weobserve current approaches, which solely rely on geometric information, oftenlearn suboptimal deformation across generic object shapes, which have highstructural variability. In this paper, we highlight the importance of partdeformation consistency and propose a semantic-aware implicit template learningframework to enable semantically plausible deformation. By leveraging semanticprior from a self-supervised feature extractor, we suggest local conditioningwith novel semantic-aware deformation code and deformation consistencyregularizations regarding part deformation, global deformation, and globalscaling. Our extensive experiments demonstrate the superiority of the proposedmethod over baselines in various tasks: keypoint transfer, part label transfer,and texture transfer. More interestingly, our framework shows a largerperformance gain under more challenging settings. We also provide qualitativeanalyses to validate the effectiveness of semantic-aware deformation. The codeis available at https://github.com/mlvlab/PDC.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.11916v1 |
|  77 | ACLS: Adaptive and Conditional Label Smoothing for Network Calibration                                                                               | Hyekang Park                   | 2023-08-23     | cs.CV                                   | We address the problem of network calibration adjusting miscalibratedconfidences of deep neural networks. Many approaches to network calibrationadopt a regularization-based method that exploits a regularization term tosmooth the miscalibrated confidences. Although these approaches have shown theeffectiveness on calibrating the networks, there is still a lack ofunderstanding on the underlying principles of regularization in terms ofnetwork calibration. We present in this paper an in-depth analysis of existingregularization-based methods, providing a better understanding on how theyaffect to network calibration. Specifically, we have observed that 1) theregularization-based methods can be interpreted as variants of label smoothing,and 2) they do not always behave desirably. Based on the analysis, we introducea novel loss function, dubbed ACLS, that unifies the merits of existingregularization methods, while avoiding the limitations. We show extensiveexperimental results for image classification and semantic segmentation onstandard benchmarks, including CIFAR10, Tiny-ImageNet, ImageNet, and PASCALVOC, demonstrating the effectiveness of our loss function.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.11911v2 |
|  78 | Studying the Impact of Augmentations on Medical Confidence Calibration                                                                               | Adrit Rao                      | 2023-08-23     | eess.IV                                 | The clinical explainability of convolutional neural networks (CNN) heavilyrelies on the joint interpretation of a model's predicted diagnostic label andassociated confidence. A highly certain or uncertain model can significantlyimpact clinical decision-making. Thus, ensuring that confidence estimatesreflect the true correctness likelihood for a prediction is essential. CNNs areoften poorly calibrated and prone to overconfidence leading to impropermeasures of uncertainty. This creates the need for confidence calibration.However, accuracy and performance-based evaluations of CNNs are commonly usedas the sole benchmark for medical tasks. Taking into consideration the risksassociated with miscalibration is of high importance. In recent years, modernaugmentation techniques, which cut, mix, and combine images, have beenintroduced. Such augmentations have benefited CNNs through regularization,robustness to adversarial samples, and calibration. Standard augmentationsbased on image scaling, rotating, and zooming, are widely leveraged in themedical domain to combat the scarcity of data. In this paper, we evaluate theeffects of three modern augmentation techniques, CutMix, MixUp, and CutOut onthe calibration and performance of CNNs for medical tasks. CutMix improvedcalibration the most while CutOut often lowered the level of calibration.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.11902v1 |
|  79 | Camera-Driven Representation Learning for Unsupervised Domain Adaptive Person Re-identification                                                      | Geon Lee                       | 2023-08-23     | cs.CV                                   | We present a novel unsupervised domain adaption method for personre-identification (reID) that generalizes a model trained on a labeled sourcedomain to an unlabeled target domain. We introduce a camera-driven curriculumlearning (CaCL) framework that leverages camera labels of person images totransfer knowledge from source to target domains progressively. To this end, wedivide target domain dataset into multiple subsets based on the camera labels,and initially train our model with a single subset (i.e., images captured by asingle camera). We then gradually exploit more subsets for training, accordingto a curriculum sequence obtained with a camera-driven scheduling rule. Thescheduler considers maximum mean discrepancies (MMD) between each subset andthe source domain dataset, such that the subset closer to the source domain isexploited earlier within the curriculum. For each curriculum sequence, wegenerate pseudo labels of person images in a target domain to train a reIDmodel in a supervised way. We have observed that the pseudo labels are highlybiased toward cameras, suggesting that person images obtained from the samecamera are likely to have the same pseudo labels, even for different IDs. Toaddress the camera bias problem, we also introduce a camera-diversity (CD) lossencouraging person images of the same pseudo label, but captured across variouscameras, to involve more for discriminative feature learning, providing personrepresentations robust to inter-camera variations. Experimental results onstandard benchmarks, including real-to-real and synthetic-to-real scenarios,demonstrate the effectiveness of our framework.                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.11901v1 |
|  80 | Does Physical Adversarial Example Really Matter to Autonomous Driving? Towards System-Level Effect of Adversarial Object Evasion Attack              | Ningfei Wang                   | 2023-08-23     | cs.CR, cs.CV                            | In autonomous driving (AD), accurate perception is indispensable to achievingsafe and secure driving. Due to its safety-criticality, the security of ADperception has been widely studied. Among different attacks on AD perception,the physical adversarial object evasion attacks are especially severe. However,we find that all existing literature only evaluates their attack effect at thetargeted AI component level but not at the system level, i.e., with the entiresystem semantics and context such as the full AD pipeline. Thereby, this raisesa critical research question: can these existing researches effectively achievesystem-level attack effects (e.g., traffic rule violations) in the real-worldAD context? In this work, we conduct the first measurement study on whether andhow effectively the existing designs can lead to system-level effects,especially for the STOP sign-evasion attacks due to their popularity andseverity. Our evaluation results show that all the representative prior workscannot achieve any system-level effects. We observe two design limitations inthe prior works: 1) physical model-inconsistent object size distribution inpixel sampling and 2) lack of vehicle plant model and AD system modelconsideration. Then, we propose SysAdv, a novel system-driven attack design inthe AD context and our evaluation results show that the system-level effectscan be significantly improved, i.e., the violation rate increases by around70%.                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.11894v1 |
|  81 | SUMMIT: Source-Free Adaptation of Uni-Modal Models to Multi-Modal Targets                                                                            | Cody Simons                    | 2023-08-23     | cs.CV, cs.LG                            | Scene understanding using multi-modal data is necessary in many applications,e.g., autonomous navigation. To achieve this in a variety of situations,existing models must be able to adapt to shifting data distributions withoutarduous data annotation. Current approaches assume that the source data isavailable during adaptation and that the source consists of paired multi-modaldata. Both these assumptions may be problematic for many applications. Sourcedata may not be available due to privacy, security, or economic concerns.Assuming the existence of paired multi-modal data for training also entailssignificant data collection costs and fails to take advantage of widelyavailable freely distributed pre-trained uni-modal models. In this work, werelax both of these assumptions by addressing the problem of adapting a set ofmodels trained independently on uni-modal data to a target domain consisting ofunlabeled multi-modal data, without having access to the original sourcedataset. Our proposed approach solves this problem through a switchingframework which automatically chooses between two complementary methods ofcross-modal pseudo-label fusion -- agreement filtering and entropy weighting --based on the estimated domain gap. We demonstrate our work on the semanticsegmentation problem. Experiments across seven challenging adaptation scenariosverify the efficacy of our approach, achieving results comparable to, and insome cases outperforming, methods which assume access to source data. Ourmethod achieves an improvement in mIoU of up to 12% over competing baselines.Our code is publicly available at https://github.com/csimo005/SUMMIT.                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.11880v1 |
|  82 | Semi-Supervised Learning via Weight-aware Distillation under Class Distribution Mismatch                                                             | Pan Du                         | 2023-08-23     | cs.CV                                   | Semi-Supervised Learning (SSL) under class distribution mismatch aims totackle a challenging problem wherein unlabeled data contain lots of unknowncategories unseen in the labeled ones. In such mismatch scenarios, traditionalSSL suffers severe performance damage due to the harmful invasion of theinstances with unknown categories into the target classifier. In this study, bystrict mathematical reasoning, we reveal that the SSL error under classdistribution mismatch is composed of pseudo-labeling error and invasion error,both of which jointly bound the SSL population risk. To alleviate the SSLerror, we propose a robust SSL framework called Weight-Aware Distillation (WAD)that, by weights, selectively transfers knowledge beneficial to the target taskfrom unsupervised contrastive representation to the target classifier.Specifically, WAD captures adaptive weights and high-quality pseudo labels totarget instances by exploring point mutual information (PMI) in representationspace to maximize the role of unlabeled data and filter unknown categories.Theoretically, we prove that WAD has a tight upper bound of population riskunder class distribution mismatch. Experimentally, extensive resultsdemonstrate that WAD outperforms five state-of-the-art SSL approaches and onestandard baseline on two benchmark datasets, CIFAR10 and CIFAR100, and anartificial cross-dataset. The code is available athttps://github.com/RUC-DWBI-ML/research/tree/main/WAD-master.                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.11874v1 |
|  83 | Understanding Hessian Alignment for Domain Generalization                                                                                            | Sobhan Hemati                  | 2023-08-22     | cs.LG, cs.CV, stat.ML                   | Out-of-distribution (OOD) generalization is a critical ability for deeplearning models in many real-world scenarios including healthcare andautonomous vehicles. Recently, different techniques have been proposed toimprove OOD generalization. Among these methods, gradient-based regularizershave shown promising performance compared with other competitors. Despite thissuccess, our understanding of the role of Hessian and gradient alignment indomain generalization is still limited. To address this shortcoming, we analyzethe role of the classifier's head Hessian matrix and gradient in domaingeneralization using recent OOD theory of transferability. Theoretically, weshow that spectral norm between the classifier's head Hessian matrices acrossdomains is an upper bound of the transfer measure, a notion of distance betweentarget and source domains. Furthermore, we analyze all the attributes that getaligned when we encourage similarity between Hessians and gradients. Ouranalysis explains the success of many regularizers like CORAL, IRM, V-REx,Fish, IGA, and Fishr as they regularize part of the classifier's head Hessianand/or gradient. Finally, we propose two simple yet effective methods to matchthe classifier's head Hessians and gradients in an efficient way, based on theHessian Gradient Product (HGP) and Hutchinson's method (Hutchinson), andwithout directly calculating Hessians. We validate the OOD generalizationability of proposed methods in different scenarios, including transferability,severe correlation shift, label shift and diversity shift. Our results showthat Hessian alignment methods achieve promising performance on various OODbenchmarks. The code is available at\url{https://github.com/huawei-noah/Federated-Learning/tree/main/HessianAlignment}.                                                                                                                 | http://arxiv.org/abs/2308.11778v1 |
|  84 | Efficient Controllable Multi-Task Architectures                                                                                                      | Abhishek Aich                  | 2023-08-22     | cs.CV                                   | We aim to train a multi-task model such that users can adjust the desiredcompute budget and relative importance of task performances after deployment,without retraining. This enables optimizing performance for dynamically varyinguser needs, without heavy computational overhead to train and save models forvarious scenarios. To this end, we propose a multi-task model consisting of ashared encoder and task-specific decoders where both encoder and decoderchannel widths are slimmable. Our key idea is to control the task importance byvarying the capacities of task-specific decoders, while controlling the totalcomputational cost by jointly adjusting the encoder capacity. This improvesoverall accuracy by allowing a stronger encoder for a given budget, increasescontrol over computational cost, and delivers high-quality slimmedsub-architectures based on user's constraints. Our training strategy involves anovel 'Configuration-Invariant Knowledge Distillation' loss that enforcesbackbone representations to be invariant under different runtime widthconfigurations to enhance accuracy. Further, we present a simple but effectivesearch algorithm that translates user constraints to runtime widthconfigurations of both the shared encoder and task decoders, for sampling thesub-architectures. The key rule for the search algorithm is to provide a largercomputational budget to the higher preferred task decoder, while searching ashared encoder configuration that enhances the overall MTL performance. Variousexperiments on three multi-task benchmarks (PASCALContext, NYUDv2, andCIFAR100-MTL) with diverse backbone architectures demonstrate the advantage ofour approach. For example, our method shows a higher controllability by ~33.5%in the NYUD-v2 dataset over prior methods, while incurring much less computecost.                                                                         | http://arxiv.org/abs/2308.11744v1 |
|  85 | Delving into Motion-Aware Matching for Monocular 3D Object Tracking                                                                                  | Kuan-Chih Huang                | 2023-08-22     | cs.CV                                   | Recent advances of monocular 3D object detection facilitate the 3Dmulti-object tracking task based on low-cost camera sensors. In this paper, wefind that the motion cue of objects along different time frames is critical in3D multi-object tracking, which is less explored in existing monocular-basedapproaches. In this paper, we propose a motion-aware framework for monocular 3DMOT. To this end, we propose MoMA-M3T, a framework that mainly consists ofthree motion-aware components. First, we represent the possible movement of anobject related to all object tracklets in the feature space as its motionfeatures. Then, we further model the historical object tracklet along the timeframe in a spatial-temporal perspective via a motion transformer. Finally, wepropose a motion-aware matching module to associate historical object trackletsand current observations as final tracking results. We conduct extensiveexperiments on the nuScenes and KITTI datasets to demonstrate that our MoMA-M3Tachieves competitive performance against state-of-the-art methods. Moreover,the proposed tracker is flexible and can be easily plugged into existingimage-based 3D object detectors without re-training. Code and models areavailable at https://github.com/kuanchihhuang/MoMA-M3T.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.11607v1 |
|  86 | SPANet: Frequency-balancing Token Mixer using Spectral Pooling Aggregation Modulation                                                                | Guhnoo Yun                     | 2023-08-22     | cs.CV                                   | Recent studies show that self-attentions behave like low-pass filters (asopposed to convolutions) and enhancing their high-pass filtering capabilityimproves model performance. Contrary to this idea, we investigate existingconvolution-based models with spectral analysis and observe that improving thelow-pass filtering in convolution operations also leads to performanceimprovement. To account for this observation, we hypothesize that utilizingoptimal token mixers that capture balanced representations of both high- andlow-frequency components can enhance the performance of models. We verify thisby decomposing visual features into the frequency domain and combining them ina balanced manner. To handle this, we replace the balancing problem with a maskfiltering problem in the frequency domain. Then, we introduce a noveltoken-mixer named SPAM and leverage it to derive a MetaFormer model termed asSPANet. Experimental results show that the proposed method provides a way toachieve this balance, and the balanced representations of both high- andlow-frequency components can improve the performance of models on multiplecomputer vision tasks. Our code is available at$\href{https://doranlyong.github.io/projects/spanet/}{\text{https://doranlyong.github.io/projects/spanet/}}$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.11568v1 |
|  87 | Target-Grounded Graph-Aware Transformer for Aerial Vision-and-Dialog Navigation                                                                      | Yifei Su                       | 2023-08-22     | cs.CV                                   | This report details the methods of the winning entry of the AVDN Challenge inICCV CLVL 2023. The competition addresses the Aerial Navigation from DialogHistory (ANDH) task, which requires a drone agent to associate dialog historywith aerial observations to reach the destination. For better cross-modalgrounding abilities of the drone agent, we propose a Target-GroundedGraph-Aware Transformer (TG-GAT) framework. Concretely, TG-GAT first leveragesa graph-aware transformer to capture spatiotemporal dependency, which benefitsnavigation state tracking and robust action planning. In addition,an auxiliaryvisual grounding task is devised to boost the agent's awareness of referredlandmarks. Moreover, a hybrid augmentation strategy based on large languagemodels is utilized to mitigate data scarcity limitations. Our TG-GAT frameworkwon the AVDN Challenge, with 2.2% and 3.0% absolute improvements over thebaseline on SPL and SR metrics, respectively. The code is available athttps://github.com/yifeisu/TG-GAT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.11561v3 |
|  88 | TrackFlow: Multi-Object Tracking with Normalizing Flows                                                                                              | Gianluca Mancusi               | 2023-08-22     | cs.CV, cs.AI, cs.LG                     | The field of multi-object tracking has recently seen a renewed interest inthe good old schema of tracking-by-detection, as its simplicity and strongpriors spare it from the complex design and painful babysitting oftracking-by-attention approaches. In view of this, we aim at extendingtracking-by-detection to multi-modal settings, where a comprehensive cost hasto be computed from heterogeneous information e.g., 2D motion cues, visualappearance, and pose estimates. More precisely, we follow a case study where arough estimate of 3D information is also available and must be merged withother traditional metrics (e.g., the IoU). To achieve that, recent approachesresort to either simple rules or complex heuristics to balance the contributionof each cost. However, i) they require careful tuning of tailoredhyperparameters on a hold-out set, and ii) they imply these costs to beindependent, which does not hold in reality. We address these issues bybuilding upon an elegant probabilistic formulation, which considers the cost ofa candidate association as the negative log-likelihood yielded by a deepdensity estimator, trained to model the conditional joint probabilitydistribution of correct associations. Our experiments, conducted on bothsimulated and real benchmarks, show that our approach consistently enhances theperformance of several tracking-by-detection algorithms.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.11513v1 |
|  89 | Learning from Semantic Alignment between Unpaired Multiviews for Egocentric Video Recognition                                                        | Qitong Wang                    | 2023-08-22     | cs.CV                                   | We are concerned with a challenging scenario in unpaired multiview videolearning. In this case, the model aims to learn comprehensive multiviewrepresentations while the cross-view semantic information exhibits variations.We propose Semantics-based Unpaired Multiview Learning (SUM-L) to tackle thisunpaired multiview learning problem. The key idea is to build cross-viewpseudo-pairs and do view-invariant alignment by leveraging the semanticinformation of videos. To facilitate the data efficiency of multiview learning,we further perform video-text alignment for first-person and third-personvideos, to fully leverage the semantic knowledge to improve videorepresentations. Extensive experiments on multiple benchmark datasets verifythe effectiveness of our framework. Our method also outperforms multipleexisting view-alignment methods, under the more challenging scenario thantypical paired or unpaired multimodal or multiview learning. Our code isavailable at https://github.com/wqtwjt1996/SUM-L.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.11489v2 |
|  90 | ScanNet++: A High-Fidelity Dataset of 3D Indoor Scenes                                                                                               | Chandan Yeshwanth              | 2023-08-22     | cs.CV                                   | We present ScanNet++, a large-scale dataset that couples together capture ofhigh-quality and commodity-level geometry and color of indoor scenes. Eachscene is captured with a high-end laser scanner at sub-millimeter resolution,along with registered 33-megapixel images from a DSLR camera, and RGB-D streamsfrom an iPhone. Scene reconstructions are further annotated with an openvocabulary of semantics, with label-ambiguous scenarios explicitly annotatedfor comprehensive semantic understanding. ScanNet++ enables a new real-worldbenchmark for novel view synthesis, both from high-quality RGB capture, andimportantly also from commodity-level images, in addition to a new benchmarkfor 3D semantic scene understanding that comprehensively encapsulates diverseand ambiguous semantic labeling scenarios. Currently, ScanNet++ contains 460scenes, 280,000 captured DSLR images, and over 3.7M iPhone RGBD frames.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.11417v1 |
|  91 | How Much Temporal Long-Term Context is Needed for Action Segmentation?                                                                               | Emad Bahrami                   | 2023-08-22     | cs.CV, cs.AI, cs.LG                     | Modeling long-term context in videos is crucial for many fine-grained tasksincluding temporal action segmentation. An interesting question that is stillopen is how much long-term temporal context is needed for optimal performance.While transformers can model the long-term context of a video, this becomescomputationally prohibitive for long videos. Recent works on temporal actionsegmentation thus combine temporal convolutional networks with self-attentionsthat are computed only for a local temporal window. While these approaches showgood results, their performance is limited by their inability to capture thefull context of a video. In this work, we try to answer how much long-termtemporal context is required for temporal action segmentation by introducing atransformer-based model that leverages sparse attention to capture the fullcontext of a video. We compare our model with the current state of the art onthree datasets for temporal action segmentation, namely 50Salads, Breakfast,and Assembly101. Our experiments show that modeling the full context of a videois necessary to obtain the best performance for temporal action segmentation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.11358v1 |
|  92 | Exemplar-Free Continual Transformer with Convolutions                                                                                                | Anurag Roy                     | 2023-08-22     | cs.CV                                   | Continual Learning (CL) involves training a machine learning model in asequential manner to learn new information while retaining previously learnedtasks without the presence of previous training data. Although there has beensignificant interest in CL, most recent CL approaches in computer vision havefocused on convolutional architectures only. However, with the recent successof vision transformers, there is a need to explore their potential for CL.Although there have been some recent CL approaches for vision transformers,they either store training instances of previous tasks or require a taskidentifier during test time, which can be limiting. This paper proposes a newexemplar-free approach for class/task incremental learning called ConTraCon,which does not require task-id to be explicitly present during inference andavoids the need for storing previous training instances. The proposed approachleverages the transformer architecture and involves re-weighting the key,query, and value weights of the multi-head self-attention layers of atransformer trained on a similar task. The re-weighting is done usingconvolution, which enables the approach to maintain low parameter requirementsper task. Additionally, an image augmentation-based entropic taskidentification approach is used to predict tasks without requiring task-idsduring inference. Experiments on four benchmark datasets demonstrate that theproposed approach outperforms several competitive approaches while requiringfewer parameters.                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.11357v1 |
|  93 | CiteTracker: Correlating Image and Text for Visual Tracking                                                                                          | Xin Li                         | 2023-08-22     | cs.CV                                   | Existing visual tracking methods typically take an image patch as thereference of the target to perform tracking. However, a single image patchcannot provide a complete and precise concept of the target object as imagesare limited in their ability to abstract and can be ambiguous, which makes itdifficult to track targets with drastic variations. In this paper, we proposethe CiteTracker to enhance target modeling and inference in visual tracking byconnecting images and text. Specifically, we develop a text generation moduleto convert the target image patch into a descriptive text containing its classand attribute information, providing a comprehensive reference point for thetarget. In addition, a dynamic description module is designed to adapt totarget variations for more effective target representation. We then associatethe target description and the search image using an attention-basedcorrelation module to generate the correlated features for target statereference. Extensive experiments on five diverse datasets are conducted toevaluate the proposed algorithm and the favorable performance against thestate-of-the-art methods demonstrates the effectiveness of the proposedtracking method.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.11322v1 |
|  94 | HMD-NeMo: Online 3D Avatar Motion Generation From Sparse Observations                                                                                | Sadegh Aliakbarian             | 2023-08-22     | cs.CV                                   | Generating both plausible and accurate full body avatar motion is the key tothe quality of immersive experiences in mixed reality scenarios. Head-MountedDevices (HMDs) typically only provide a few input signals, such as head andhands 6-DoF. Recently, different approaches achieved impressive performance ingenerating full body motion given only head and hands signal. However, to thebest of our knowledge, all existing approaches rely on full hand visibility.While this is the case when, e.g., using motion controllers, a considerableproportion of mixed reality experiences do not involve motion controllers andinstead rely on egocentric hand tracking. This introduces the challenge ofpartial hand visibility owing to the restricted field of view of the HMD. Inthis paper, we propose the first unified approach, HMD-NeMo, that addressesplausible and accurate full body motion generation even when the hands may beonly partially visible. HMD-NeMo is a lightweight neural network that predictsthe full body motion in an online and real-time fashion. At the heart ofHMD-NeMo is the spatio-temporal encoder with novel temporally adaptable masktokens that encourage plausible motion in the absence of hand observations. Weperform extensive analysis of the impact of different components in HMD-NeMoand introduce a new state-of-the-art on AMASS dataset through our evaluation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.11261v1 |
|  95 | Affordance segmentation of hand-occluded containers from exocentric images                                                                           | Tommaso Apicella               | 2023-08-22     | cs.CV                                   | Visual affordance segmentation identifies the surfaces of an object an agentcan interact with. Common challenges for the identification of affordances arethe variety of the geometry and physical properties of these surfaces as wellas occlusions. In this paper, we focus on occlusions of an object that ishand-held by a person manipulating it. To address this challenge, we propose anaffordance segmentation model that uses auxiliary branches to process theobject and hand regions separately. The proposed model learns affordancefeatures under hand-occlusion by weighting the feature map through hand andobject segmentation. To train the model, we annotated the visual affordances ofan existing dataset with mixed-reality images of hand-held containers inthird-person (exocentric) images. Experiments on both real and mixed-realityimages show that our model achieves better affordance segmentation andgeneralisation than existing models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.11233v1 |
|  96 | LDP-Feat: Image Features with Local Differential Privacy                                                                                             | Francesco Pittaluga            | 2023-08-22     | cs.CV, cs.CR                            | Modern computer vision services often require users to share raw featuredescriptors with an untrusted server. This presents an inherent privacy risk,as raw descriptors may be used to recover the source images from which theywere extracted. To address this issue, researchers recently proposedprivatizing image features by embedding them within an affine subspacecontaining the original feature as well as adversarial feature samples. In thispaper, we propose two novel inversion attacks to show that it is possible to(approximately) recover the original image features from these embeddings,allowing us to recover privacy-critical image content. In light of suchsuccesses and the lack of theoretical privacy guarantees afforded by existingvisual privacy methods, we further propose the first method to privatize imagefeatures via local differential privacy, which, unlike prior approaches,provides a guaranteed bound for privacy leakage regardless of the strength ofthe attacks. In addition, our method yields strong performance in visuallocalization as a downstream task while enjoying the privacy guarantee.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.11223v1 |
|  97 | ViLLA: Fine-Grained Vision-Language Representation Learning from Real-World Data                                                                     | Maya Varma                     | 2023-08-22     | cs.CV, cs.AI                            | Vision-language models (VLMs), such as CLIP and ALIGN, are generally trainedon datasets consisting of image-caption pairs obtained from the web. However,real-world multimodal datasets, such as healthcare data, are significantly morecomplex: each image (e.g. X-ray) is often paired with text (e.g. physicianreport) that describes many distinct attributes occurring in fine-grainedregions of the image. We refer to these samples as exhibiting high pairwisecomplexity, since each image-text pair can be decomposed into a large number ofregion-attribute pairings. The extent to which VLMs can capture fine-grainedrelationships between image regions and textual attributes when trained on suchdata has not been previously evaluated. The first key contribution of this workis to demonstrate through systematic evaluations that as the pairwisecomplexity of the training dataset increases, standard VLMs struggle to learnregion-attribute relationships, exhibiting performance degradations of up to37% on retrieval tasks. In order to address this issue, we introduce ViLLA asour second key contribution. ViLLA, which is trained to capture fine-grainedregion-attribute relationships from complex datasets, involves two components:(a) a lightweight, self-supervised mapping model to decompose image-textsamples into region-attribute pairs, and (b) a contrastive VLM to learnrepresentations from generated region-attribute pairs. We demonstrate withexperiments across four domains (synthetic, product, medical, and naturalimages) that ViLLA outperforms comparable VLMs on fine-grained reasoning tasks,such as zero-shot object detection (up to 3.6 AP50 points on COCO and 0.6 mAPpoints on LVIS) and retrieval (up to 14.2 R-Precision points).                                                                                                                                                              | http://arxiv.org/abs/2308.11194v1 |
|  98 | Knowledge-Aware Prompt Tuning for Generalizable Vision-Language Models                                                                               | Baoshuo Kan                    | 2023-08-22     | cs.CV                                   | Pre-trained vision-language models, e.g., CLIP, working with manuallydesigned prompts have demonstrated great capacity of transfer learning.Recently, learnable prompts achieve state-of-the-art performance, which howeverare prone to overfit to seen classes, failing to generalize to unseen classes.In this paper, we propose a Knowledge-Aware Prompt Tuning (KAPT) framework forvision-language models. Our approach takes inspiration from human intelligencein which external knowledge is usually incorporated into recognizing novelcategories of objects. Specifically, we design two complementary types ofknowledge-aware prompts for the text encoder to leverage the distinctivecharacteristics of category-related external knowledge. The discrete promptextracts the key information from descriptions of an object category, and thelearned continuous prompt captures overall contexts. We further design anadaptation head for the visual encoder to aggregate salient attentive visualcues, which establishes discriminative and task-aware visual representations.We conduct extensive experiments on 11 widely-used benchmark datasets and theresults verify the effectiveness in few-shot image classification, especiallyin generalizing to unseen categories. Compared with the state-of-the-art CoCoOpmethod, KAPT exhibits favorable performance and achieves an absolute gain of3.22% on new classes and 2.57% in terms of harmonic mean.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.11186v1 |
|  99 | MEGA: Multimodal Alignment Aggregation and Distillation For Cinematic Video Segmentation                                                             | Najmeh Sadoughi                | 2023-08-22     | cs.CV                                   | Previous research has studied the task of segmenting cinematic videos intoscenes and into narrative acts. However, these studies have overlooked theessential task of multimodal alignment and fusion for effectively andefficiently processing long-form videos (>60min). In this paper, we introduceMultimodal alignmEnt aGgregation and distillAtion (MEGA) for cinematiclong-video segmentation. MEGA tackles the challenge by leveraging multiplemedia modalities. The method coarsely aligns inputs of variable lengths anddifferent modalities with alignment positional encoding. To maintain temporalsynchronization while reducing computation, we further introduce an enhancedbottleneck fusion layer which uses temporal alignment. Additionally, MEGAemploys a novel contrastive loss to synchronize and transfer labels acrossmodalities, enabling act segmentation from labeled synopsis sentences on videoshots. Our experimental results show that MEGA outperforms state-of-the-artmethods on MovieNet dataset for scene segmentation (with an Average Precisionimprovement of +1.19%) and on TRIPOD dataset for act segmentation (with a TotalAgreement improvement of +5.51%)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.11185v1 |
| 100 | ReFit: Recurrent Fitting Network for 3D Human Recovery                                                                                               | Yufu Wang                      | 2023-08-22     | cs.CV                                   | We present Recurrent Fitting (ReFit), a neural network architecture forsingle-image, parametric 3D human reconstruction. ReFit learns afeedback-update loop that mirrors the strategy of solving an inverse problemthrough optimization. At each iterative step, it reprojects keypoints from thehuman model to feature maps to query feedback, and uses a recurrent-basedupdater to adjust the model to fit the image better. Because ReFit encodesstrong knowledge of the inverse problem, it is faster to train than previousregression models. At the same time, ReFit improves state-of-the-artperformance on standard benchmarks. Moreover, ReFit applies to otheroptimization settings, such as multi-view fitting and single-view shapefitting. Project website: https://yufu-wang.github.io/refit_humans/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.11184v1 |
| 101 | Hierarchical Point-based Active Learning for Semi-supervised Point Cloud Semantic Segmentation                                                       | Zongyi Xu                      | 2023-08-22     | cs.CV, cs.AI                            | Impressive performance on point cloud semantic segmentation has been achievedby fully-supervised methods with large amounts of labelled data. As it islabour-intensive to acquire large-scale point cloud data with point-wiselabels, many attempts have been made to explore learning 3D point cloudsegmentation with limited annotations. Active learning is one of the effectivestrategies to achieve this purpose but is still under-explored. The most recentmethods of this kind measure the uncertainty of each pre-divided region formanual labelling but they suffer from redundant information and requireadditional efforts for region division. This paper aims at addressing thisissue by developing a hierarchical point-based active learning strategy.Specifically, we measure the uncertainty for each point by a hierarchicalminimum margin uncertainty module which considers the contextual information atmultiple levels. Then, a feature-distance suppression strategy is designed toselect important and representative points for manual labelling. Besides, tobetter exploit the unlabelled data, we build a semi-supervised segmentationframework based on our active strategy. Extensive experiments on the S3DIS andScanNetV2 datasets demonstrate that the proposed framework achieves 96.5% and100% performance of fully-supervised baseline with only 0.07% and 0.1% trainingdata, respectively, outperforming the state-of-the-art weakly-supervised andactive learning methods. The code will be available athttps://github.com/SmiletoE/HPAL.                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.11166v1 |
| 102 | Domain Generalization via Rationale Invariance                                                                                                       | Liang Chen                     | 2023-08-22     | cs.CV                                   | This paper offers a new perspective to ease the challenge of domaingeneralization, which involves maintaining robust results even in unseenenvironments. Our design focuses on the decision-making process in the finalclassifier layer. Specifically, we propose treating the element-wisecontributions to the final results as the rationale for making a decision andrepresenting the rationale for each sample as a matrix. For a well-generalizedmodel, we suggest the rationale matrices for samples belonging to the samecategory should be similar, indicating the model relies on domain-invariantclues to make decisions, thereby ensuring robust results. To implement thisidea, we introduce a rationale invariance loss as a simple regularizationtechnique, requiring only a few lines of code. Our experiments demonstrate thatthe proposed approach achieves competitive results across various datasets,despite its simplicity. Code is available at\url{https://github.com/liangchen527/RIDG}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.11158v1 |
| 103 | LAN-HDR: Luminance-based Alignment Network for High Dynamic Range Video Reconstruction                                                               | Haesoo Chung                   | 2023-08-22     | cs.CV                                   | As demands for high-quality videos continue to rise, high-resolution andhigh-dynamic range (HDR) imaging techniques are drawing attention. To generatean HDR video from low dynamic range (LDR) images, one of the critical steps isthe motion compensation between LDR frames, for which most existing worksemployed the optical flow algorithm. However, these methods suffer from flowestimation errors when saturation or complicated motions exist. In this paper,we propose an end-to-end HDR video composition framework, which aligns LDRframes in the feature space and then merges aligned features into an HDR frame,without relying on pixel-domain optical flow. Specifically, we propose aluminance-based alignment network for HDR (LAN-HDR) consisting of an alignmentmodule and a hallucination module. The alignment module aligns a frame to theadjacent reference by evaluating luminance-based attention, excluding colorinformation. The hallucination module generates sharp details, especially forwashed-out areas due to saturation. The aligned and hallucinated features arethen blended adaptively to complement each other. Finally, we merge thefeatures to generate a final HDR frame. In training, we adopt a temporal loss,in addition to frame reconstruction losses, to enhance temporal consistency andthus reduce flickering. Extensive experiments demonstrate that our methodperforms better or comparable to state-of-the-art methods on severalbenchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.11116v1 |
| 104 | Recursive Video Lane Detection                                                                                                                       | Dongkwon Jin                   | 2023-08-22     | cs.CV                                   | A novel algorithm to detect road lanes in videos, called recursive video lanedetector (RVLD), is proposed in this paper, which propagates the state of acurrent frame recursively to the next frame. RVLD consists of an intra-framelane detector (ILD) and a predictive lane detector (PLD). First, we design ILDto localize lanes in a still frame. Second, we develop PLD to exploit theinformation of the previous frame for lane detection in a current frame. Tothis end, we estimate a motion field and warp the previous output to thecurrent frame. Using the warped information, we refine the feature map of thecurrent frame to detect lanes more reliably. Experimental results show thatRVLD outperforms existing detectors on video lane datasets. Our codes areavailable at https://github.com/dongkwonjin/RVLD.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.11106v1 |
| 105 | MosaiQ: Quantum Generative Adversarial Networks for Image Generation on NISQ Computers                                                               | Daniel Silver                  | 2023-08-22     | quant-ph, cs.AR, cs.CV                  | Quantum machine learning and vision have come to the fore recently, withhardware advances enabling rapid advancement in the capabilities of quantummachines. Recently, quantum image generation has been explored with manypotential advantages over non-quantum techniques; however, previous techniqueshave suffered from poor quality and robustness. To address these problems, weintroduce, MosaiQ, a high-quality quantum image generation GAN framework thatcan be executed on today's Near-term Intermediate Scale Quantum (NISQ)computers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.11096v1 |
| 106 | Video OWL-ViT: Temporally-consistent open-world localization in video                                                                                | Georg Heigold                  | 2023-08-22     | cs.CV, cs.AI, cs.LG                     | We present an architecture and a training recipe that adapts pre-trainedopen-world image models to localization in videos. Understanding the openvisual world (without being constrained by fixed label spaces) is crucial formany real-world vision tasks. Contrastive pre-training on large image-textdatasets has recently led to significant improvements for image-level tasks.For more structured tasks involving object localization applying pre-trainedmodels is more challenging. This is particularly true for video tasks, wheretask-specific data is limited. We show successful transfer of open-world modelsby building on the OWL-ViT open-vocabulary detection model and adapting it tovideo by adding a transformer decoder. The decoder propagates objectrepresentations recurrently through time by using the output tokens for oneframe as the object queries for the next. Our model is end-to-end trainable onvideo data and enjoys improved temporal consistency compared totracking-by-detection baselines, while retaining the open-world capabilities ofthe backbone detector. We evaluate our model on the challenging TAO-OWbenchmark and demonstrate that open-world capabilities, learned fromlarge-scale image-text pre-training, can be transferred successfully toopen-world localization across diverse videos.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.11093v1 |
| 107 | Audio-Visual Class-Incremental Learning                                                                                                              | Weiguo Pian                    | 2023-08-21     | cs.CV                                   | In this paper, we introduce audio-visual class-incremental learning, aclass-incremental learning scenario for audio-visual video recognition. Wedemonstrate that joint audio-visual modeling can improve class-incrementallearning, but current methods fail to preserve semantic similarity betweenaudio and visual features as incremental step grows. Furthermore, we observethat audio-visual correlations learned in previous tasks can be forgotten asincremental steps progress, leading to poor performance. To overcome thesechallenges, we propose AV-CIL, which incorporates Dual-Audio-Visual SimilarityConstraint (D-AVSC) to maintain both instance-aware and class-aware semanticsimilarity between audio-visual modalities and Visual Attention Distillation(VAD) to retain previously learned audio-guided visual attentive ability. Wecreate three audio-visual class-incremental datasets, AVE-Class-Incremental(AVE-CI), Kinetics-Sounds-Class-Incremental (K-S-CI), andVGGSound100-Class-Incremental (VS100-CI) based on the AVE, Kinetics-Sounds, andVGGSound datasets, respectively. Our experiments on AVE-CI, K-S-CI, andVS100-CI demonstrate that AV-CIL significantly outperforms existingclass-incremental learning methods in audio-visual class-incremental learning.Code and data are available at: https://github.com/weiguoPian/AV-CIL_ICCV2023.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.11073v1 |
| 108 | TeD-SPAD: Temporal Distinctiveness for Self-supervised Privacy-preservation for video Anomaly Detection                                              | Joseph Fioresi                 | 2023-08-21     | cs.CV, cs.CR                            | Video anomaly detection (VAD) without human monitoring is a complex computervision task that can have a positive impact on society if implementedsuccessfully. While recent advances have made significant progress in solvingthis task, most existing approaches overlook a critical real-world concern:privacy. With the increasing popularity of artificial intelligencetechnologies, it becomes crucial to implement proper AI ethics into theirdevelopment. Privacy leakage in VAD allows models to pick up and amplifyunnecessary biases related to people's personal information, which may lead toundesirable decision making. In this paper, we propose TeD-SPAD, aprivacy-aware video anomaly detection framework that destroys visual privateinformation in a self-supervised manner. In particular, we propose the use of atemporally-distinct triplet loss to promote temporally discriminative features,which complements current weakly-supervised VAD methods. Using TeD-SPAD, weachieve a positive trade-off between privacy protection and utility anomalydetection performance on three popular weakly supervised VAD datasets:UCF-Crime, XD-Violence, and ShanghaiTech. Our proposed anonymization modelreduces private attribute prediction by 32.25% while only reducing frame-levelROC AUC on the UCF-Crime anomaly detection dataset by 3.69%. Project Page:https://joefioresi718.github.io/TeD-SPAD_webpage/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.11072v1 |
| 109 | UnLoc: A Unified Framework for Video Localization Tasks                                                                                              | Shen Yan                       | 2023-08-21     | cs.CV, cs.LG                            | While large-scale image-text pretrained models such as CLIP have been usedfor multiple video-level tasks on trimmed videos, their use for temporallocalization in untrimmed videos is still a relatively unexplored task. Wedesign a new approach for this called UnLoc, which uses pretrained image andtext towers, and feeds tokens to a video-text fusion model. The output of thefusion module are then used to construct a feature pyramid in which each levelconnects to a head to predict a per-frame relevancy score and start/end timedisplacements. Unlike previous works, our architecture enables MomentRetrieval, Temporal Localization, and Action Segmentation with a single stagemodel, without the need for action proposals, motion based pretrained featuresor representation masking. Unlike specialized models, we achieve state of theart results on all three different localization tasks with a unified approach.Code will be available at: \url{https://github.com/google-research/scenic}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.11062v1 |
| 110 | Coordinate Quantized Neural Implicit Representations for Multi-view Reconstruction                                                                   | Sijia Jiang                    | 2023-08-21     | cs.CV                                   | In recent years, huge progress has been made on learning neural implicitrepresentations from multi-view images for 3D reconstruction. As an additionalinput complementing coordinates, using sinusoidal functions as positionalencodings plays a key role in revealing high frequency details withcoordinate-based neural networks. However, high frequency positional encodingsmake the optimization unstable, which results in noisy reconstructions andartifacts in empty space. To resolve this issue in a general sense, weintroduce to learn neural implicit representations with quantized coordinates,which reduces the uncertainty and ambiguity in the field during optimization.Instead of continuous coordinates, we discretize continuous coordinates intodiscrete coordinates using nearest interpolation among quantized coordinateswhich are obtained by discretizing the field in an extremely high resolution.We use discrete coordinates and their positional encodings to learn implicitfunctions through volume rendering. This significantly reduces the variationsin the sample space, and triggers more multi-view consistency constraints onintersections of rays from different views, which enables to infer implicitfunction in a more effective way. Our quantized coordinates do not bring anycomputational burden, and can seamlessly work upon the latest methods. Ourevaluations under the widely used benchmarks show our superiority over thestate-of-the-art. Our code is available athttps://github.com/MachinePerceptionLab/CQ-NIR.                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.11025v1 |
| 111 | Multi-Task Hypergraphs for Semi-supervised Learning using Earth Observations                                                                         | Mihai Pirvu                    | 2023-08-21     | cs.CV, cs.LG                            | There are many ways of interpreting the world and they are highlyinterdependent. We exploit such complex dependencies and introduce a powerfulmulti-task hypergraph, in which every node is a task and different pathsthrough the hypergraph reaching a given task become unsupervised teachers, byforming ensembles that learn to generate reliable pseudolabels for that task.Each hyperedge is part of an ensemble teacher for a given task and it is also astudent of the self-supervised hypergraph system. We apply our model to one ofthe most important problems of our times, that of Earth Observation, which ishighly multi-task and it often suffers from missing ground-truth data. Byperforming extensive experiments on the NASA NEO Dataset, spanning a period of22 years, we demonstrate the value of our multi-task semi-supervised approach,by consistent improvements over strong baselines and recent work. We also showthat the hypergraph can adapt unsupervised to gradual data distribution shiftsand reliably recover, through its multi-task self-supervision process, themissing data for several observational layers for up to seven years.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.11021v1 |
| 112 | Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images                           | Tze Ho Elden Tse               | 2023-08-21     | cs.CV                                   | We propose a novel transformer-based framework that reconstructs two highfidelity hands from multi-view RGB images. Unlike existing hand pose estimationmethods, where one typically trains a deep network to regress hand modelparameters from single RGB image, we consider a more challenging problemsetting where we directly regress the absolute root poses of two-hands withextended forearm at high resolution from egocentric view. As existing datasetsare either infeasible for egocentric viewpoints or lack background variations,we create a large-scale synthetic dataset with diverse scenarios and collect areal dataset from multi-calibrated camera setup to verify our proposedmulti-view image feature fusion strategy. To make the reconstruction physicallyplausible, we propose two strategies: (i) a coarse-to-fine spectral graphconvolution decoder to smoothen the meshes during upsampling and (ii) anoptimisation-based refinement stage at inference to prevent self-penetrations.Through extensive quantitative and qualitative evaluations, we show that ourframework is able to produce realistic two-hand reconstructions and demonstratethe generalisation of synthetic-trained models to real data, as well asreal-time AR/VR applications.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.11015v1 |
| 113 | Few-Shot Physically-Aware Articulated Mesh Generation via Hierarchical Deformation                                                                   | Xueyi Liu                      | 2023-08-21     | cs.CV, cs.CG                            | We study the problem of few-shot physically-aware articulated meshgeneration. By observing an articulated object dataset containing only a fewexamples, we wish to learn a model that can generate diverse meshes with highvisual fidelity and physical validity. Previous mesh generative models eitherhave difficulties in depicting a diverse data space from only a few examples orfail to ensure physical validity of their samples. Regarding the abovechallenges, we propose two key innovations, including 1) a hierarchical meshdeformation-based generative model based upon the divide-and-conquer philosophyto alleviate the few-shot challenge by borrowing transferrable deformationpatterns from large scale rigid meshes and 2) a physics-aware deformationcorrection scheme to encourage physically plausible generations. We conductextensive experiments on 6 articulated categories to demonstrate thesuperiority of our method in generating articulated meshes with betterdiversity, higher visual fidelity, and better physical validity over previousmethods in the few-shot setting. Further, we validate solid contributions ofour two innovations in the ablation study. Project page with code is availableat https://meowuu7.github.io/few-arti-obj-gen.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.10898v1 |
| 114 | Can Language Models Learn to Listen?                                                                                                                 | Evonne Ng                      | 2023-08-21     | cs.CV                                   | We present a framework for generating appropriate facial responses from alistener in dyadic social interactions based on the speaker's words. Given aninput transcription of the speaker's words with their timestamps, our approachautoregressively predicts a response of a listener: a sequence of listenerfacial gestures, quantized using a VQ-VAE. Since gesture is a languagecomponent, we propose treating the quantized atomic motion elements asadditional language token inputs to a transformer-based large language model.Initializing our transformer with the weights of a language model pre-trainedonly on text results in significantly higher quality listener responses thantraining a transformer from scratch. We show that our generated listener motionis fluent and reflective of language semantics through quantitative metrics anda qualitative user study. In our evaluation, we analyze the model's ability toutilize temporal and semantic aspects of spoken text. Project page:https://people.eecs.berkeley.edu/~evonne_ng/projects/text2listen/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.10897v1 |
| 115 | EigenPlaces: Training Viewpoint Robust Models for Visual Place Recognition                                                                           | Gabriele Berton                | 2023-08-21     | cs.CV                                   | Visual Place Recognition is a task that aims to predict the place of an image(called query) based solely on its visual features. This is typically donethrough image retrieval, where the query is matched to the most similar imagesfrom a large database of geotagged photos, using learned global descriptors. Amajor challenge in this task is recognizing places seen from differentviewpoints. To overcome this limitation, we propose a new method, calledEigenPlaces, to train our neural network on images from different point ofviews, which embeds viewpoint robustness into the learned global descriptors.The underlying idea is to cluster the training data so as to explicitly presentthe model with different views of the same points of interest. The selection ofthis points of interest is done without the need for extra supervision. We thenpresent experiments on the most comprehensive set of datasets in literature,finding that EigenPlaces is able to outperform previous state of the art on themajority of datasets, while requiring 60\% less GPU memory for training andusing 50\% smaller descriptors. The code and trained models for EigenPlaces areavailable at {\small{\url{https://github.com/gmberton/EigenPlaces}}}, whileresults with any other baseline can be computed with the codebase at{\small{\url{https://github.com/gmberton/auto_VPR}}}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.10832v1 |
| 116 | Pixel Adaptive Deep Unfolding Transformer for Hyperspectral Image Reconstruction                                                                     | Miaoyu Li                      | 2023-08-21     | cs.CV, eess.IV                          | Hyperspectral Image (HSI) reconstruction has made gratifying progress withthe deep unfolding framework by formulating the problem into a data module anda prior module. Nevertheless, existing methods still face the problem ofinsufficient matching with HSI data. The issues lie in three aspects: 1) fixedgradient descent step in the data module while the degradation of HSI isagnostic in the pixel-level. 2) inadequate prior module for 3D HSI cube. 3)stage interaction ignoring the differences in features at different stages. Toaddress these issues, in this work, we propose a Pixel Adaptive Deep UnfoldingTransformer (PADUT) for HSI reconstruction. In the data module, a pixeladaptive descent step is employed to focus on pixel-level agnostic degradation.In the prior module, we introduce the Non-local Spectral Transformer (NST) toemphasize the 3D characteristics of HSI for recovering. Moreover, inspired bythe diverse expression of features in different stages and depths, the stageinteraction is improved by the Fast Fourier Transform (FFT). Experimentalresults on both simulated and real scenes exhibit the superior performance ofour method compared to state-of-the-art HSI reconstruction methods. The code isreleased at: https://github.com/MyuLi/PADUT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.10820v1 |
| 117 | Improving Continuous Sign Language Recognition with Cross-Lingual Signs                                                                              | Fangyun Wei                    | 2023-08-21     | cs.CV                                   | This work dedicates to continuous sign language recognition (CSLR), which isa weakly supervised task dealing with the recognition of continuous signs fromvideos, without any prior knowledge about the temporal boundaries betweenconsecutive signs. Data scarcity heavily impedes the progress of CSLR. Existingapproaches typically train CSLR models on a monolingual corpus, which is ordersof magnitude smaller than that of speech recognition. In this work, we explorethe feasibility of utilizing multilingual sign language corpora to facilitatemonolingual CSLR. Our work is built upon the observation of cross-lingualsigns, which originate from different sign languages but have similar visualsignals (e.g., hand shape and motion). The underlying idea of our approach isto identify the cross-lingual signs in one sign language and properly leveragethem as auxiliary training data to improve the recognition capability ofanother. To achieve the goal, we first build two sign language dictionariescontaining isolated signs that appear in two datasets. Then we identify thesign-to-sign mappings between two sign languages via a well-optimized isolatedsign language recognition model. At last, we train a CSLR model on thecombination of the target data with original labels and the auxiliary data withmapped labels. Experimentally, our approach achieves state-of-the-artperformance on two widely-used CSLR datasets: Phoenix-2014 and Phoenix-2014T.                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.10809v1 |
| 118 | MGMAE: Motion Guided Masking for Video Masked Autoencoding                                                                                           | Bingkun Huang                  | 2023-08-21     | cs.CV, cs.LG                            | Masked autoencoding has shown excellent performance on self-supervised videorepresentation learning. Temporal redundancy has led to a high masking ratioand customized masking strategy in VideoMAE. In this paper, we aim to furtherimprove the performance of video masked autoencoding by introducing a motionguided masking strategy. Our key insight is that motion is a general and uniqueprior in video, which should be taken into account during masked pre-training.Our motion guided masking explicitly incorporates motion information to buildtemporal consistent masking volume. Based on this masking volume, we can trackthe unmasked tokens in time and sample a set of temporal consistent cubes fromvideos. These temporal aligned unmasked tokens will further relieve theinformation leakage issue in time and encourage the MGMAE to learn more usefulstructure information. We implement our MGMAE with an online efficient opticalflow estimator and backward masking map warping strategy. We performexperiments on the datasets of Something-Something V2 and Kinetics-400,demonstrating the superior performance of our MGMAE to the original VideoMAE.In addition, we provide the visualization analysis to illustrate that our MGMAEcan sample temporal consistent cubes in a motion-adaptive manner for moreeffective video pre-training.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.10794v1 |
| 119 | On the Adversarial Robustness of Multi-Modal Foundation Models                                                                                       | Christian Schlarmann           | 2023-08-21     | cs.LG, cs.AI, cs.CR                     | Multi-modal foundation models combining vision and language models such asFlamingo or GPT-4 have recently gained enormous interest. Alignment offoundation models is used to prevent models from providing toxic or harmfuloutput. While malicious users have successfully tried to jailbreak foundationmodels, an equally important question is if honest users could be harmed bymalicious third-party content. In this paper we show that imperceivable attackson images in order to change the caption output of a multi-modal foundationmodel can be used by malicious content providers to harm honest users e.g. byguiding them to malicious websites or broadcast fake information. Thisindicates that countermeasures to adversarial attacks should be used by anydeployed multi-modal foundation model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.10741v1 |
| 120 | Vanishing Point Estimation in Uncalibrated Images with Prior Gravity Direction                                                                       | Rémi Pautrat                   | 2023-08-21     | cs.CV                                   | We tackle the problem of estimating a Manhattan frame, i.e. three orthogonalvanishing points, and the unknown focal length of the camera, leveraging aprior vertical direction. The direction can come from an Inertial MeasurementUnit that is a standard component of recent consumer devices, e.g.,smartphones. We provide an exhaustive analysis of minimal line configurationsand derive two new 2-line solvers, one of which does not suffer fromsingularities affecting existing solvers. Additionally, we design a newnon-minimal method, running on an arbitrary number of lines, to boost theperformance in local optimization. Combining all solvers in a hybrid robustestimator, our method achieves increased accuracy even with a rough prior.Experiments on synthetic and real-world datasets demonstrate the superioraccuracy of our method compared to the state of the art, while havingcomparable runtimes. We further demonstrate the applicability of our solversfor relative rotation estimation. The code is available athttps://github.com/cvg/VP-Estimation-with-Prior-Gravity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.10694v1 |
| 121 | Learning Clothing and Pose Invariant 3D Shape Representation for Long-Term Person Re-Identification                                                  | Feng Liu                       | 2023-08-21     | cs.CV                                   | Long-Term Person Re-Identification (LT-ReID) has become increasingly crucialin computer vision and biometrics. In this work, we aim to extend LT-ReIDbeyond pedestrian recognition to include a wider range of real-world humanactivities while still accounting for cloth-changing scenarios over large timegaps. This setting poses additional challenges due to the geometricmisalignment and appearance ambiguity caused by the diversity of human pose andclothing. To address these challenges, we propose a new approach 3DInvarReIDfor (i) disentangling identity from non-identity components (pose, clothingshape, and texture) of 3D clothed humans, and (ii) reconstructing accurate 3Dclothed body shapes and learning discriminative features of naked body shapesfor person ReID in a joint manner. To better evaluate our study of LT-ReID, wecollect a real-world dataset called CCDA, which contains a wide variety ofhuman activities and clothing changes. Experimentally, we show the superiorperformance of our approach for person ReID.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.10658v2 |
| 122 | Polarimetric Information for Multi-Modal 6D Pose Estimation of Photometrically Challenging Objects with Limited Data                                 | Patrick Ruhkamp                | 2023-08-21     | cs.CV                                   | 6D pose estimation pipelines that rely on RGB-only or RGB-D data showlimitations for photometrically challenging objects with e.g. texturelesssurfaces, reflections or transparency. A supervised learning-based methodutilising complementary polarisation information as input modality is proposedto overcome such limitations. This supervised approach is then extended to aself-supervised paradigm by leveraging physical characteristics of polarisedlight, thus eliminating the need for annotated real data. The methods achievesignificant advancements in pose estimation by leveraging geometric informationfrom polarised light and incorporating shape priors and invertible physicalconstraints.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.10627v1 |
| 123 | Multi-Modal Dataset Acquisition for Photometrically Challenging Object                                                                               | HyunJun Jung                   | 2023-08-21     | cs.CV                                   | This paper addresses the limitations of current datasets for 3D vision tasksin terms of accuracy, size, realism, and suitable imaging modalities forphotometrically challenging objects. We propose a novel annotation andacquisition pipeline that enhances existing 3D perception and 6D object posedatasets. Our approach integrates robotic forward-kinematics, external infraredtrackers, and improved calibration and annotation procedures. We present amulti-modal sensor rig, mounted on a robotic end-effector, and demonstrate howit is integrated into the creation of highly accurate datasets. Additionally,we introduce a freehand procedure for wider viewpoint coverage. Both approachesyield high-quality 3D data with accurate object and camera pose annotations.Our methods overcome the limitations of existing datasets and provide valuableresources for 3D vision research.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.10621v1 |
| 124 | A step towards understanding why classification helps regression                                                                                     | Silvia L. Pintea               | 2023-08-21     | cs.CV                                   | A number of computer vision deep regression approaches report improvedresults when adding a classification loss to the regression loss. Here, weexplore why this is useful in practice and when it is beneficial. To do so, westart from precisely controlled dataset variations and data samplings and findthat the effect of adding a classification loss is the most pronounced forregression with imbalanced data. We explain these empirical findings byformalizing the relation between the balanced and imbalanced regression losses.Finally, we show that our findings hold on two real imbalanced image datasetsfor depth estimation (NYUD2-DIR), and age estimation (IMDB-WIKI-DIR), and onthe problem of imbalanced video progress prediction (Breakfast). Our maintakeaway is: for a regression task, if the data sampling is imbalanced, thenadd a classification loss.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.10603v1 |
| 125 | Image-free Classifier Injection for Zero-Shot Classification                                                                                         | Anders Christensen             | 2023-08-21     | cs.CV, cs.LG                            | Zero-shot learning models achieve remarkable results on image classificationfor samples from classes that were not seen during training. However, suchmodels must be trained from scratch with specialised methods: therefore, accessto a training dataset is required when the need for zero-shot classificationarises. In this paper, we aim to equip pre-trained models with zero-shotclassification capabilities without the use of image data. We achieve this withour proposed Image-free Classifier Injection with Semantics (ICIS) that injectsclassifiers for new, unseen classes into pre-trained classification models in apost-hoc fashion without relying on image data. Instead, the existingclassifier weights and simple class-wise descriptors, such as class names orattributes, are used. ICIS has two encoder-decoder networks that learn toreconstruct classifier weights from descriptors (and vice versa), exploiting(cross-)reconstruction and cosine losses to regularise the decoding process.Notably, ICIS can be cheaply trained and applied directly on top of pre-trainedclassification models. Experiments on benchmark ZSL datasets show that ICISproduces unseen classifier weights that achieve strong (generalised) zero-shotclassification performance. Code is available athttps://github.com/ExplainableML/ImageFreeZSL .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.10599v1 |
| 126 | CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation                                                                          | Kailin Li                      | 2023-08-21     | cs.CV                                   | In daily life, humans utilize hands to manipulate objects. Modeling the shapeof objects that are manipulated by the hand is essential for AI to comprehenddaily tasks and to learn manipulation skills. However, previous approaches haveencountered difficulties in reconstructing the precise shapes of hand-heldobjects, primarily owing to a deficiency in prior shape knowledge andinadequate data for training. As illustrated, given a particular type of tool,such as a mug, despite its infinite variations in shape and appearance, humanshave a limited number of 'effective' modes and poses for its manipulation. Thiscan be attributed to the fact that humans have mastered the shape prior of the'mug' category, and can quickly establish the corresponding relations betweendifferent mug instances and the prior, such as where the rim and handle arelocated. In light of this, we propose a new method, CHORD, for Category-levelHand-held Object Reconstruction via shape Deformation. CHORD deforms acategorical shape prior for reconstructing the intra-class objects. To ensureaccurate reconstruction, we empower CHORD with three types of awareness:appearance, shape, and interacting pose. In addition, we have constructed a newdataset, COMIC, of category-level hand-object interaction. COMIC contains arich array of object instances, materials, hand interactions, and viewingdirections. Extensive evaluation shows that CHORD outperforms state-of-the-artapproaches in both quantitative and qualitative measures. Code, model, anddatasets are available at https://kailinli.github.io/CHORD.                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.10574v1 |
| 127 | Self-Feedback DETR for Temporal Action Detection                                                                                                     | Jihwan Kim                     | 2023-08-21     | cs.CV                                   | Temporal Action Detection (TAD) is challenging but fundamental for real-worldvideo applications. Recently, DETR-based models have been devised for TAD buthave not performed well yet. In this paper, we point out the problem in theself-attention of DETR for TAD; the attention modules focus on a few keyelements, called temporal collapse problem. It degrades the capability of theencoder and decoder since their self-attention modules play no role. To solvethe problem, we propose a novel framework, Self-DETR, which utilizescross-attention maps of the decoder to reactivate self-attention modules. Werecover the relationship between encoder features by simple matrixmultiplication of the cross-attention map and its transpose. Likewise, we alsoget the information within decoder queries. By guiding collapsed self-attentionmaps with the guidance map calculated, we settle down the temporal collapse ofself-attention modules in the encoder and decoder. Our extensive experimentsdemonstrate that Self-DETR resolves the temporal collapse problem by keepinghigh diversity of attention over all layers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.10570v1 |
| 128 | Improving Diversity in Zero-Shot GAN Adaptation with Semantic Variations                                                                             | Seogkyu Jeon                   | 2023-08-21     | cs.CV                                   | Training deep generative models usually requires a large amount of data. Toalleviate the data collection cost, the task of zero-shot GAN adaptation aimsto reuse well-trained generators to synthesize images of an unseen targetdomain without any further training samples. Due to the data absence, thetextual description of the target domain and the vision-language models, e.g.,CLIP, are utilized to effectively guide the generator. However, with only asingle representative text feature instead of real images, the synthesizedimages gradually lose diversity as the model is optimized, which is also knownas mode collapse. To tackle the problem, we propose a novel method to findsemantic variations of the target text in the CLIP space. Specifically, weexplore diverse semantic variations based on the informative text feature ofthe target domain while regularizing the uncontrolled deviation of the semanticinformation. With the obtained variations, we design a novel directional momentloss that matches the first and second moments of image and text directiondistributions. Moreover, we introduce elastic weight consolidation and arelation consistency loss to effectively preserve valuable content informationfrom the source domain, e.g., appearances. Through extensive experiments, wedemonstrate the efficacy of the proposed methods in ensuring sample diversityin various scenarios of zero-shot GAN adaptation. We also conduct ablationstudies to validate the effect of each proposed component. Notably, our modelachieves a new state-of-the-art on zero-shot GAN adaptation in terms of bothdiversity and quality.                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.10554v1 |
| 129 | QD-BEV : Quantization-aware View-guided Distillation for Multi-view 3D Object Detection                                                              | Yifan Zhang                    | 2023-08-21     | cs.CV                                   | Multi-view 3D detection based on BEV (bird-eye-view) has recently achievedsignificant improvements. However, the huge memory consumption ofstate-of-the-art models makes it hard to deploy them on vehicles, and thenon-trivial latency will affect the real-time perception of streamingapplications. Despite the wide application of quantization to lighten models,we show in our paper that directly applying quantization in BEV tasks will 1)make the training unstable, and 2) lead to intolerable performance degradation.To solve these issues, our method QD-BEV enables a novel view-guideddistillation (VGD) objective, which can stabilize the quantization-awaretraining (QAT) while enhancing the model performance by leveraging both imagefeatures and BEV features. Our experiments show that QD-BEV achieves similar oreven better accuracy than previous methods with significant efficiency gains.On the nuScenes datasets, the 4-bit weight and 6-bit activation quantizedQD-BEV-Tiny model achieves 37.2% NDS with only 15.8 MB model size,outperforming BevFormer-Tiny by 1.8% with an 8x model compression. On the Smalland Base variants, QD-BEV models also perform superbly and achieve 47.9% NDS(28.2 MB) and 50.9% NDS (32.9 MB), respectively.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.10515v1 |
| 130 | SynDrone -- Multi-modal UAV Dataset for Urban Scenarios                                                                                              | Giulia Rizzoli                 | 2023-08-21     | cs.CV                                   | The development of computer vision algorithms for Unmanned Aerial Vehicles(UAVs) imagery heavily relies on the availability of annotated high-resolutionaerial data. However, the scarcity of large-scale real datasets withpixel-level annotations poses a significant challenge to researchers as thelimited number of images in existing datasets hinders the effectiveness of deeplearning models that require a large amount of training data. In this paper, wepropose a multimodal synthetic dataset containing both images and 3D data takenat multiple flying heights to address these limitations. In addition toobject-level annotations, the provided data also include pixel-level labelingin 28 classes, enabling exploration of the potential advantages in tasks likesemantic segmentation. In total, our dataset contains 72k labeled samples thatallow for effective training of deep architectures showing promising results insynthetic-to-real adaptation. The dataset will be made publicly available tosupport the development of novel computer vision methods targeting UAVapplications.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.10491v1 |
| 131 | Texture Generation on 3D Meshes with Point-UV Diffusion                                                                                              | Xin Yu                         | 2023-08-21     | cs.CV, cs.AI, cs.GR                     | In this work, we focus on synthesizing high-quality textures on 3D meshes. Wepresent Point-UV diffusion, a coarse-to-fine pipeline that marries thedenoising diffusion model with UV mapping to generate 3D consistent andhigh-quality texture images in UV space. We start with introducing a pointdiffusion model to synthesize low-frequency texture components with ourtailored style guidance to tackle the biased color distribution. The derivedcoarse texture offers global consistency and serves as a condition for thesubsequent UV diffusion stage, aiding in regularizing the model to generate a3D consistent UV texture image. Then, a UV diffusion model with hybridconditions is developed to enhance the texture fidelity in the 2D UV space. Ourmethod can process meshes of any genus, generating diversified,geometry-compatible, and high-fidelity textures. Code is available athttps://cvmi-lab.github.io/Point-UV-Diffusion                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.10490v1 |
| 132 | Enhancing Medical Image Segmentation: Optimizing Cross-Entropy Weights and Post-Processing with Autoencoders                                         | Pranav Singh                   | 2023-08-21     | eess.IV, cs.CV                          | The task of medical image segmentation presents unique challenges,necessitating both localized and holistic semantic understanding to accuratelydelineate areas of interest, such as critical tissues or aberrant features.This complexity is heightened in medical image segmentation due to the highdegree of inter-class similarities, intra-class variations, and possible imageobfuscation. The segmentation task further diversifies when considering thestudy of histopathology slides for autoimmune diseases like dermatomyositis.The analysis of cell inflammation and interaction in these cases has been lessstudied due to constraints in data acquisition pipelines. Despite theprogressive strides in medical science, we lack a comprehensive collection ofautoimmune diseases. As autoimmune diseases globally escalate in prevalence andexhibit associations with COVID-19, their study becomes increasingly essential.While there is existing research that integrates artificial intelligence in theanalysis of various autoimmune diseases, the exploration of dermatomyositisremains relatively underrepresented. In this paper, we present a deep-learningapproach tailored for Medical image segmentation. Our proposed methodoutperforms the current state-of-the-art techniques by an average of 12.26% forU-Net and 12.04% for U-Net++ across the ResNet family of encoders on thedermatomyositis dataset. Furthermore, we probe the importance of optimizingloss function weights and benchmark our methodology on three challengingmedical image segmentation tasks                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.10488v1 |
| 133 | Privacy-Preserving Face Recognition Using Random Frequency Components                                                                                | Yuxi Mi                        | 2023-08-21     | cs.CV                                   | The ubiquitous use of face recognition has sparked increasing privacyconcerns, as unauthorized access to sensitive face images could compromise theinformation of individuals. This paper presents an in-depth study of theprivacy protection of face images' visual information and against recovery.Drawing on the perceptual disparity between humans and models, we propose toconceal visual information by pruning human-perceivable low-frequencycomponents. For impeding recovery, we first elucidate the seeming paradoxbetween reducing model-exploitable information and retaining high recognitionaccuracy. Based on recent theoretical insights and our observation on modelattention, we propose a solution to the dilemma, by advocating for the trainingand inference of recognition models on randomly selected frequency components.We distill our findings into a novel privacy-preserving face recognitionmethod, PartialFace. Extensive experiments demonstrate that PartialFaceeffectively balances privacy protection goals and recognition accuracy. Code isavailable at: https://github.com/Tencent/TFace.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.10461v1 |
| 134 | Explore and Tell: Embodied Visual Captioning in 3D Environments                                                                                      | Anwen Hu                       | 2023-08-21     | cs.CV                                   | While current visual captioning models have achieved impressive performance,they often assume that the image is well-captured and provides a complete viewof the scene. In real-world scenarios, however, a single image may not offer agood viewpoint, hindering fine-grained scene understanding. To overcome thislimitation, we propose a novel task called Embodied Captioning, which equipsvisual captioning models with navigation capabilities, enabling them toactively explore the scene and reduce visual ambiguity from suboptimalviewpoints. Specifically, starting at a random viewpoint, an agent mustnavigate the environment to gather information from different viewpoints andgenerate a comprehensive paragraph describing all objects in the scene. Tosupport this task, we build the ET-Cap dataset with Kubric simulator,consisting of 10K 3D scenes with cluttered objects and three annotatedparagraphs per scene. We propose a Cascade Embodied Captioning model (CaBOT),which comprises of a navigator and a captioner, to tackle this task. Thenavigator predicts which actions to take in the environment, while thecaptioner generates a paragraph description based on the whole navigationtrajectory. Extensive experiments demonstrate that our model outperforms othercarefully designed baselines. Our dataset, codes and models are available athttps://aim3-ruc.github.io/ExploreAndTell.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.10447v1 |
| 135 | When Prompt-based Incremental Learning Does Not Meet Strong Pretraining                                                                              | Yu-Ming Tang                   | 2023-08-21     | cs.CV                                   | Incremental learning aims to overcome catastrophic forgetting when learningdeep networks from sequential tasks. With impressive learning efficiency andperformance, prompt-based methods adopt a fixed backbone to sequential tasks bylearning task-specific prompts. However, existing prompt-based methods heavilyrely on strong pretraining (typically trained on ImageNet-21k), and we findthat their models could be trapped if the potential gap between the pretrainingtask and unknown future tasks is large. In this work, we develop a learnableAdaptive Prompt Generator (APG). The key is to unify the prompt retrieval andprompt learning processes into a learnable prompt generator. Hence, the wholeprompting process can be optimized to reduce the negative effects of the gapbetween tasks effectively. To make our APG avoid learning ineffectiveknowledge, we maintain a knowledge pool to regularize APG with the featuredistribution of each class. Extensive experiments show that our methodsignificantly outperforms advanced methods in exemplar-free incrementallearning without (strong) pretraining. Besides, under strong retraining, ourmethod also has comparable performance to existing prompt-based models, showingthat our method can still benefit from pretraining. Codes can be found athttps://github.com/TOM-tym/APG                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.10445v1 |
| 136 | X-VoE: Measuring eXplanatory Violation of Expectation in Physical Events                                                                             | Bo Dai                         | 2023-08-21     | cs.AI, cs.CV                            | Intuitive physics is pivotal for human understanding of the physical world,enabling prediction and interpretation of events even in infancy. Nonetheless,replicating this level of intuitive physics in artificial intelligence (AI)remains a formidable challenge. This study introduces X-VoE, a comprehensivebenchmark dataset, to assess AI agents' grasp of intuitive physics. Built onthe developmental psychology-rooted Violation of Expectation (VoE) paradigm,X-VoE establishes a higher bar for the explanatory capacities of intuitivephysics models. Each VoE scenario within X-VoE encompasses three distinctsettings, probing models' comprehension of events and their underlyingexplanations. Beyond model evaluation, we present an explanation-based learningsystem that captures physics dynamics and infers occluded object states solelyfrom visual sequences, without explicit occlusion labels. Experimental outcomeshighlight our model's alignment with human commonsense when tested againstX-VoE. A remarkable feature is our model's ability to visually expound VoEevents by reconstructing concealed scenes. Concluding, we discuss the findings'implications and outline future research directions. Through X-VoE, we catalyzethe advancement of AI endowed with human-like intuitive physics capabilities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.10441v1 |
| 137 | Diffusion Model as Representation Learner                                                                                                            | Xingyi Yang                    | 2023-08-21     | cs.CV, cs.AI                            | Diffusion Probabilistic Models (DPMs) have recently demonstrated impressiveresults on various generative tasks.Despite its promises, the learnedrepresentations of pre-trained DPMs, however, have not been fully understood.In this paper, we conduct an in-depth investigation of the representation powerof DPMs, and propose a novel knowledge transfer method that leverages theknowledge acquired by generative DPMs for recognition tasks. Our study beginsby examining the feature space of DPMs, revealing that DPMs are inherentlydenoising autoencoders that balance the representation learning withregularizing model capacity. To this end, we introduce a novel knowledgetransfer paradigm named RepFusion. Our paradigm extracts representations atdifferent time steps from off-the-shelf DPMs and dynamically employs them assupervision for student networks, in which the optimal time is determinedthrough reinforcement learning. We evaluate our approach on several imageclassification, semantic segmentation, and landmark detection benchmarks, anddemonstrate that it outperforms state-of-the-art methods. Our results uncoverthe potential of DPMs as a powerful tool for representation learning andprovide insights into the usefulness of generative models beyond samplegeneration. The code is available at\url{https://github.com/Adamdad/Repfusion}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.10916v1 |
| 138 | Simple Baselines for Interactive Video Retrieval with Questions and Answers                                                                          | Kaiqu Liang                    | 2023-08-21     | cs.CV, cs.AI, cs.CL, cs.HC              | To date, the majority of video retrieval systems have been optimized for a"single-shot" scenario in which the user submits a query in isolation, ignoringprevious interactions with the system. Recently, there has been renewedinterest in interactive systems to enhance retrieval, but existing approachesare complex and deliver limited gains in performance. In this work, we revisitthis topic and propose several simple yet effective baselines for interactivevideo retrieval via question-answering. We employ a VideoQA model to simulateuser interactions and show that this enables the productive study of theinteractive retrieval task without access to ground truth dialogue data.Experiments on MSR-VTT, MSVD, and AVSD show that our framework usingquestion-based interaction significantly improves the performance of text-basedvideo retrieval systems.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.10402v1 |
| 139 | Strata-NeRF : Neural Radiance Fields for Stratified Scenes                                                                                           | Ankit Dhiman                   | 2023-08-20     | cs.CV                                   | Neural Radiance Field (NeRF) approaches learn the underlying 3Drepresentation of a scene and generate photo-realistic novel views with highfidelity. However, most proposed settings concentrate on modelling a singleobject or a single level of a scene. However, in the real world, we may capturea scene at multiple levels, resulting in a layered capture. For example,tourists usually capture a monument's exterior structure before capturing theinner structure. Modelling such scenes in 3D with seamless switching betweenlevels can drastically improve immersive experiences. However, most existingtechniques struggle in modelling such scenes. We propose Strata-NeRF, a singleneural radiance field that implicitly captures a scene with multiple levels.Strata-NeRF achieves this by conditioning the NeRFs on Vector Quantized (VQ)latent representations which allow sudden changes in scene structure. Weevaluate the effectiveness of our approach in multi-layered synthetic datasetcomprising diverse scenes and then further validate its generalization on thereal-world RealEstate10K dataset. We find that Strata-NeRF effectively capturesstratified scenes, minimizes artifacts, and synthesizes high-fidelity viewscompared to existing approaches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.10337v1 |
| 140 | Coordinate Transformer: Achieving Single-stage Multi-person Mesh Recovery from Videos                                                                | Haoyuan Li                     | 2023-08-20     | cs.CV                                   | Multi-person 3D mesh recovery from videos is a critical first step towardsautomatic perception of group behavior in virtual reality, physical therapy andbeyond. However, existing approaches rely on multi-stage paradigms, where theperson detection and tracking stages are performed in a multi-person setting,while temporal dynamics are only modeled for one person at a time.Consequently, their performance is severely limited by the lack of inter-personinteractions in the spatial-temporal mesh recovery, as well as by detection andtracking defects. To address these challenges, we propose the CoordinatetransFormer (CoordFormer) that directly models multi-person spatial-temporalrelations and simultaneously performs multi-mesh recovery in an end-to-endmanner. Instead of partitioning the feature map into coarse-scale patch-wisetokens, CoordFormer leverages a novel Coordinate-Aware Attention to preservepixel-level spatial-temporal coordinate information. Additionally, we propose asimple, yet effective Body Center Attention mechanism to fuse positioninformation. Extensive experiments on the 3DPW dataset demonstrate thatCoordFormer significantly improves the state-of-the-art, outperforming thepreviously best results by 4.2%, 8.8% and 4.7% according to the MPJPE, PAMPJPE,and PVE metrics, respectively, while being 40% faster than recent video-basedapproaches. The released code can be found athttps://github.com/Li-Hao-yuan/CoordFormer.                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.10334v1 |
| 141 | A Comprehensive Empirical Evaluation on Online Continual Learning                                                                                    | Albin Soutif--Cormerais        | 2023-08-20     | cs.LG                                   | Online continual learning aims to get closer to a live learning experience bylearning directly on a stream of data with temporally shifting distribution andby storing a minimum amount of data from that stream. In this empiricalevaluation, we evaluate various methods from the literature that tackle onlinecontinual learning. More specifically, we focus on the class-incrementalsetting in the context of image classification, where the learner must learnnew classes incrementally from a stream of data. We compare these methods onthe Split-CIFAR100 and Split-TinyImagenet benchmarks, and measure their averageaccuracy, forgetting, stability, and quality of the representations, toevaluate various aspects of the algorithm at the end but also during the wholetraining period. We find that most methods suffer from stability andunderfitting issues. However, the learned representations are comparable toi.i.d. training under the same computational budget. No clear winner emergesfrom the results and basic experience replay, when properly tuned andimplemented, is a very strong baseline. We release our modular and extensiblecodebase at https://github.com/AlbinSou/ocl_survey based on the avalancheframework to reproduce our results and encourage future research.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.10328v1 |
| 142 | Improving Adversarial Robustness of Masked Autoencoders via Test-time Frequency-domain Prompting                                                     | Qidong Huang                   | 2023-08-20     | cs.CV                                   | In this paper, we investigate the adversarial robustness of visiontransformers that are equipped with BERT pretraining (e.g., BEiT, MAE). Asurprising observation is that MAE has significantly worse adversarialrobustness than other BERT pretraining methods. This observation drives us torethink the basic differences between these BERT pretraining methods and howthese differences affect the robustness against adversarial perturbations. Ourempirical analysis reveals that the adversarial robustness of BERT pretrainingis highly related to the reconstruction target, i.e., predicting the raw pixelsof masked image patches will degrade more adversarial robustness of the modelthan predicting the semantic context, since it guides the model to concentratemore on medium-/high-frequency components of images. Based on our analysis, weprovide a simple yet effective way to boost the adversarial robustness of MAE.The basic idea is using the dataset-extracted domain knowledge to occupy themedium-/high-frequency of images, thus narrowing the optimization space ofadversarial perturbations. Specifically, we group the distribution ofpretraining data and optimize a set of cluster-specific visual prompts onfrequency domain. These prompts are incorporated with input images throughprototype-based prompt selection during test period. Extensive evaluation showsthat our method clearly boost MAE's adversarial robustness while maintainingits clean performance on ImageNet-1k classification. Our code is available at:https://github.com/shikiw/RobustMAE.                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.10315v2 |
| 143 | DVGaze: Dual-View Gaze Estimation                                                                                                                    | Yihua Cheng                    | 2023-08-20     | cs.CV                                   | Gaze estimation methods estimate gaze from facial appearance with a singlecamera. However, due to the limited view of a single camera, the capturedfacial appearance cannot provide complete facial information and thuscomplicate the gaze estimation problem. Recently, camera devices are rapidlyupdated. Dual cameras are affordable for users and have been integrated in manydevices. This development suggests that we can further improve gaze estimationperformance with dual-view gaze estimation. In this paper, we propose adual-view gaze estimation network (DV-Gaze). DV-Gaze estimates dual-view gazedirections from a pair of images. We first propose a dual-view interactiveconvolution (DIC) block in DV-Gaze. DIC blocks exchange dual-view informationduring convolution in multiple feature scales. It fuses dual-view featuresalong epipolar lines and compensates for the original feature with the fusedfeature. We further propose a dual-view transformer to estimate gaze fromdual-view features. Camera poses are encoded to indicate the positioninformation in the transformer. We also consider the geometric relation betweendual-view gaze directions and propose a dual-view gaze consistency loss forDV-Gaze. DV-Gaze achieves state-of-the-art performance on ETH-XGaze and EVEdatasets. Our experiments also prove the potential of dual-view gazeestimation. We release codes in https://github.com/yihuacheng/DVGaze.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.10310v1 |
| 144 | Omnidirectional Information Gathering for Knowledge Transfer-based Audio-Visual Navigation                                                           | Jinyu Chen                     | 2023-08-20     | cs.CV                                   | Audio-visual navigation is an audio-targeted wayfinding task where a robotagent is entailed to travel a never-before-seen 3D environment towards thesounding source. In this article, we present ORAN, an omnidirectionalaudio-visual navigator based on cross-task navigation skill transfer. Inparticular, ORAN sharpens its two basic abilities for a such challenging task,namely wayfinding and audio-visual information gathering. First, ORAN istrained with a confidence-aware cross-task policy distillation (CCPD) strategy.CCPD transfers the fundamental, point-to-point wayfinding skill that is welltrained on the large-scale PointGoal task to ORAN, so as to help ORAN to bettermaster audio-visual navigation with far fewer training samples. To improve theefficiency of knowledge transfer and address the domain gap, CCPD is made to beadaptive to the decision confidence of the teacher policy. Second, ORAN isequipped with an omnidirectional information gathering (OIG) mechanism, i.e.,gleaning visual-acoustic observations from different directions beforedecision-making. As a result, ORAN yields more robust navigation behaviour.Taking CCPD and OIG together, ORAN significantly outperforms previouscompetitors. After the model ensemble, we got 1st in Soundspaces Challenge2022, improving SPL and SR by 53% and 35% relatively.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.10306v1 |
| 145 | Co-Evolution of Pose and Mesh for 3D Human Body Estimation from Video                                                                                | Yingxuan You                   | 2023-08-20     | cs.CV, cs.AI, cs.LG                     | Despite significant progress in single image-based 3D human mesh recovery,accurately and smoothly recovering 3D human motion from a video remainschallenging. Existing video-based methods generally recover human mesh byestimating the complex pose and shape parameters from coupled image features,whose high complexity and low representation ability often result ininconsistent pose motion and limited shape patterns. To alleviate this issue,we introduce 3D pose as the intermediary and propose a Pose and MeshCo-Evolution network (PMCE) that decouples this task into two parts: 1)video-based 3D human pose estimation and 2) mesh vertices regression from theestimated 3D pose and temporal image feature. Specifically, we propose atwo-stream encoder that estimates mid-frame 3D pose and extracts a temporalimage feature from the input image sequence. In addition, we design aco-evolution decoder that performs pose and mesh interactions with theimage-guided Adaptive Layer Normalization (AdaLN) to make pose and mesh fit thehuman body shape. Extensive experiments demonstrate that the proposed PMCEoutperforms previous state-of-the-art methods in terms of both per-frameaccuracy and temporal consistency on three benchmark datasets: 3DPW, Human3.6M,and MPI-INF-3DHP. Our code is available at https://github.com/kasvii/PMCE.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.10305v1 |
| 146 | Neural Interactive Keypoint Detection                                                                                                                | Jie Yang                       | 2023-08-20     | cs.CV                                   | This work proposes an end-to-end neural interactive keypoint detectionframework named Click-Pose, which can significantly reduce more than 10 timeslabeling costs of 2D keypoint annotation compared with manual-only annotation.Click-Pose explores how user feedback can cooperate with a neural keypointdetector to correct the predicted keypoints in an interactive way for a fasterand more effective annotation process. Specifically, we design the pose errormodeling strategy that inputs the ground truth pose combined with four typicalpose errors into the decoder and trains the model to reconstruct the correctposes, which enhances the self-correction ability of the model. Then, we attachan interactive human-feedback loop that allows receiving users' clicks tocorrect one or several predicted keypoints and iteratively utilizes the decoderto update all other keypoints with a minimum number of clicks (NoC) forefficient annotation. We validate Click-Pose in in-domain, out-of-domainscenes, and a new task of keypoint adaptation. For annotation, Click-Pose onlyneeds 1.97 and 6.45 NoC@95 (at precision 95%) on COCO and Human-Art, reducing31.4% and 36.3% efforts than the SOTA model (ViTPose) with manual correction,respectively. Besides, without user clicks, Click-Pose surpasses the previousend-to-end model by 1.4 AP on COCO and 3.0 AP on Human-Art. The code isavailable at https://github.com/IDEA-Research/Click-Pose.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.10174v1 |
| 147 | VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation                                                                   | Yanyuan Qiao                   | 2023-08-20     | cs.CV                                   | The performance of the Vision-and-Language Navigation~(VLN) tasks haswitnessed rapid progress recently thanks to the use of large pre-trainedvision-and-language models. However, full fine-tuning the pre-trained model forevery downstream VLN task is becoming costly due to the considerable modelsize. Recent research hotspot of Parameter-Efficient Transfer Learning (PETL)shows great potential in efficiently tuning large pre-trained models for thecommon CV and NLP tasks, which exploits the most of the representationknowledge implied in the pre-trained model while only tunes a minimal set ofparameters. However, simply utilizing existing PETL methods for the morechallenging VLN tasks may bring non-trivial degeneration to the performance.Therefore, we present the first study to explore PETL methods for VLN tasks andpropose a VLN-specific PETL method named VLN-PETL. Specifically, we design twoPETL modules: Historical Interaction Booster (HIB) and Cross-modal InteractionBooster (CIB). Then we combine these two modules with several existing PETLmethods as the integrated VLN-PETL. Extensive experimental results on fourmainstream VLN tasks (R2R, REVERIE, NDH, RxR) demonstrate the effectiveness ofour proposed VLN-PETL, where VLN-PETL achieves comparable or even betterperformance to full fine-tuning and outperforms other PETL methods withpromising margins.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.10172v1 |
| 148 | FashionNTM: Multi-turn Fashion Image Retrieval via Cascaded Memory                                                                                   | Anwesan Pal                    | 2023-08-20     | cs.CV, cs.CL                            | Multi-turn textual feedback-based fashion image retrieval focuses on areal-world setting, where users can iteratively provide information to refineretrieval results until they find an item that fits all their requirements. Inthis work, we present a novel memory-based method, called FashionNTM, for sucha multi-turn system. Our framework incorporates a new Cascaded Memory NeuralTuring Machine (CM-NTM) approach for implicit state management, therebylearning to integrate information across all past turns to retrieve new images,for a given turn. Unlike vanilla Neural Turing Machine (NTM), our CM-NTMoperates on multiple inputs, which interact with their respective memories viaindividual read and write heads, to learn complex relationships. Extensiveevaluation results show that our proposed method outperforms the previousstate-of-the-art algorithm by 50.5%, on Multi-turn FashionIQ -- the onlyexisting multi-turn fashion dataset currently, in addition to having a relativeimprovement of 12.6% on Multi-turn Shoes -- an extension of the single-turnShoes dataset that we created in this work. Further analysis of the model in areal-world interactive setting demonstrates two important capabilities of ourmodel -- memory retention across turns, and agnosticity to turn order fornon-contradictory feedback. Finally, user study results show that imagesretrieved by FashionNTM were favored by 83.1% over other multi-turn models.Project page: https://sites.google.com/eng.ucsd.edu/fashionntm                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.10170v1 |
| 149 | Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection                                                    | Guodong Wang                   | 2023-08-20     | cs.CV                                   | Anomaly detection (AD), aiming to find samples that deviate from the trainingdistribution, is essential in safety-critical applications. Though recentself-supervised learning based attempts achieve promising results by creatingvirtual outliers, their training objectives are less faithful to AD whichrequires a concentrated inlier distribution as well as a dispersive outlierdistribution. In this paper, we propose Unilaterally Aggregated ContrastiveLearning with Hierarchical Augmentation (UniCon-HA), taking into account boththe requirements above. Specifically, we explicitly encourage the concentrationof inliers and the dispersion of virtual outliers via supervised andunsupervised contrastive losses, respectively. Considering that standardcontrastive data augmentation for generating positive views may induceoutliers, we additionally introduce a soft mechanism to re-weight eachaugmented inlier according to its deviation from the inlier distribution, toensure a purified concentration. Moreover, to prompt a higher concentration,inspired by curriculum learning, we adopt an easy-to-hard hierarchicalaugmentation strategy and perform contrastive aggregation at different depthsof the network based on the strengths of data augmentation. Our method isevaluated under three AD settings including unlabeled one-class, unlabeledmulti-class, and labeled multi-class, demonstrating its consistent superiorityover other competitors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.10155v1 |
| 150 | ESTextSpotter: Towards Better Scene Text Spotting with Explicit Synergy in Transformer                                                               | Mingxin Huang                  | 2023-08-20     | cs.CV                                   | In recent years, end-to-end scene text spotting approaches are evolving tothe Transformer-based framework. While previous studies have shown the crucialimportance of the intrinsic synergy between text detection and recognition,recent advances in Transformer-based methods usually adopt an implicit synergystrategy with shared query, which can not fully realize the potential of thesetwo interactive tasks. In this paper, we argue that the explicit synergyconsidering distinct characteristics of text detection and recognition cansignificantly improve the performance text spotting. To this end, we introducea new model named Explicit Synergy-based Text Spotting Transformer framework(ESTextSpotter), which achieves explicit synergy by modeling discriminative andinteractive features for text detection and recognition within a singledecoder. Specifically, we decompose the conventional shared query intotask-aware queries for text polygon and content, respectively. Through thedecoder with the proposed vision-language communication module, the queriesinteract with each other in an explicit manner while preserving discriminativepatterns of text detection and recognition, thus improving performancesignificantly. Additionally, we propose a task-aware query initializationscheme to ensure stable training. Experimental results demonstrate that ourmodel significantly outperforms previous state-of-the-art methods. Code isavailable at https://github.com/mxin262/ESTextSpotter.                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.10147v1 |
| 151 | OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision                                                                                  | Shujie Zhang                   | 2023-08-20     | cs.CV, cs.LG                            | Hand Pose Estimation (HPE) is crucial to many applications, but conventionalcameras-based CM-HPE methods are completely subject to Line-of-Sight (LoS), ascameras cannot capture occluded objects. In this paper, we propose to exploitRadio-Frequency-Vision (RF-vision) capable of bypassing obstacles for achievingoccluded HPE, and we introduce OCHID-Fi as the first RF-HPE method with 3D poseestimation capability. OCHID-Fi employs wideband RF sensors widely available onsmart devices (e.g., iPhones) to probe 3D human hand pose and extract theirskeletons behind obstacles. To overcome the challenge in labeling RF imaginggiven its human incomprehensible nature, OCHID-Fi employs a cross-modality andcross-domain training process. It uses a pre-trained CM-HPE network and asynchronized CM/RF dataset, to guide the training of its complex-valued RF-HPEnetwork under LoS conditions. It further transfers knowledge learned fromlabeled LoS domain to unlabeled occluded domain via adversarial learning,enabling OCHID-Fi to generalize to unseen occluded scenarios. Experimentalresults demonstrate the superiority of OCHID-Fi: it achieves comparableaccuracy to CM-HPE under normal conditions while maintaining such accuracy evenin occluded scenarios, with empirical evidence for its generalizability to newdomains.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.10146v1 |
| 152 | March in Chat: Interactive Prompting for Remote Embodied Referring Expression                                                                        | Yanyuan Qiao                   | 2023-08-20     | cs.CV                                   | Many Vision-and-Language Navigation (VLN) tasks have been proposed in recentyears, from room-based to object-based and indoor to outdoor. The REVERIE(Remote Embodied Referring Expression) is interesting since it only provideshigh-level instructions to the agent, which are closer to human commands inpractice. Nevertheless, this poses more challenges than other VLN tasks sinceit requires agents to infer a navigation plan only based on a shortinstruction. Large Language Models (LLMs) show great potential in robot actionplanning by providing proper prompts. Still, this strategy has not beenexplored under the REVERIE settings. There are several new challenges. Forexample, the LLM should be environment-aware so that the navigation plan can beadjusted based on the current visual observation. Moreover, the LLM plannedactions should be adaptable to the much larger and more complex REVERIEenvironment. This paper proposes a March-in-Chat (MiC) model that can talk tothe LLM on the fly and plan dynamically based on a newly proposedRoom-and-Object Aware Scene Perceiver (ROASP). Our MiC model outperforms theprevious state-of-the-art by large margins by SPL and RGSPL metrics on theREVERIE benchmark.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.10141v1 |
| 153 | AutoReP: Automatic ReLU Replacement for Fast Private Network Inference                                                                               | Hongwu Peng                    | 2023-08-20     | cs.CR, cs.LG, E.3; I.2; B.0             | The growth of the Machine-Learning-As-A-Service (MLaaS) market hashighlighted clients' data privacy and security issues. Private inference (PI)techniques using cryptographic primitives offer a solution but often have highcomputation and communication costs, particularly with non-linear operatorslike ReLU. Many attempts to reduce ReLU operations exist, but they may needheuristic threshold selection or cause substantial accuracy loss. This workintroduces AutoReP, a gradient-based approach to lessen non-linear operatorsand alleviate these issues. It automates the selection of ReLU and polynomialfunctions to speed up PI applications and introduces distribution-awarepolynomial approximation (DaPa) to maintain model expressivity while accuratelyapproximating ReLUs. Our experimental results demonstrate significant accuracyimprovements of 6.12% (94.31%, 12.9K ReLU budget, CIFAR-10), 8.39% (74.92%,12.9K ReLU budget, CIFAR-100), and 9.45% (63.69%, 55K ReLU budget,Tiny-ImageNet) over current state-of-the-art methods, e.g., SNL. Morever,AutoReP is applied to EfficientNet-B2 on ImageNet dataset, and achieved 75.55%accuracy with 176.1 times ReLU budget reduction.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.10134v1 |
| 154 | TransFace: Calibrating Transformer Training for Face Recognition from a Data-Centric Perspective                                                     | Jun Dan                        | 2023-08-20     | cs.CV, cs.AI                            | Vision Transformers (ViTs) have demonstrated powerful representation abilityin various visual tasks thanks to their intrinsic data-hungry nature. However,we unexpectedly find that ViTs perform vulnerably when applied to facerecognition (FR) scenarios with extremely large datasets. We investigate thereasons for this phenomenon and discover that the existing data augmentationapproach and hard sample mining strategy are incompatible with ViTs-based FRbackbone due to the lack of tailored consideration on preserving facestructural information and leveraging each local token information. To remedythese problems, this paper proposes a superior FR model called TransFace, whichemploys a patch-level data augmentation strategy named DPAP and a hard samplemining strategy named EHSM. Specially, DPAP randomly perturbs the amplitudeinformation of dominant patches to expand sample diversity, which effectivelyalleviates the overfitting problem in ViTs. EHSM utilizes the informationentropy in the local tokens to dynamically adjust the importance weight of easyand hard samples during training, leading to a more stable prediction.Experiments on several benchmarks demonstrate the superiority of our TransFace.Code and models are available at https://github.com/DanJun6737/TransFace.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.10133v1 |
| 155 | 3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose Estimation                                                                           | Yi Zhang                       | 2023-08-19     | cs.CV, cs.AI                            | Regression-based methods for 3D human pose estimation directly predict the 3Dpose parameters from a 2D image using deep networks. While achievingstate-of-the-art performance on standard benchmarks, their performance degradesunder occlusion. In contrast, optimization-based methods fit a parametric bodymodel to 2D features in an iterative manner. The localized reconstruction losscan potentially make them robust to occlusion, but they suffer from the 2D-3Dambiguity.  Motivated by the recent success of generative models in rigid object poseestimation, we propose 3D-aware Neural Body Fitting (3DNBF) - an approximateanalysis-by-synthesis approach to 3D human pose estimation with SOTAperformance and occlusion robustness. In particular, we propose a generativemodel of deep features based on a volumetric human representation with Gaussianellipsoidal kernels emitting 3D pose-dependent feature vectors. The neuralfeatures are trained with contrastive learning to become 3D-aware and hence toovercome the 2D-3D ambiguity.  Experiments show that 3DNBF outperforms other approaches on both occluded andstandard benchmarks. Code is available at https://github.com/edz-o/3DNBF                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.10123v1 |
| 156 | HollowNeRF: Pruning Hashgrid-Based NeRFs with Trainable Collision Mitigation                                                                         | Xiufeng Xie                    | 2023-08-19     | cs.CV, I.4.5                            | Neural radiance fields (NeRF) have garnered significant attention, withrecent works such as Instant-NGP accelerating NeRF training and evaluationthrough a combination of hashgrid-based positional encoding and neuralnetworks. However, effectively leveraging the spatial sparsity of 3D scenesremains a challenge. To cull away unnecessary regions of the feature grid,existing solutions rely on prior knowledge of object shape or periodicallyestimate object shape during training by repeated model evaluations, which arecostly and wasteful.  To address this issue, we propose HollowNeRF, a novel compression solutionfor hashgrid-based NeRF which automatically sparsifies the feature grid duringthe training phase. Instead of directly compressing dense features, HollowNeRFtrains a coarse 3D saliency mask that guides efficient feature pruning, andemploys an alternating direction method of multipliers (ADMM) pruner tosparsify the 3D saliency mask during training. By exploiting the sparsity inthe 3D scene to redistribute hash collisions, HollowNeRF improves renderingquality while using a fraction of the parameters of comparable state-of-the-artsolutions, leading to a better cost-accuracy trade-off. Our method deliverscomparable rendering quality to Instant-NGP, while utilizing just 31% of theparameters. In addition, our solution can achieve a PSNR accuracy gain of up to1dB using only 56% of the parameters.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.10122v1 |
| 157 | Robust Mixture-of-Expert Training for Convolutional Neural Networks                                                                                  | Yihua Zhang                    | 2023-08-19     | cs.CV, cs.AI, cs.LG                     | Sparsely-gated Mixture of Expert (MoE), an emerging deep model architecture,has demonstrated a great promise to enable high-accuracy and ultra-efficientmodel inference. Despite the growing popularity of MoE, little workinvestigated its potential to advance convolutional neural networks (CNNs),especially in the plane of adversarial robustness. Since the lack of robustnesshas become one of the main hurdles for CNNs, in this paper we ask: How toadversarially robustify a CNN-based MoE model? Can we robustly train it like anordinary CNN model? Our pilot study shows that the conventional adversarialtraining (AT) mechanism (developed for vanilla CNNs) no longer remainseffective to robustify an MoE-CNN. To better understand this phenomenon, wedissect the robustness of an MoE-CNN into two dimensions: Robustness of routers(i.e., gating functions to select data-specific experts) and robustness ofexperts (i.e., the router-guided pathways defined by the subnetworks of thebackbone CNN). Our analyses show that routers and experts are hard to adapt toeach other in the vanilla AT. Thus, we propose a new router-expert alternatingAdversarial training framework for MoE, termed AdvMoE. The effectiveness of ourproposal is justified across 4 commonly-used CNN model architectures over 4benchmark datasets. We find that AdvMoE achieves 1% ~ 4% adversarial robustnessimprovement over the original dense CNN, and enjoys the efficiency merit ofsparsity-gated MoE, leading to more than 50% inference cost reduction. Codesare available at https://github.com/OPTML-Group/Robust-MoE-CNN.                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.10110v1 |
| 158 | Root Pose Decomposition Towards Generic Non-rigid 3D Reconstruction with Monocular Videos                                                            | Yikai Wang                     | 2023-08-19     | cs.CV                                   | This work focuses on the 3D reconstruction of non-rigid objects based onmonocular RGB video sequences. Concretely, we aim at building high-fidelitymodels for generic object categories and casually captured scenes. To this end,we do not assume known root poses of objects, and do not utilizecategory-specific templates or dense pose priors. The key idea of our method,Root Pose Decomposition (RPD), is to maintain a per-frame root posetransformation, meanwhile building a dense field with local transformations torectify the root pose. The optimization of local transformations is performedby point registration to the canonical space. We also adapt RPD to multi-objectscenarios with object occlusions and individual differences. As a result, RPDallows non-rigid 3D reconstruction for complicated scenarios containing objectswith large deformations, complex motion patterns, occlusions, and scalediversities of different individuals. Such a pipeline potentially scales todiverse sets of objects in the wild. We experimentally show that RPD surpassesstate-of-the-art methods on the challenging DAVIS, OVIS, and AMA datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.10089v1 |
| 159 | Sensitivity analysis of AI-based algorithms for autonomous driving on optical wavefront aberrations induced by the windshield                        | Dominik Werner Wolf            | 2023-08-19     | eess.IV, cs.CV                          | Autonomous driving perception techniques are typically based on supervisedmachine learning models that are trained on real-world street data. A typicaltraining process involves capturing images with a single car model andwindshield configuration. However, deploying these trained models on differentcar types can lead to a domain shift, which can potentially hurt the neuralnetworks performance and violate working ADAS requirements. To address thisissue, this paper investigates the domain shift problem further by evaluatingthe sensitivity of two perception models to different windshieldconfigurations. This is done by evaluating the dependencies between neuralnetwork benchmark metrics and optical merit functions by applying a Fourieroptics based threat model. Our results show that there is a performance gapintroduced by windshields and existing optical metrics used for posingrequirements might not be sufficient.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.11711v1 |
| 160 | Single Image Reflection Separation via Component Synergy                                                                                             | Qiming Hu                      | 2023-08-19     | cs.CV                                   | The reflection superposition phenomenon is complex and widely distributed inthe real world, which derives various simplified linear and nonlinearformulations of the problem. In this paper, based on the investigation of theweaknesses of existing models, we propose a more general form of thesuperposition model by introducing a learnable residue term, which caneffectively capture residual information during decomposition, guiding theseparated layers to be complete. In order to fully capitalize on itsadvantages, we further design the network structure elaborately, including anovel dual-stream interaction mechanism and a powerful decomposition networkwith a semantic pyramid encoder. Extensive experiments and ablation studies areconducted to verify our superiority over state-of-the-art approaches onmultiple real-world benchmark datasets. Our code is publicly available athttps://github.com/mingcv/DSRNet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.10027v1 |
| 161 | Pseudo Flow Consistency for Self-Supervised 6D Object Pose Estimation                                                                                | Yang Hai                       | 2023-08-19     | cs.CV                                   | Most self-supervised 6D object pose estimation methods can only work withadditional depth information or rely on the accurate annotation of 2Dsegmentation masks, limiting their application range. In this paper, we proposea 6D object pose estimation method that can be trained with pure RGB imageswithout any auxiliary information. We first obtain a rough pose initializationfrom networks trained on synthetic images rendered from the target's 3D mesh.Then, we introduce a refinement strategy leveraging the geometry constraint insynthetic-to-real image pairs from multiple different views. We formulate thisgeometry constraint as pixel-level flow consistency between the training imageswith dynamically generated pseudo labels. We evaluate our method on threechallenging datasets and demonstrate that it outperforms state-of-the-artself-supervised methods significantly, with neither 2D annotations noradditional depth images.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.10016v1 |
| 162 | Partition-and-Debias: Agnostic Biases Mitigation via A Mixture of Biases-Specific Experts                                                            | Jiaxuan Li                     | 2023-08-19     | cs.CV                                   | Bias mitigation in image classification has been widely researched, andexisting methods have yielded notable results. However, most of these methodsimplicitly assume that a given image contains only one type of known or unknownbias, failing to consider the complexities of real-world biases. We introduce amore challenging scenario, agnostic biases mitigation, aiming at bias removalregardless of whether the type of bias or the number of types is unknown in thedatasets. To address this difficult task, we present the Partition-and-Debias(PnD) method that uses a mixture of biases-specific experts to implicitlydivide the bias space into multiple subspaces and a gating module to find aconsensus among experts to achieve debiased classification. Experiments on bothpublic and constructed benchmarks demonstrated the efficacy of the PnD. Code isavailable at: https://github.com/Jiaxuan-Li/PnD.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.10005v1 |
| 163 | ClothesNet: An Information-Rich 3D Garment Model Repository with Simulated Clothes Environment                                                       | Bingyang Zhou                  | 2023-08-19     | cs.RO, cs.AI, cs.CV                     | We present ClothesNet: a large-scale dataset of 3D clothes objects withinformation-rich annotations. Our dataset consists of around 4400 modelscovering 11 categories annotated with clothes features, boundary lines, andkeypoints. ClothesNet can be used to facilitate a variety of computer visionand robot interaction tasks. Using our dataset, we establish benchmark tasksfor clothes perception, including classification, boundary line segmentation,and keypoint detection, and develop simulated clothes environments for roboticinteraction tasks, including rearranging, folding, hanging, and dressing. Wealso demonstrate the efficacy of our ClothesNet in real-world experiments.Supplemental materials and dataset are available on our project webpage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.09987v1 |
| 164 | Disposable Transfer Learning for Selective Source Task Unlearning                                                                                    | Seunghee Koh                   | 2023-08-19     | cs.LG, cs.AI, cs.CV                     | Transfer learning is widely used for training deep neural networks (DNN) forbuilding a powerful representation. Even after the pre-trained model is adaptedfor the target task, the representation performance of the feature extractor isretained to some extent. As the performance of the pre-trained model can beconsidered the private property of the owner, it is natural to seek theexclusive right of the generalized performance of the pre-trained weight. Toaddress this issue, we suggest a new paradigm of transfer learning calleddisposable transfer learning (DTL), which disposes of only the source taskwithout degrading the performance of the target task. To achieve knowledgedisposal, we propose a novel loss named Gradient Collision loss (GC loss). GCloss selectively unlearns the source knowledge by leading the gradient vectorsof mini-batches in different directions. Whether the model successfullyunlearns the source task is measured by piggyback learning accuracy (PLaccuracy). PL accuracy estimates the vulnerability of knowledge leakage byretraining the scrubbed model on a subset of source data or new downstreamdata. We demonstrate that GC loss is an effective approach to the DTL problemby showing that the model trained with GC loss retains the performance on thetarget task with a significantly reduced PL accuracy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.09971v1 |
| 165 | Semantics Meets Temporal Correspondence: Self-supervised Object-centric Learning in Videos                                                           | Rui Qian                       | 2023-08-19     | cs.CV                                   | Self-supervised methods have shown remarkable progress in learning high-levelsemantics and low-level temporal correspondence. Building on these results, wetake one step further and explore the possibility of integrating these twofeatures to enhance object-centric representations. Our preliminary experimentsindicate that query slot attention can extract different semantic componentsfrom the RGB feature map, while random sampling based slot attention canexploit temporal correspondence cues between frames to assist instanceidentification. Motivated by this, we propose a novel semantic-aware maskedslot attention on top of the fused semantic features and correspondence maps.It comprises two slot attention stages with a set of shared learnable Gaussiandistributions. In the first stage, we use the mean vectors as slotinitialization to decompose potential semantics and generate semanticsegmentation masks through iterative attention. In the second stage, for eachsemantics, we randomly sample slots from the corresponding Gaussiandistribution and perform masked feature aggregation within the semantic area toexploit temporal correspondence patterns for instance identification. We adoptsemantic- and instance-level temporal consistency as self-supervision toencourage temporally coherent object-centric representations. Our modeleffectively identifies multiple object instances with semantic structure,reaching promising results on unsupervised video object discovery. Furthermore,we achieve state-of-the-art performance on dense label propagation tasks,demonstrating the potential for object-centric analysis. The code is releasedat https://github.com/shvdiwnkozbw/SMTC.                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.09951v1 |
| 166 | Scene-Aware Feature Matching                                                                                                                         | Xiaoyong Lu                    | 2023-08-19     | cs.CV                                   | Current feature matching methods focus on point-level matching, pursuingbetter representation learning of individual features, but lacking furtherunderstanding of the scene. This results in significant performance degradationwhen handling challenging scenes such as scenes with large viewpoint andillumination changes. To tackle this problem, we propose a novel model namedSAM, which applies attentional grouping to guide Scene-Aware feature Matching.SAM handles multi-level features, i.e., image tokens and group tokens, withattention layers, and groups the image tokens with the proposed token groupingmodule. Our model can be trained by ground-truth matches only and producereasonable grouping results. With the sense-aware grouping guidance, SAM is notonly more accurate and robust but also more interpretable than conventionalfeature matching models. Sufficient experiments on various applications,including homography estimation, pose estimation, and image matching,demonstrate that our model achieves state-of-the-art performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.09949v2 |
| 167 | Weakly-Supervised Action Localization by Hierarchically-structured Latent Attention Modeling                                                         | Guiqin Wang                    | 2023-08-19     | cs.CV                                   | Weakly-supervised action localization aims to recognize and localize actioninstancese in untrimmed videos with only video-level labels. Most existingmodels rely on multiple instance learning(MIL), where the predictions ofunlabeled instances are supervised by classifying labeled bags. The MIL-basedmethods are relatively well studied with cogent performance achieved onclassification but not on localization. Generally, they locate temporal regionsby the video-level classification but overlook the temporal variations offeature semantics. To address this problem, we propose a novel attention-basedhierarchically-structured latent model to learn the temporal variations offeature semantics. Specifically, our model entails two components, the first isan unsupervised change-points detection module that detects change-points bylearning the latent representations of video features in a temporal hierarchybased on their rates of change, and the second is an attention-basedclassification model that selects the change-points of the foreground as theboundaries. To evaluate the effectiveness of our model, we conduct extensiveexperiments on two benchmark datasets, THUMOS-14 and ActivityNet-v1.3. Theexperiments show that our method outperforms current state-of-the-art methods,and even achieves comparable performance with fully-supervised methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.09946v1 |
| 168 | On the Robustness of Open-World Test-Time Training: Self-Training with Dynamic Prototype Expansion                                                   | Yushu Li                       | 2023-08-19     | cs.CV, cs.LG                            | Generalizing deep learning models to unknown target domain distribution withlow latency has motivated research into test-time training/adaptation(TTT/TTA). Existing approaches often focus on improving test-time trainingperformance under well-curated target domain data. As figured out in this work,many state-of-the-art methods fail to maintain the performance when the targetdomain is contaminated with strong out-of-distribution (OOD) data, a.k.a.open-world test-time training (OWTTT). The failure is mainly due to theinability to distinguish strong OOD samples from regular weak OOD samples. Toimprove the robustness of OWTTT we first develop an adaptive strong OOD pruningwhich improves the efficacy of the self-training TTT method. We further proposea way to dynamically expand the prototypes to represent strong OOD samples foran improved weak/strong OOD data separation. Finally, we regularizeself-training with distribution alignment and the combination yields thestate-of-the-art performance on 5 OWTTT benchmarks. The code is available athttps://github.com/Yushu-Li/OWTTT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.09942v1 |
| 169 | Understanding Self-attention Mechanism via Dynamical System Perspective                                                                              | Zhongzhan Huang                | 2023-08-19     | cs.CV, cs.AI                            | The self-attention mechanism (SAM) is widely used in various fields ofartificial intelligence and has successfully boosted the performance ofdifferent models. However, current explanations of this mechanism are mainlybased on intuitions and experiences, while there still lacks direct modelingfor how the SAM helps performance. To mitigate this issue, in this paper, basedon the dynamical system perspective of the residual neural network, we firstshow that the intrinsic stiffness phenomenon (SP) in the high-precisionsolution of ordinary differential equations (ODEs) also widely exists inhigh-performance neural networks (NN). Thus the ability of NN to measure SP atthe feature level is necessary to obtain high performance and is an importantfactor in the difficulty of training NN. Similar to the adaptive step-sizemethod which is effective in solving stiff ODEs, we show that the SAM is also astiffness-aware step size adaptor that can enhance the model's representationalability to measure intrinsic SP by refining the estimation of stiffnessinformation and generating adaptive attention values, which provides a newunderstanding about why and how the SAM can benefit the model performance. Thisnovel perspective can also explain the lottery ticket hypothesis in SAM, designnew quantitative metrics of representational ability, and inspire a newtheoretic-inspired approach, StepNet. Extensive experiments on several popularbenchmarks demonstrate that StepNet can extract fine-grained stiffnessinformation and measure SP accurately, leading to significant improvements invarious visual tasks.                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.09939v1 |
| 170 | Scalable Video Object Segmentation with Simplified Framework                                                                                         | Qiangqiang Wu                  | 2023-08-19     | cs.CV                                   | The current popular methods for video object segmentation (VOS) implementfeature matching through several hand-crafted modules that separately performfeature extraction and matching. However, the above hand-crafted designsempirically cause insufficient target interaction, thus limiting the dynamictarget-aware feature learning in VOS. To tackle these limitations, this paperpresents a scalable Simplified VOS (SimVOS) framework to perform joint featureextraction and matching by leveraging a single transformer backbone.Specifically, SimVOS employs a scalable ViT backbone for simultaneous featureextraction and matching between query and reference features. This designenables SimVOS to learn better target-ware features for accurate maskprediction. More importantly, SimVOS could directly apply well-pretrained ViTbackbones (e.g., MAE) for VOS, which bridges the gap between VOS andlarge-scale self-supervised pre-training. To achieve a better performance-speedtrade-off, we further explore within-frame attention and propose a new tokenrefinement module to improve the running speed and save computational cost.Experimentally, our SimVOS achieves state-of-the-art results on popular videoobject segmentation benchmarks, i.e., DAVIS-2017 (88.0% J&F), DAVIS-2016 (92.9%J&F) and YouTube-VOS 2019 (84.2% J&F), without applying any synthetic video orBL30K pre-training used in previous VOS approaches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.09903v1 |
| 171 | SwinLSTM:Improving Spatiotemporal Prediction Accuracy using Swin Transformer and LSTM                                                                | Song Tang                      | 2023-08-19     | cs.CV, cs.AI                            | Integrating CNNs and RNNs to capture spatiotemporal dependencies is aprevalent strategy for spatiotemporal prediction tasks. However, the propertyof CNNs to learn local spatial information decreases their efficiency incapturing spatiotemporal dependencies, thereby limiting their predictionaccuracy. In this paper, we propose a new recurrent cell, SwinLSTM, whichintegrates Swin Transformer blocks and the simplified LSTM, an extension thatreplaces the convolutional structure in ConvLSTM with the self-attentionmechanism. Furthermore, we construct a network with SwinLSTM cell as the corefor spatiotemporal prediction. Without using unique tricks, SwinLSTMoutperforms state-of-the-art methods on Moving MNIST, Human3.6m, TaxiBJ, andKTH datasets. In particular, it exhibits a significant improvement inprediction accuracy compared to ConvLSTM. Our competitive experimental resultsdemonstrate that learning global spatial dependencies is more advantageous formodels to capture spatiotemporal dependencies. We hope that SwinLSTM can serveas a solid baseline to promote the advancement of spatiotemporal predictionaccuracy. The codes are publicly available athttps://github.com/SongTang-x/SwinLSTM.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.09891v1 |
| 172 | Calibrating Uncertainty for Semi-Supervised Crowd Counting                                                                                           | Chen Li                        | 2023-08-19     | cs.CV, cs.LG                            | Semi-supervised crowd counting is an important yet challenging task. Apopular approach is to iteratively generate pseudo-labels for unlabeled dataand add them to the training set. The key is to use uncertainty to selectreliable pseudo-labels. In this paper, we propose a novel method to calibratemodel uncertainty for crowd counting. Our method takes a supervised uncertaintyestimation strategy to train the model through a surrogate function. Thisensures the uncertainty is well controlled throughout the training. We proposea matching-based patch-wise surrogate function to better approximateuncertainty for crowd counting tasks. The proposed method pays a sufficientamount of attention to details, while maintaining a proper granularity.Altogether our method is able to generate reliable uncertainty estimation, highquality pseudolabels, and achieve state-of-the-art performance insemisupervised crowd counting.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.09887v1 |
| 173 | DatasetEquity: Are All Samples Created Equal? In The Quest For Equity Within Datasets                                                                | Shubham Shrivastava            | 2023-08-19     | cs.CV, cs.LG                            | Data imbalance is a well-known issue in the field of machine learning,attributable to the cost of data collection, the difficulty of labeling, andthe geographical distribution of the data. In computer vision, bias in datadistribution caused by image appearance remains highly unexplored. Compared tocategorical distributions using class labels, image appearance reveals complexrelationships between objects beyond what class labels provide. Clustering deepperceptual features extracted from raw pixels gives a richer representation ofthe data. This paper presents a novel method for addressing data imbalance inmachine learning. The method computes sample likelihoods based on imageappearance using deep perceptual embeddings and clustering. It then uses theselikelihoods to weigh samples differently during training with a proposed$\textbf{Generalized Focal Loss}$ function. This loss can be easily integratedwith deep learning algorithms. Experiments validate the method's effectivenessacross autonomous driving vision datasets including KITTI and nuScenes. Theloss function improves state-of-the-art 3D object detection methods, achievingover $200\%$ AP gains on under-represented classes (Cyclist) in the KITTIdataset. The results demonstrate the method is generalizable, complementsexisting techniques, and is particularly beneficial for smaller datasets andrare classes. Code is available at:https://github.com/towardsautonomy/DatasetEquity                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.09878v2 |
| 174 | A Theory of Topological Derivatives for Inverse Rendering of Geometry                                                                                | Ishit Mehta                    | 2023-08-19     | cs.CV, cs.GR                            | We introduce a theoretical framework for differentiable surface evolutionthat allows discrete topology changes through the use of topologicalderivatives for variational optimization of image functionals. While priormethods for inverse rendering of geometry rely on silhouette gradients fortopology changes, such signals are sparse. In contrast, our theory derivestopological derivatives that relate the introduction of vanishing holes andphases to changes in image intensity. As a result, we enable differentiableshape perturbations in the form of hole or phase nucleation. We validate theproposed theory with optimization of closed curves in 2D and surfaces in 3D tolend insights into limitations of current methods and enable improvedapplications such as image vectorization, vector-graphics generation from textprompts, single-image reconstruction of shape ambigrams and multi-view 3Dreconstruction.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.09865v1 |
| 175 | VL-PET: Vision-and-Language Parameter-Efficient Tuning via Granularity Control                                                                       | Zi-Yuan Hu                     | 2023-08-18     | cs.CV, cs.AI, cs.CL, cs.LG              | As the model size of pre-trained language models (PLMs) grows rapidly, fullfine-tuning becomes prohibitively expensive for model training and storage. Invision-and-language (VL), parameter-efficient tuning (PET) techniques areproposed to integrate modular modifications (e.g., Adapter and LoRA) intoencoder-decoder PLMs. By tuning a small set of trainable parameters, thesetechniques perform on par with full fine-tuning. However, excessive modularmodifications and neglecting the functionality gap between the encoders anddecoders can lead to performance degradation, while existing PET techniques(e.g., VL-Adapter) overlook these critical issues. In this paper, we propose aVision-and-Language Parameter-Efficient Tuning (VL-PET) framework to imposeeffective control over modular modifications via a novel granularity-controlledmechanism. Considering different granularity-controlled matrices generated bythis mechanism, a variety of model-agnostic VL-PET modules can be instantiatedfrom our framework for better efficiency and effectiveness trade-offs. Wefurther propose lightweight PET module designs to enhance VL alignment andmodeling for the encoders and maintain text generation for the decoders.Extensive experiments conducted on four image-text tasks and four video-texttasks demonstrate the efficiency, effectiveness and transferability of ourVL-PET framework. In particular, our VL-PET-large with lightweight PET moduledesigns significantly outperforms VL-Adapter by 2.92% (3.41%) and LoRA by 3.37%(7.03%) with BART-base (T5-base) on image-text tasks. Furthermore, we validatethe enhanced effect of employing our VL-PET designs on existing PET techniques,enabling them to achieve significant performance improvements. Our code isavailable at https://github.com/HenryHZY/VL-PET.                                                                                                 | http://arxiv.org/abs/2308.09804v1 |
| 176 | Long-range Multimodal Pretraining for Movie Understanding                                                                                            | Dawit Mureja Argaw             | 2023-08-18     | cs.CV                                   | Learning computer vision models from (and for) movies has a long-standinghistory. While great progress has been attained, there is still a need for apretrained multimodal model that can perform well in the ever-growing set ofmovie understanding tasks the community has been establishing. In this work, weintroduce Long-range Multimodal Pretraining, a strategy, and a model thatleverages movie data to train transferable multimodal and cross-modal encoders.Our key idea is to learn from all modalities in a movie by observing andextracting relationships over a long-range. After pretraining, we run ablationstudies on the LVU benchmark and validate our modeling choices and theimportance of learning from long-range time spans. Our model achievesstate-of-the-art on several LVU tasks while being much more data efficient thanprevious works. Finally, we evaluate our model's transferability by setting anew state-of-the-art in five different benchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.09775v1 |
| 177 | Smoothness Similarity Regularization for Few-Shot GAN Adaptation                                                                                     | Vadim Sushko                   | 2023-08-18     | cs.CV                                   | The task of few-shot GAN adaptation aims to adapt a pre-trained GAN model toa small dataset with very few training images. While existing methods performwell when the dataset for pre-training is structurally similar to the targetdataset, the approaches suffer from training instabilities or memorizationissues when the objects in the two domains have a very different structure. Tomitigate this limitation, we propose a new smoothness similarity regularizationthat transfers the inherently learned smoothness of the pre-trained GAN to thefew-shot target domain even if the two domains are very different. We evaluateour approach by adapting an unconditional and a class-conditional GAN todiverse few-shot target domains. Our proposed method significantly outperformsprior few-shot GAN adaptation methods in the challenging case of structurallydissimilar source-target domains, while performing on par with the state of theart for similar source-target domains.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.09717v1 |
| 178 | Robust Monocular Depth Estimation under Challenging Conditions                                                                                       | Stefano Gasperini              | 2023-08-18     | cs.CV, cs.LG, cs.RO                     | While state-of-the-art monocular depth estimation approaches achieveimpressive results in ideal settings, they are highly unreliable underchallenging illumination and weather conditions, such as at nighttime or in thepresence of rain. In this paper, we uncover these safety-critical issues andtackle them with md4all: a simple and effective solution that works reliablyunder both adverse and ideal conditions, as well as for different types oflearning supervision. We achieve this by exploiting the efficacy of existingmethods under perfect settings. Therefore, we provide valid training signalsindependently of what is in the input. First, we generate a set of complexsamples corresponding to the normal training ones. Then, we train the model byguiding its self- or full-supervision by feeding the generated samples andcomputing the standard losses on the corresponding original images. Doing soenables a single model to recover information across diverse conditions withoutmodifications at inference time. Extensive experiments on two challengingpublic datasets, namely nuScenes and Oxford RobotCar, demonstrate theeffectiveness of our techniques, outperforming prior works by a large margin inboth standard and challenging conditions. Source code and data are availableat: https://md4all.github.io.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.09711v1 |
| 179 | LaRS: A Diverse Panoptic Maritime Obstacle Detection Dataset and Benchmark                                                                           | Lojze Žust                     | 2023-08-18     | cs.CV                                   | The progress in maritime obstacle detection is hindered by the lack of adiverse dataset that adequately captures the complexity of general maritimeenvironments. We present the first maritime panoptic obstacle detectionbenchmark LaRS, featuring scenes from Lakes, Rivers and Seas. Our majorcontribution is the new dataset, which boasts the largest diversity inrecording locations, scene types, obstacle classes, and acquisition conditionsamong the related datasets. LaRS is composed of over 4000 per-pixel labeled keyframes with nine preceding frames to allow utilization of the temporal texture,amounting to over 40k frames. Each key frame is annotated with 8 thing, 3 stuffclasses and 19 global scene attributes. We report the results of 27 semanticand panoptic segmentation methods, along with several performance insights andfuture research directions. To enable objective evaluation, we have implementedan online evaluation server. The LaRS dataset, evaluation toolkit and benchmarkare publicly available at: https://lojzezust.github.io/lars-dataset                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.09618v1 |
| 180 | StableVideo: Text-driven Consistency-aware Diffusion Video Editing                                                                                   | Wenhao Chai                    | 2023-08-18     | cs.CV                                   | Diffusion-based methods can generate realistic images and videos, but theystruggle to edit existing objects in a video while preserving their appearanceover time. This prevents diffusion models from being applied to natural videoediting in practical scenarios. In this paper, we tackle this problem byintroducing temporal dependency to existing text-driven diffusion models, whichallows them to generate consistent appearance for the edited objects.Specifically, we develop a novel inter-frame propagation mechanism fordiffusion video editing, which leverages the concept of layered representationsto propagate the appearance information from one frame to the next. We thenbuild up a text-driven video editing framework based on this mechanism, namelyStableVideo, which can achieve consistency-aware video editing. Extensiveexperiments demonstrate the strong editing capability of our approach. Comparedwith state-of-the-art video editing methods, our approach shows superiorqualitative and quantitative results. Our code is available at\href{https://github.com/rese1f/StableVideo}{this https URL}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.09592v1 |
| 181 | Adapt Your Teacher: Improving Knowledge Distillation for Exemplar-free Continual Learning                                                            | Filip Szatkowski               | 2023-08-18     | cs.LG, cs.AI, cs.CV                     | In this work, we investigate exemplar-free class incremental learning (CIL)with knowledge distillation (KD) as a regularization strategy, aiming toprevent forgetting. KD-based methods are successfully used in CIL, but theyoften struggle to regularize the model without access to exemplars of thetraining data from previous tasks. Our analysis reveals that this issueoriginates from substantial representation shifts in the teacher network whendealing with out-of-distribution data. This causes large errors in the KD losscomponent, leading to performance degradation in CIL. Inspired by recenttest-time adaptation methods, we introduce Teacher Adaptation (TA), a methodthat concurrently updates the teacher and the main model during incrementaltraining. Our method seamlessly integrates with KD-based CIL approaches andallows for consistent enhancement of their performance across multipleexemplar-free CIL benchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.09544v1 |
| 182 | Meta-ZSDETR: Zero-shot DETR with Meta-learning                                                                                                       | Lu Zhang                       | 2023-08-18     | cs.CV, cs.AI                            | Zero-shot object detection aims to localize and recognize objects of unseenclasses. Most of existing works face two problems: the low recall of RPN inunseen classes and the confusion of unseen classes with background. In thispaper, we present the first method that combines DETR and meta-learning toperform zero-shot object detection, named Meta-ZSDETR, where model training isformalized as an individual episode based meta-learning task. Different fromFaster R-CNN based methods that firstly generate class-agnostic proposals, andthen classify them with visual-semantic alignment module, Meta-ZSDETR directlypredict class-specific boxes with class-specific queries and further filterthem with the predicted accuracy from classification head. The model isoptimized with meta-contrastive learning, which contains a regression head togenerate the coordinates of class-specific boxes, a classification head topredict the accuracy of generated boxes, and a contrastive head that utilizesthe proposed contrastive-reconstruction loss to further separate differentclasses in visual space. We conduct extensive experiments on two benchmarkdatasets MS COCO and PASCAL VOC. Experimental results show that our methodoutperforms the existing ZSD methods by a large margin.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.09540v1 |
| 183 | Leveraging Intrinsic Properties for Non-Rigid Garment Alignment                                                                                      | Siyou Lin                      | 2023-08-18     | cs.CV                                   | We address the problem of aligning real-world 3D data of garments, whichbenefits many applications such as texture learning, physical parameterestimation, generative modeling of garments, etc. Existing extrinsic methodstypically perform non-rigid iterative closest point and struggle to aligndetails due to incorrect closest matches and rigidity constraints. Whileintrinsic methods based on functional maps can produce high-qualitycorrespondences, they work under isometric assumptions and become unreliablefor garment deformations which are highly non-isometric. To achievewrinkle-level as well as texture-level alignment, we present a novelcoarse-to-fine two-stage method that leverages intrinsic manifold propertieswith two neural deformation fields, in the 3D space and the intrinsic space,respectively. The coarse stage performs a 3D fitting, where we leverageintrinsic manifold properties to define a manifold deformation field. Thecoarse fitting then induces a functional map that produces an alignment ofintrinsic embeddings. We further refine the intrinsic alignment with a secondneural deformation field for higher accuracy. We evaluate our method with ourcaptured garment dataset, GarmCap. The method achieves accurate wrinkle-leveland texture-level alignment and works for difficult garment types such as longcoats. Our project page ishttps://jsnln.github.io/iccv2023_intrinsic/index.html.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.09519v1 |
| 184 | ResQ: Residual Quantization for Video Perception                                                                                                     | Davide Abati                   | 2023-08-18     | cs.CV                                   | This paper accelerates video perception, such as semantic segmentation andhuman pose estimation, by levering cross-frame redundancies. Unlike theexisting approaches, which avoid redundant computations by warping the pastfeatures using optical-flow or by performing sparse convolutions on framedifferences, we approach the problem from a new perspective: low-bitquantization. We observe that residuals, as the difference in networkactivations between two neighboring frames, exhibit properties that make themhighly quantizable. Based on this observation, we propose a novel quantizationscheme for video networks coined as Residual Quantization. ResQ extends thestandard, frame-by-frame, quantization scheme by incorporating temporaldependencies that lead to better performance in terms of accuracy vs.bit-width. Furthermore, we extend our model to dynamically adjust the bit-widthproportional to the amount of changes in the video. We demonstrate thesuperiority of our model, against the standard quantization and existingefficient video perception models, using various architectures on semanticsegmentation and human pose estimation benchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.09511v1 |
| 185 | Vision Relation Transformer for Unbiased Scene Graph Generation                                                                                      | Gopika Sudhakaran              | 2023-08-18     | cs.CV, cs.AI                            | Recent years have seen a growing interest in Scene Graph Generation (SGG), acomprehensive visual scene understanding task that aims to predict entityrelationships using a relation encoder-decoder pipeline stacked on top of anobject encoder-decoder backbone. Unfortunately, current SGG methods suffer froman information loss regarding the entities local-level cues during the relationencoding process. To mitigate this, we introduce the Vision rElationTransfOrmer (VETO), consisting of a novel local-level entity relation encoder.We further observe that many existing SGG methods claim to be unbiased, but arestill biased towards either head or tail classes. To overcome this bias, weintroduce a Mutually Exclusive ExperT (MEET) learning strategy that capturesimportant relation features without bias towards head or tail classes.Experimental results on the VG and GQA datasets demonstrate that VETO + MEETboosts the predictive performance by up to 47 percentage over the state of theart while being 10 times smaller.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.09472v1 |
| 186 | Transformer-based Detection of Microorganisms on High-Resolution Petri Dish Images                                                                   | Nikolas Ebert                  | 2023-08-18     | cs.CV                                   | Many medical or pharmaceutical processes have strict guidelines regardingcontinuous hygiene monitoring. This often involves the labor-intensive task ofmanually counting microorganisms in Petri dishes by trained personnel.Automation attempts often struggle due to major challenges: significant scalingdifferences, low separation, low contrast, etc. To address these challenges, weintroduce AttnPAFPN, a high-resolution detection pipeline that leverages anovel transformer variation, the efficient-global self-attention mechanism. Ourstreamlined approach can be easily integrated in almost any multi-scale objectdetection pipeline. In a comprehensive evaluation on the publicly availableAGAR dataset, we demonstrate the superior accuracy of our network over thecurrent state-of-the-art. In order to demonstrate the task-independentperformance of our approach, we perform further experiments on COCO andLIVECell datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.09436v2 |
| 187 | MonoNeRD: NeRF-like Representations for Monocular 3D Object Detection                                                                                | Junkai Xu                      | 2023-08-18     | cs.CV                                   | In the field of monocular 3D detection, it is common practice to utilizescene geometric clues to enhance the detector's performance. However, manyexisting works adopt these clues explicitly such as estimating a depth map andback-projecting it into 3D space. This explicit methodology induces sparsity in3D representations due to the increased dimensionality from 2D to 3D, and leadsto substantial information loss, especially for distant and occluded objects.To alleviate this issue, we propose MonoNeRD, a novel detection framework thatcan infer dense 3D geometry and occupancy. Specifically, we model scenes withSigned Distance Functions (SDF), facilitating the production of dense 3Drepresentations. We treat these representations as Neural Radiance Fields(NeRF) and then employ volume rendering to recover RGB images and depth maps.To the best of our knowledge, this work is the first to introduce volumerendering for M3D, and demonstrates the potential of implicit reconstructionfor image-based 3D perception. Extensive experiments conducted on the KITTI-3Dbenchmark and Waymo Open Dataset demonstrate the effectiveness of MonoNeRD.Codes are available at https://github.com/cskkxjk/MonoNeRD.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.09421v1 |
| 188 | DReg-NeRF: Deep Registration for Neural Radiance Fields                                                                                              | Yu Chen                        | 2023-08-18     | cs.CV                                   | Although Neural Radiance Fields (NeRF) is popular in the computer visioncommunity recently, registering multiple NeRFs has yet to gain much attention.Unlike the existing work, NeRF2NeRF, which is based on traditional optimizationmethods and needs human annotated keypoints, we propose DReg-NeRF to solve theNeRF registration problem on object-centric scenes without human intervention.After training NeRF models, our DReg-NeRF first extracts features from theoccupancy grid in NeRF. Subsequently, our DReg-NeRF utilizes a transformerarchitecture with self-attention and cross-attention layers to learn therelations between pairwise NeRF blocks. In contrast to state-of-the-art (SOTA)point cloud registration methods, the decoupled correspondences are supervisedby surface fields without any ground truth overlapping labels. We construct anovel view synthesis dataset with 1,700+ 3D objects obtained from Objaverse totrain our network. When evaluated on the test set, our proposed method beatsthe SOTA point cloud registration methods by a large margin, with a mean$\text{RPE}=9.67^{\circ}$ and a mean $\text{RTE}=0.038$.  Our code is available at https://github.com/AIBluefisher/DReg-NeRF.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.09386v1 |
| 189 | Label-Free Event-based Object Recognition via Joint Learning with Image Reconstruction from Events                                                   | Hoonhee Cho                    | 2023-08-18     | cs.CV                                   | Recognizing objects from sparse and noisy events becomes extremely difficultwhen paired images and category labels do not exist. In this paper, we studylabel-free event-based object recognition where category labels and pairedimages are not available. To this end, we propose a joint formulation of objectrecognition and image reconstruction in a complementary manner. Our methodfirst reconstructs images from events and performs object recognition throughContrastive Language-Image Pre-training (CLIP), enabling better recognitionthrough a rich context of images. Since the category information is essentialin reconstructing images, we propose category-guided attraction loss andcategory-agnostic repulsion loss to bridge the textual features of predictedcategories and the visual features of reconstructed images using CLIP.Moreover, we introduce a reliable data sampling strategy and local-globalreconstruction consistency to boost joint learning of two tasks. To enhance theaccuracy of prediction and quality of reconstruction, we also propose aprototype-based approach using unpaired images. Extensive experimentsdemonstrate the superiority of our method and its extensibility for zero-shotobject recognition. Our project code is available at\url{https://github.com/Chohoonhee/Ev-LaFOR}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.09383v1 |
| 190 | Open-vocabulary Video Question Answering: A New Benchmark for Evaluating the Generalizability of Video Question Answering Models                     | Dohwan Ko                      | 2023-08-18     | cs.CV                                   | Video Question Answering (VideoQA) is a challenging task that entails complexmulti-modal reasoning. In contrast to multiple-choice VideoQA which aims topredict the answer given several options, the goal of open-ended VideoQA is toanswer questions without restricting candidate answers. However, the majorityof previous VideoQA models formulate open-ended VideoQA as a classificationtask to classify the video-question pairs into a fixed answer set, i.e.,closed-vocabulary, which contains only frequent answers (e.g., top-1000answers). This leads the model to be biased toward only frequent answers andfail to generalize on out-of-vocabulary answers. We hence propose a newbenchmark, Open-vocabulary Video Question Answering (OVQA), to measure thegeneralizability of VideoQA models by considering rare and unseen answers. Inaddition, in order to improve the model's generalization power, we introduce anovel GNN-based soft verbalizer that enhances the prediction on rare and unseenanswers by aggregating the information from their similar words. Forevaluation, we introduce new baselines by modifying the existing(closed-vocabulary) open-ended VideoQA models and improve their performances byfurther taking into account rare and unseen answers. Our ablation studies andqualitative analyses demonstrate that our GNN-based soft verbalizer furtherimproves the model performance, especially on rare and unseen answers. We hopethat our benchmark OVQA can serve as a guide for evaluating thegeneralizability of VideoQA models and inspire future research. Code isavailable at https://github.com/mlvlab/OVQA.                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.09363v1 |
| 191 | RLIPv2: Fast Scaling of Relational Language-Image Pre-training                                                                                       | Hangjie Yuan                   | 2023-08-18     | cs.CV, cs.AI, cs.LG, cs.MM              | Relational Language-Image Pre-training (RLIP) aims to align visionrepresentations with relational texts, thereby advancing the capability ofrelational reasoning in computer vision tasks. However, hindered by the slowconvergence of RLIPv1 architecture and the limited availability of existingscene graph data, scaling RLIPv1 is challenging. In this paper, we proposeRLIPv2, a fast converging model that enables the scaling of relationalpre-training to large-scale pseudo-labelled scene graph data. To enable fastscaling, RLIPv2 introduces Asymmetric Language-Image Fusion (ALIF), a mechanismthat facilitates earlier and deeper gated cross-modal fusion with sparsifiedlanguage encoding layers. ALIF leads to comparable or better performance thanRLIPv1 in a fraction of the time for pre-training and fine-tuning. To obtainscene graph data at scale, we extend object detection datasets with free-formrelation labels by introducing a captioner (e.g., BLIP) and a designed RelationTagger. The Relation Tagger assigns BLIP-generated relation texts to regionpairs, thus enabling larger-scale relational pre-training. Through extensiveexperiments conducted on Human-Object Interaction Detection and Scene GraphGeneration, RLIPv2 shows state-of-the-art performance on three benchmarks underfully-finetuning, few-shot and zero-shot settings. Notably, the largest RLIPv2achieves 23.29mAP on HICO-DET without any fine-tuning, yields 32.22mAP withjust 1% data and yields 45.09mAP with 100% data. Code and models are publiclyavailable at https://github.com/JacobYuan7/RLIPv2.                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.09351v1 |
| 192 | Audio-Visual Glance Network for Efficient Video Recognition                                                                                          | Muhammad Adi Nugroho           | 2023-08-18     | cs.CV, cs.AI, cs.MM                     | Deep learning has made significant strides in video understanding tasks, butthe computation required to classify lengthy and massive videos usingclip-level video classifiers remains impractical and prohibitively expensive.To address this issue, we propose Audio-Visual Glance Network (AVGN), whichleverages the commonly available audio and visual modalities to efficientlyprocess the spatio-temporally important parts of a video. AVGN firstly dividesthe video into snippets of image-audio clip pair and employs lightweightunimodal encoders to extract global visual features and audio features. Toidentify the important temporal segments, we use an Audio-Visual TemporalSaliency Transformer (AV-TeST) that estimates the saliency scores of eachframe. To further increase efficiency in the spatial dimension, AVGN processesonly the important patches instead of the whole images. We use anAudio-Enhanced Spatial Patch Attention (AESPA) module to produce a set ofenhanced coarse visual features, which are fed to a policy network thatproduces the coordinates of the important patches. This approach enables us tofocus only on the most important spatio-temporally parts of the video, leadingto more efficient video recognition. Moreover, we incorporate various trainingtechniques and multi-modal feature fusion to enhance the robustness andeffectiveness of our AVGN. By combining these strategies, our AVGN sets newstate-of-the-art performance in multiple video recognition benchmarks whileachieving faster processing speed.                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.09322v1 |
| 193 | Towards Attack-tolerant Federated Learning via Critical Parameter Analysis                                                                           | Sungwon Han                    | 2023-08-18     | cs.LG, cs.AI, cs.CR                     | Federated learning is used to train a shared model in a decentralized waywithout clients sharing private data with each other. Federated learningsystems are susceptible to poisoning attacks when malicious clients send falseupdates to the central server. Existing defense strategies are ineffectiveunder non-IID data settings. This paper proposes a new defense strategy, FedCPA(Federated learning with Critical Parameter Analysis). Our attack-tolerantaggregation method is based on the observation that benign local models havesimilar sets of top-k and bottom-k critical parameters, whereas poisoned localmodels do not. Experiments with different attack scenarios on multiple datasetsdemonstrate that our model outperforms existing defense strategies in defendingagainst poisoning attacks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.09318v1 |
| 194 | Retro-FPN: Retrospective Feature Pyramid Network for Point Cloud Semantic Segmentation                                                               | Peng Xiang                     | 2023-08-18     | cs.CV                                   | Learning per-point semantic features from the hierarchical feature pyramid isessential for point cloud semantic segmentation. However, most previous methodssuffered from ambiguous region features or failed to refine per-point featureseffectively, which leads to information loss and ambiguous semanticidentification. To resolve this, we propose Retro-FPN to model the per-pointfeature prediction as an explicit and retrospective refining process, whichgoes through all the pyramid layers to extract semantic features explicitly foreach point. Its key novelty is a retro-transformer for summarizing semanticcontexts from the previous layer and accordingly refining the features in thecurrent stage. In this way, the categorization of each point is conditioned onits local semantic pattern. Specifically, the retro-transformer consists of alocal cross-attention block and a semantic gate unit. The cross-attentionserves to summarize the semantic pattern retrospectively from the previouslayer. And the gate unit carefully incorporates the summarized contexts andrefines the current semantic features. Retro-FPN is a pluggable neural networkthat applies to hierarchical decoders. By integrating Retro-FPN with threerepresentative backbones, including both point-based and voxel-based methods,we show that Retro-FPN can significantly improve performance overstate-of-the-art backbones. Comprehensive experiments on widely used benchmarkscan justify the effectiveness of our design. The source is available athttps://github.com/AllenXiangX/Retro-FPN                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.09314v1 |
| 195 | Lip Reading for Low-resource Languages by Learning and Combining General Speech Knowledge and Language-specific Knowledge                            | Minsu Kim                      | 2023-08-18     | cs.CV, cs.CL, cs.SD, eess.AS, eess.IV   | This paper proposes a novel lip reading framework, especially forlow-resource languages, which has not been well addressed in the previousliterature. Since low-resource languages do not have enough video-text paireddata to train the model to have sufficient power to model lip movements andlanguage, it is regarded as challenging to develop lip reading models forlow-resource languages. In order to mitigate the challenge, we try to learngeneral speech knowledge, the ability to model lip movements, from ahigh-resource language through the prediction of speech units. It is known thatdifferent languages partially share common phonemes, thus general speechknowledge learned from one language can be extended to other languages. Then,we try to learn language-specific knowledge, the ability to model language, byproposing Language-specific Memory-augmented Decoder (LMDecoder). LMDecodersaves language-specific audio features into memory banks and can be trained onaudio-text paired data which is more easily accessible than video-text paireddata. Therefore, with LMDecoder, we can transform the input speech units intolanguage-specific audio features and translate them into texts by utilizing thelearned rich language knowledge. Finally, by combining general speech knowledgeand language-specific knowledge, we can efficiently develop lip reading modelseven for low-resource languages. Through extensive experiments using fivelanguages, English, Spanish, French, Italian, and Portuguese, the effectivenessof the proposed method is evaluated.                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.09311v1 |
| 196 | Human Part-wise 3D Motion Context Learning for Sign Language Recognition                                                                             | Taeryung Lee                   | 2023-08-18     | cs.CV                                   | In this paper, we propose P3D, the human part-wise motion context learningframework for sign language recognition. Our main contributions lie in twodimensions: learning the part-wise motion context and employing the poseensemble to utilize 2D and 3D pose jointly. First, our empirical observationimplies that part-wise context encoding benefits the performance of signlanguage recognition. While previous methods of sign language recognitionlearned motion context from the sequence of the entire pose, we argue that suchmethods cannot exploit part-specific motion context. In order to utilizepart-wise motion context, we propose the alternating combination of a part-wiseencoding Transformer (PET) and a whole-body encoding Transformer (WET). PETencodes the motion contexts from a part sequence, while WET merges them into aunified context. By learning part-wise motion context, our P3D achievessuperior performance on WLASL compared to previous state-of-the-art methods.Second, our framework is the first to ensemble 2D and 3D poses for signlanguage recognition. Since the 3D pose holds rich motion context and depthinformation to distinguish the words, our P3D outperformed the previousstate-of-the-art methods employing a pose ensemble.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.09305v1 |
| 197 | NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization for Continual Learning                                                   | Tamasha Malepathirana          | 2023-08-18     | cs.CV                                   | Catastrophic forgetting; the loss of old knowledge upon acquiring newknowledge, is a pitfall faced by deep neural networks in real-worldapplications. Many prevailing solutions to this problem rely on storingexemplars (previously encountered data), which may not be feasible inapplications with memory limitations or privacy constraints. Therefore, therecent focus has been on Non-Exemplar based Class Incremental Learning (NECIL)where a model incrementally learns about new classes without using any pastexemplars. However, due to the lack of old data, NECIL methods struggle todiscriminate between old and new classes causing their feature representationsto overlap. We propose NAPA-VQ: Neighborhood Aware Prototype Augmentation withVector Quantization, a framework that reduces this class overlap in NECIL. Wedraw inspiration from Neural Gas to learn the topological relationships in thefeature space, identifying the neighboring classes that are most likely to getconfused with each other. This neighborhood information is utilized to enforcestrong separation between the neighboring classes as well as to generate oldclass representative prototypes that can better aid in obtaining adiscriminative decision boundary between old and new classes. Our comprehensiveexperiments on CIFAR-100, TinyImageNet, and ImageNet-Subset demonstrate thatNAPA-VQ outperforms the State-of-the-art NECIL methods by an averageimprovement of 5%, 2%, and 4% in accuracy and 10%, 3%, and 9% in forgettingrespectively. Our code can be found in https://github.com/TamashaM/NAPA-VQ.git.                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.09297v1 |
| 198 | Self-Calibrated Cross Attention Network for Few-Shot Segmentation                                                                                    | Qianxiong Xu                   | 2023-08-18     | cs.CV                                   | The key to the success of few-shot segmentation (FSS) lies in how toeffectively utilize support samples. Most solutions compress support foreground(FG) features into prototypes, but lose some spatial details. Instead, othersuse cross attention to fuse query features with uncompressed support FG. QueryFG could be fused with support FG, however, query background (BG) cannot findmatched BG features in support FG, yet inevitably integrates dissimilarfeatures. Besides, as both query FG and BG are combined with support FG, theyget entangled, thereby leading to ineffective segmentation. To cope with theseissues, we design a self-calibrated cross attention (SCCA) block. For efficientpatch-based attention, query and support features are firstly split intopatches. Then, we design a patch alignment module to align each query patchwith its most similar support patch for better cross attention. Specifically,SCCA takes a query patch as Q, and groups the patches from the same query imageand the aligned patches from the support image as K&V. In this way, the queryBG features are fused with matched BG features (from query patches), and thusthe aforementioned issues will be mitigated. Moreover, when calculating SCCA,we design a scaled-cosine mechanism to better utilize the support features forsimilarity calculation. Extensive experiments conducted on PASCAL-5^i andCOCO-20^i demonstrate the superiority of our model, e.g., the mIoU score under5-shot setting on COCO-20^i is 5.6%+ better than previous state-of-the-arts.The code is available at https://github.com/Sam1224/SCCAN.                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.09294v1 |
| 199 | Point Contrastive Prediction with Semantic Clustering for Self-Supervised Learning on Point Cloud Videos                                             | Xiaoxiao Sheng                 | 2023-08-18     | cs.CV, cs.AI                            | We propose a unified point cloud video self-supervised learning framework forobject-centric and scene-centric data. Previous methods commonly conductrepresentation learning at the clip or frame level and cannot well capturefine-grained semantics. Instead of contrasting the representations of clips orframes, in this paper, we propose a unified self-supervised framework byconducting contrastive learning at the point level. Moreover, we introduce anew pretext task by achieving semantic alignment of superpoints, which furtherfacilitates the representations to capture semantic cues at multiple scales. Inaddition, due to the high redundancy in the temporal dimension of dynamic pointclouds, directly conducting contrastive learning at the point level usuallyleads to massive undesired negatives and insufficient modeling of positiverepresentations. To remedy this, we propose a selection strategy to retainproper negatives and make use of high-similarity samples from other instancesas positive supplements. Extensive experiments show that our method outperformssupervised counterparts on a wide range of downstream tasks and demonstratesthe superior transferability of the learned representations.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.09247v1 |
| 200 | Masked Spatio-Temporal Structure Prediction for Self-supervised Learning on Point Cloud Videos                                                       | Zhiqiang Shen                  | 2023-08-18     | cs.CV, cs.AI                            | Recently, the community has made tremendous progress in developing effectivemethods for point cloud video understanding that learn from massive amounts oflabeled data. However, annotating point cloud videos is usually notoriouslyexpensive. Moreover, training via one or only a few traditional tasks (e.g.,classification) may be insufficient to learn subtle details of thespatio-temporal structure existing in point cloud videos. In this paper, wepropose a Masked Spatio-Temporal Structure Prediction (MaST-Pre) method tocapture the structure of point cloud videos without human annotations. MaST-Preis based on spatio-temporal point-tube masking and consists of twoself-supervised learning tasks. First, by reconstructing masked point tubes,our method is able to capture the appearance information of point cloud videos.Second, to learn motion, we propose a temporal cardinality differenceprediction task that estimates the change in the number of points within apoint tube. In this way, MaST-Pre is forced to model the spatial and temporalstructure in point cloud videos. Extensive experiments on MSRAction-3D,NTU-RGBD, NvGesture, and SHREC'17 demonstrate the effectiveness of the proposedmethod.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.09245v1 |
| 201 | SparseBEV: High-Performance Sparse 3D Object Detection from Multi-Camera Videos                                                                      | Haisong Liu                    | 2023-08-18     | cs.CV                                   | Camera-based 3D object detection in BEV (Bird's Eye View) space has drawngreat attention over the past few years. Dense detectors typically follow atwo-stage pipeline by first constructing a dense BEV feature and thenperforming object detection in BEV space, which suffers from complex viewtransformations and high computation cost. On the other side, sparse detectorsfollow a query-based paradigm without explicit dense BEV feature construction,but achieve worse performance than the dense counterparts. In this paper, wefind that the key to mitigate this performance gap is the adaptability of thedetector in both BEV and image space. To achieve this goal, we proposeSparseBEV, a fully sparse 3D object detector that outperforms the densecounterparts. SparseBEV contains three key designs, which are (1)scale-adaptive self attention to aggregate features with adaptive receptivefield in BEV space, (2) adaptive spatio-temporal sampling to generate samplinglocations under the guidance of queries, and (3) adaptive mixing to decode thesampled features with dynamic weights from the queries. On the test split ofnuScenes, SparseBEV achieves the state-of-the-art performance of 67.5 NDS. Onthe val split, SparseBEV achieves 55.8 NDS while maintaining a real-timeinference speed of 23.5 FPS. Code is available athttps://github.com/MCG-NJU/SparseBEV.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.09244v1 |
| 202 | ASAG: Building Strong One-Decoder-Layer Sparse Detectors via Adaptive Sparse Anchor Generation                                                       | Shenghao Fu                    | 2023-08-18     | cs.CV                                   | Recent sparse detectors with multiple, e.g. six, decoder layers achievepromising performance but much inference time due to complex heads. Previousworks have explored using dense priors as initialization and builtone-decoder-layer detectors. Although they gain remarkable acceleration, theirperformance still lags behind their six-decoder-layer counterparts by a largemargin. In this work, we aim to bridge this performance gap while retainingfast speed. We find that the architecture discrepancy between dense and sparsedetectors leads to feature conflict, hampering the performance ofone-decoder-layer detectors. Thus we propose Adaptive Sparse Anchor Generator(ASAG) which predicts dynamic anchors on patches rather than grids in a sparseway so that it alleviates the feature conflict problem. For each image, ASAGdynamically selects which feature maps and which locations to predict, forminga fully adaptive way to generate image-specific anchors. Further, a simple andeffective Query Weighting method eases the training instability fromadaptiveness. Extensive experiments show that our method outperformsdense-initialized ones and achieves a better speed-accuracy trade-off. The codeis available at \url{https://github.com/iSEE-Laboratory/ASAG}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.09242v1 |
| 203 | Generalized Sum Pooling for Metric Learning                                                                                                          | Yeti Z. Gurbuz                 | 2023-08-18     | cs.CV, cs.LG, stat.ML                   | A common architectural choice for deep metric learning is a convolutionalneural network followed by global average pooling (GAP). Albeit simple, GAP isa highly effective way to aggregate information. One possible explanation forthe effectiveness of GAP is considering each feature vector as representing adifferent semantic entity and GAP as a convex combination of them. Followingthis perspective, we generalize GAP and propose a learnable generalized sumpooling method (GSP). GSP improves GAP with two distinct abilities: i) theability to choose a subset of semantic entities, effectively learning to ignorenuisance information, and ii) learning the weights corresponding to theimportance of each entity. Formally, we propose an entropy-smoothed optimaltransport problem and show that it is a strict generalization of GAP, i.e., aspecific realization of the problem gives back GAP. We show that thisoptimization problem enjoys analytical gradients enabling us to use it as adirect learnable replacement for GAP. We further propose a zero-shot loss toease the learning of GSP. We show the effectiveness of our method withextensive evaluations on 4 popular metric learning benchmarks. Code isavailable at: GSP-DML Framework                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.09228v2 |
| 204 | FedPerfix: Towards Partial Model Personalization of Vision Transformers in Federated Learning                                                        | Guangyu Sun                    | 2023-08-17     | cs.CV, cs.LG                            | Personalized Federated Learning (PFL) represents a promising solution fordecentralized learning in heterogeneous data environments. Partial modelpersonalization has been proposed to improve the efficiency of PFL byselectively updating local model parameters instead of aggregating all of them.However, previous work on partial model personalization has mainly focused onConvolutional Neural Networks (CNNs), leaving a gap in understanding how it canbe applied to other popular models such as Vision Transformers (ViTs). In thiswork, we investigate where and how to partially personalize a ViT model.Specifically, we empirically evaluate the sensitivity to data distribution ofeach type of layer. Based on the insights that the self-attention layer and theclassification head are the most sensitive parts of a ViT, we propose a novelapproach called FedPerfix, which leverages plugins to transfer information fromthe aggregated model to the local client as a personalization. Finally, weevaluate the proposed approach on CIFAR-100, OrganAMNIST, and Office-Homedatasets and demonstrate its effectiveness in improving the model's performancecompared to several advanced PFL methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.09160v1 |
| 205 | ImGeoNet: Image-induced Geometry-aware Voxel Representation for Multi-view 3D Object Detection                                                       | Tao Tu                         | 2023-08-17     | cs.CV                                   | We propose ImGeoNet, a multi-view image-based 3D object detection frameworkthat models a 3D space by an image-induced geometry-aware voxel representation.Unlike previous methods which aggregate 2D features into 3D voxels withoutconsidering geometry, ImGeoNet learns to induce geometry from multi-view imagesto alleviate the confusion arising from voxels of free space, and during theinference phase, only images from multiple views are required. Besides, apowerful pre-trained 2D feature extractor can be leveraged by ourrepresentation, leading to a more robust performance. To evaluate theeffectiveness of ImGeoNet, we conduct quantitative and qualitative experimentson three indoor datasets, namely ARKitScenes, ScanNetV2, and ScanNet200. Theresults demonstrate that ImGeoNet outperforms the current state-of-the-artmulti-view image-based method, ImVoxelNet, on all three datasets in terms ofdetection accuracy. In addition, ImGeoNet shows great data efficiency byachieving results comparable to ImVoxelNet with 100 views while utilizing only40 views. Furthermore, our studies indicate that our proposed image-inducedgeometry-aware representation can enable image-based methods to attain superiordetection accuracy than the seminal point cloud-based method, VoteNet, in twopractical scenarios: (1) scenarios where point clouds are sparse and noisy,such as in ARKitScenes, and (2) scenarios involve diverse object classes,particularly classes of small objects, as in the case in ScanNet200.                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.09098v1 |
| 206 | SimFIR: A Simple Framework for Fisheye Image Rectification with Self-supervised Representation Learning                                              | Hao Feng                       | 2023-08-17     | cs.CV                                   | In fisheye images, rich distinct distortion patterns are regularlydistributed in the image plane. These distortion patterns are independent ofthe visual content and provide informative cues for rectification. To make thebest of such rectification cues, we introduce SimFIR, a simple framework forfisheye image rectification based on self-supervised representation learning.Technically, we first split a fisheye image into multiple patches and extracttheir representations with a Vision Transformer (ViT). To learn fine-graineddistortion representations, we then associate different image patches withtheir specific distortion patterns based on the fisheye model, and furthersubtly design an innovative unified distortion-aware pretext task for theirlearning. The transfer performance on the downstream rectification task isremarkably boosted, which verifies the effectiveness of the learnedrepresentations. Extensive experiments are conducted, and the quantitative andqualitative results demonstrate the superiority of our method over thestate-of-the-art algorithms as well as its strong generalization ability onreal-world fisheye images.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.09040v1 |
| 207 | Identity-Seeking Self-Supervised Representation Learning for Generalizable Person Re-identification                                                  | Zhaopeng Dou                   | 2023-08-17     | cs.CV                                   | This paper aims to learn a domain-generalizable (DG) person re-identification(ReID) representation from large-scale videos \textbf{without any annotation}.Prior DG ReID methods employ limited labeled data for training due to the highcost of annotation, which restricts further advances. To overcome the barriersof data and annotation, we propose to utilize large-scale unsupervised data fortraining. The key issue lies in how to mine identity information. To this end,we propose an Identity-seeking Self-supervised Representation learning (ISR)method. ISR constructs positive pairs from inter-frame images by modeling theinstance association as a maximum-weight bipartite matching problem. Areliability-guided contrastive loss is further presented to suppress theadverse impact of noisy positive pairs, ensuring that reliable positive pairsdominate the learning process. The training cost of ISR scales approximatelylinearly with the data size, making it feasible to utilize large-scale data fortraining. The learned representation exhibits superior generalization ability.\textbf{Without human annotation and fine-tuning, ISR achieves 87.0\% Rank-1 onMarket-1501 and 56.4\% Rank-1 on MSMT17}, outperforming the best superviseddomain-generalizable method by 5.0\% and 19.5\%, respectively. In thepre-training$\rightarrow$fine-tuning scenario, ISR achieves state-of-the-artperformance, with 88.4\% Rank-1 on MSMT17. The code is at\url{https://github.com/dcp15/ISR_ICCV2023_Oral}.                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.08887v1 |
| 208 | Event-Guided Procedure Planning from Instructional Videos with Text Supervision                                                                      | An-Lan Wang                    | 2023-08-17     | cs.CV                                   | In this work, we focus on the task of procedure planning from instructionalvideos with text supervision, where a model aims to predict an action sequenceto transform the initial visual state into the goal visual state. A criticalchallenge of this task is the large semantic gap between observed visual statesand unobserved intermediate actions, which is ignored by previous works.Specifically, this semantic gap refers to that the contents in the observedvisual states are semantically different from the elements of some action textlabels in a procedure. To bridge this semantic gap, we propose a novelevent-guided paradigm, which first infers events from the observed states andthen plans out actions based on both the states and predicted events. Ourinspiration comes from that planning a procedure from an instructional video isto complete a specific event and a specific event usually involves specificactions. Based on the proposed paradigm, we contribute an Event-guidedPrompting-based Procedure Planning (E3P) model, which encodes event informationinto the sequential modeling process to support procedure planning. To furtherconsider the strong action associations within each event, our E3P adopts amask-and-predict approach for relation mining, incorporating a probabilisticmasking scheme for regularization. Extensive experiments on three datasetsdemonstrate the effectiveness of our proposed model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.08885v1 |
| 209 | Towards Semi-supervised Learning with Non-random Missing Labels                                                                                      | Yue Duan                       | 2023-08-17     | cs.LG, cs.CV                            | Semi-supervised learning (SSL) tackles the label missing problem by enablingthe effective usage of unlabeled data. While existing SSL methods focus on thetraditional setting, a practical and challenging scenario called label MissingNot At Random (MNAR) is usually ignored. In MNAR, the labeled and unlabeleddata fall into different class distributions resulting in biased labelimputation, which deteriorates the performance of SSL models. In this work,class transition tracking based Pseudo-Rectifying Guidance (PRG) is devised forMNAR. We explore the class-level guidance information obtained by the Markovrandom walk, which is modeled on a dynamically created graph built over theclass tracking matrix. PRG unifies the historical information of classdistribution and class transitions caused by the pseudo-rectifying procedure tomaintain the model's unbiased enthusiasm towards assigning pseudo-labels to allclasses, so as the quality of pseudo-labels on both popular classes and rareclasses in MNAR could be improved. Finally, we show the superior performance ofPRG across a variety of MNAR scenarios, outperforming the latest SSL approachescombining bias removal solutions by a large margin. Code and model weights areavailable at https://github.com/NJUyued/PRG4SSL-MNAR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.08872v1 |
| 210 | Realistic Full-Body Tracking from Sparse Observations via Joint-Level Modeling                                                                       | Xiaozheng Zheng                | 2023-08-17     | cs.CV                                   | To bridge the physical and virtual worlds for rapidly developed VR/ARapplications, the ability to realistically drive 3D full-body avatars is ofgreat significance. Although real-time body tracking with only the head-mounteddisplays (HMDs) and hand controllers is heavily under-constrained, a carefullydesigned end-to-end neural network is of great potential to solve the problemby learning from large-scale motion data. To this end, we propose a two-stageframework that can obtain accurate and smooth full-body motions with the threetracking signals of head and hands only. Our framework explicitly models thejoint-level features in the first stage and utilizes them as spatiotemporaltokens for alternating spatial and temporal transformer blocks to capturejoint-level correlations in the second stage. Furthermore, we design a set ofloss terms to constrain the task of a high degree of freedom, such that we canexploit the potential of our joint-level modeling. With extensive experimentson the AMASS motion dataset and real-captured data, we validate theeffectiveness of our designs and show our proposed method can achieve moreaccurate and smooth motion compared to existing approaches.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.08855v1 |
| 211 | Bag of Tricks for Long-Tailed Multi-Label Classification on Chest X-Rays                                                                             | Feng Hong                      | 2023-08-17     | cs.CV, cs.LG                            | Clinical classification of chest radiography is particularly challenging forstandard machine learning algorithms due to its inherent long-tailed andmulti-label nature. However, few attempts take into account the coupledchallenges posed by both the class imbalance and label co-occurrence, whichhinders their value to boost the diagnosis on chest X-rays (CXRs) in thereal-world scenarios. Besides, with the prevalence of pretraining techniques,how to incorporate these new paradigms into the current framework lacks of thesystematical study. This technical report presents a brief description of oursolution in the ICCV CVAMD 2023 CXR-LT Competition. We empirically explored theeffectiveness for CXR diagnosis with the integration of several advanceddesigns about data augmentation, feature extractor, classifier design, lossfunction reweighting, exogenous data replenishment, etc. In addition, weimprove the performance through simple test-time data augmentation andensemble. Our framework finally achieves 0.349 mAP on the competition test set,ranking in the top five.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.08853v1 |
| 212 | Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts                                                                        | Sunghyun Park                  | 2023-08-17     | cs.CV, cs.LG                            | Test-time adaptation (TTA) aims to adapt a pre-trained model to the targetdomain in a batch-by-batch manner during inference. While label distributionsoften exhibit imbalances in real-world scenarios, most previous TTA approachestypically assume that both source and target domain datasets have balancedlabel distribution. Due to the fact that certain classes appear more frequentlyin certain domains (e.g., buildings in cities, trees in forests), it is naturalthat the label distribution shifts as the domain changes. However, we discoverthat the majority of existing TTA methods fail to address the coexistence ofcovariate and label shifts. To tackle this challenge, we propose a novel labelshift adapter that can be incorporated into existing TTA approaches to dealwith label shifts during the TTA process effectively. Specifically, we estimatethe label distribution of the target domain to feed it into the label shiftadapter. Subsequently, the label shift adapter produces optimal parameters forthe target label distribution. By predicting only the parameters for a part ofthe pre-trained source model, our approach is computationally efficient and canbe easily applied, regardless of the model architectures. Through extensiveexperiments, we demonstrate that integrating our strategy with TTA approachesleads to substantial performance improvements under the joint presence of labeland covariate shifts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.08810v1 |
| 213 | Long-Range Grouping Transformer for Multi-View 3D Reconstruction                                                                                     | Liying Yang                    | 2023-08-17     | cs.CV                                   | Nowadays, transformer networks have demonstrated superior performance in manycomputer vision tasks. In a multi-view 3D reconstruction algorithm followingthis paradigm, self-attention processing has to deal with intricate imagetokens including massive information when facing heavy amounts of view input.The curse of information content leads to the extreme difficulty of modellearning. To alleviate this problem, recent methods compress the token numberrepresenting each view or discard the attention operations between the tokensfrom different views. Obviously, they give a negative impact on performance.Therefore, we propose long-range grouping attention (LGA) based on thedivide-and-conquer principle. Tokens from all views are grouped for separateattention operations. The tokens in each group are sampled from all views andcan provide macro representation for the resided view. The richness of featurelearning is guaranteed by the diversity among different groups. An effectiveand efficient encoder can be established which connects inter-view featuresusing LGA and extract intra-view features using the standard self-attentionlayer. Moreover, a novel progressive upsampling decoder is also designed forvoxel generation with relatively high resolution. Hinging on the above, weconstruct a powerful transformer-based network, called LRGT. Experimentalresults on ShapeNet verify our method achieves SOTA accuracy in multi-viewreconstruction. Code will be available athttps://github.com/LiyingCV/Long-Range-Grouping-Transformer.                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.08724v1 |
| 214 | V-FUSE: Volumetric Depth Map Fusion with Long-Range Constraints                                                                                      | Nathaniel Burgdorfer           | 2023-08-17     | cs.CV                                   | We introduce a learning-based depth map fusion framework that accepts a setof depth and confidence maps generated by a Multi-View Stereo (MVS) algorithmas input and improves them. This is accomplished by integrating volumetricvisibility constraints that encode long-range surface relationships acrossdifferent views into an end-to-end trainable architecture. We also introduce adepth search window estimation sub-network trained jointly with the largerfusion sub-network to reduce the depth hypothesis search space along each ray.Our method learns to model depth consensus and violations of visibilityconstraints directly from the data; effectively removing the necessity offine-tuning fusion parameters. Extensive experiments on MVS datasets showsubstantial improvements in the accuracy of the output fused depth andconfidence maps.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.08715v1 |
| 215 | MeViS: A Large-scale Benchmark for Video Segmentation with Motion Expressions                                                                        | Henghui Ding                   | 2023-08-16     | cs.CV                                   | This paper strives for motion expressions guided video segmentation, whichfocuses on segmenting objects in video content based on a sentence describingthe motion of the objects. Existing referring video object datasets typicallyfocus on salient objects and use language expressions that contain excessivestatic attributes that could potentially enable the target object to beidentified in a single frame. These datasets downplay the importance of motionin video content for language-guided video object segmentation. To investigatethe feasibility of using motion expressions to ground and segment objects invideos, we propose a large-scale dataset called MeViS, which contains numerousmotion expressions to indicate target objects in complex environments. Webenchmarked 5 existing referring video object segmentation (RVOS) methods andconducted a comprehensive comparison on the MeViS dataset. The results showthat current RVOS methods cannot effectively address motion expression-guidedvideo segmentation. We further analyze the challenges and propose a baselineapproach for the proposed MeViS dataset. The goal of our benchmark is toprovide a platform that enables the development of effective language-guidedvideo segmentation algorithms that leverage motion expressions as a primary cuefor object segmentation in complex video scenes. The proposed MeViS dataset hasbeen released at https://henghuiding.github.io/MeViS.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.08544v1 |
| 216 | Ref-DVGO: Reflection-Aware Direct Voxel Grid Optimization for an Improved Quality-Efficiency Trade-Off in Reflective Scene Reconstruction            | Georgios Kouros                | 2023-08-16     | cs.CV, cs.GR                            | Neural Radiance Fields (NeRFs) have revolutionized the field of novel viewsynthesis, demonstrating remarkable performance. However, the modeling andrendering of reflective objects remain challenging problems. Recent methodshave shown significant improvements over the baselines in handling reflectivescenes, albeit at the expense of efficiency. In this work, we aim to strike abalance between efficiency and quality. To this end, we investigate animplicit-explicit approach based on conventional volume rendering to enhancethe reconstruction quality and accelerate the training and rendering processes.We adopt an efficient density-based grid representation and reparameterize thereflected radiance in our pipeline. Our proposed reflection-aware approachachieves a competitive quality efficiency trade-off compared to competingmethods. Based on our experimental results, we propose and discuss hypothesesregarding the factors influencing the results of density-based methods forreconstructing reflective objects. The source code is available athttps://github.com/gkouros/ref-dvgo.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.08530v3 |
| 217 | Learning to Distill Global Representation for Sparse-View CT                                                                                         | Zilong Li                      | 2023-08-16     | eess.IV, cs.CV                          | Sparse-view computed tomography (CT) -- using a small number of projectionsfor tomographic reconstruction -- enables much lower radiation dose to patientsand accelerated data acquisition. The reconstructed images, however, sufferfrom strong artifacts, greatly limiting their diagnostic value. Current trendsfor sparse-view CT turn to the raw data for better information recovery. Theresultant dual-domain methods, nonetheless, suffer from secondary artifacts,especially in ultra-sparse view scenarios, and their generalization to otherscanners/protocols is greatly limited. A crucial question arises: have theimage post-processing methods reached the limit? Our answer is not yet. In thispaper, we stick to image post-processing methods due to great flexibility andpropose global representation (GloRe) distillation framework for sparse-viewCT, termed GloReDi. First, we propose to learn GloRe with Fourier convolution,so each element in GloRe has an image-wide receptive field. Second, unlikemethods that only use the full-view images for supervision, we propose todistill GloRe from intermediate-view reconstructed images that are readilyavailable but not explored in previous literature. The success of GloRedistillation is attributed to two key components: representation directionaldistillation to align the GloRe directions, and band-pass-specific contrastivedistillation to gain clinically important details. Extensive experimentsdemonstrate the superiority of the proposed GloReDi over the state-of-the-artmethods, including dual-domain ones. The source code is available athttps://github.com/longzilicart/GloReDi.                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.08463v2 |
| 218 | Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer                                                                               | Guangyi Chen                   | 2023-08-16     | cs.CV                                   | Video-language pre-trained models have shown remarkable success in guidingvideo question-answering (VideoQA) tasks. However, due to the length of videosequences, training large-scale video-based models incurs considerably highercosts than training image-based ones. This motivates us to leverage theknowledge from image-based pretraining, despite the obvious gaps between imageand video domains. To bridge these gaps, in this paper, we propose Tem-Adapter,which enables the learning of temporal dynamics and complex semantics by avisual Temporal Aligner and a textual Semantic Aligner. Unlike conventionalpretrained knowledge adaptation methods that only concentrate on the downstreamtask objective, the Temporal Aligner introduces an extra language-guidedautoregressive task aimed at facilitating the learning of temporaldependencies, with the objective of predicting future states based onhistorical clues and language guidance that describes event progression.Besides, to reduce the semantic gap and adapt the textual representation forbetter event description, we introduce a Semantic Aligner that first designs atemplate to fuse question and answer pairs as event descriptions and thenlearns a Transformer decoder with the whole video sequence as guidance forrefinement. We evaluate Tem-Adapter and different pre-train transferringmethods on two VideoQA benchmarks, and the significant performance improvementdemonstrates the effectiveness of our method.                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.08414v1 |
| 219 | Agglomerative Transformer for Human-Object Interaction Detection                                                                                     | Danyang Tu                     | 2023-08-16     | cs.CV, cs.AI                            | We propose an agglomerative Transformer (AGER) that enables Transformer-basedhuman-object interaction (HOI) detectors to flexibly exploit extrainstance-level cues in a single-stage and end-to-end manner for the first time.AGER acquires instance tokens by dynamically clustering patch tokens andaligning cluster centers to instances with textual guidance, thus enjoying twobenefits: 1) Integrality: each instance token is encouraged to contain alldiscriminative feature regions of an instance, which demonstrates a significantimprovement in the extraction of different instance-level cues and subsequentlyleads to a new state-of-the-art performance of HOI detection with 36.75 mAP onHICO-Det. 2) Efficiency: the dynamical clustering mechanism allows AGER togenerate instance tokens jointly with the feature learning of the Transformerencoder, eliminating the need of an additional object detector or instancedecoder in prior methods, thus allowing the extraction of desirable extra cuesfor HOI detection in a single-stage and end-to-end pipeline. Concretely, AGERreduces GFLOPs by 8.5% and improves FPS by 36%, even compared to a vanillaDETR-like pipeline without extra cue extraction.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.08370v1 |
| 220 | Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations                                                           | Yuewei Yang                    | 2023-08-16     | cs.CV                                   | In recent years, discriminative self-supervised methods have made significantstrides in advancing various visual tasks. The central idea of learning a dataencoder that is robust to data distortions/augmentations is straightforward yethighly effective. Although many studies have demonstrated the empirical successof various learning methods, the resulting learned representations can exhibitinstability and hinder downstream performance. In this study, we analyzediscriminative self-supervised methods from a causal perspective to explainthese unstable behaviors and propose solutions to overcome them. Our approachdraws inspiration from prior works that empirically demonstrate the ability ofdiscriminative self-supervised methods to demix ground truth causal sources tosome extent. Unlike previous work on causality-empowered representationlearning, we do not apply our solutions during the training process but ratherduring the inference process to improve time efficiency. Through experiments onboth controlled image datasets and realistic image datasets, we show that ourproposed solutions, which involve tempering a linear transformation withcontrolled synthetic data, are effective in addressing these issues.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.08321v1 |
| 221 | Low-Light Image Enhancement with Illumination-Aware Gamma Correction and Complete Image Modelling Network                                            | Yinglong Wang                  | 2023-08-16     | cs.CV                                   | This paper presents a novel network structure with illumination-aware gammacorrection and complete image modelling to solve the low-light imageenhancement problem. Low-light environments usually lead to less informativelarge-scale dark areas, directly learning deep representations from low-lightimages is insensitive to recovering normal illumination. We propose tointegrate the effectiveness of gamma correction with the strong modellingcapacities of deep networks, which enables the correction factor gamma to belearned in a coarse to elaborate manner via adaptively perceiving the deviatedillumination. Because exponential operation introduces high computationalcomplexity, we propose to use Taylor Series to approximate gamma correction,accelerating the training and inference speed. Dark areas usually occupy largescales in low-light images, common local modelling structures, e.g., CNN,SwinIR, are thus insufficient to recover accurate illumination across wholelow-light images. We propose a novel Transformer block to completely simulatethe dependencies of all pixels across images via a local-to-global hierarchicalattention mechanism, so that dark areas could be inferred by borrowing theinformation from far informative regions in a highly effective manner.Extensive experiments on several benchmark datasets demonstrate that ourapproach outperforms state-of-the-art methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.08220v1 |
| 222 | Unsupervised Domain Adaptive Detection with Network Stability Analysis                                                                               | Wenzhang Zhou                  | 2023-08-16     | cs.CV                                   | Domain adaptive detection aims to improve the generality of a detector,learned from the labeled source domain, on the unlabeled target domain. In thiswork, drawing inspiration from the concept of stability from the control theorythat a robust system requires to remain consistent both externally andinternally regardless of disturbances, we propose a novel framework thatachieves unsupervised domain adaptive detection through stability analysis. Inspecific, we treat discrepancies between images and regions from differentdomains as disturbances, and introduce a novel simple but effective NetworkStability Analysis (NSA) framework that considers various disturbances fordomain adaptation. Particularly, we explore three types of perturbationsincluding heavy and light image-level disturbances and instanceleveldisturbance. For each type, NSA performs external consistency analysis on theoutputs from raw and perturbed images and/or internal consistency analysis ontheir features, using teacher-student models. By integrating NSA into FasterR-CNN, we immediately achieve state-of-the-art results. In particular, we set anew record of 52.7% mAP on Cityscapes-to-FoggyCityscapes, showing the potentialof NSA for domain adaptive detection. It is worth noticing, our NSA is designedfor general purpose, and thus applicable to one-stage detection model (e.g.,FCOS) besides the adopted one, as shown by experiments.https://github.com/tiankongzhang/NSA.                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.08182v1 |
| 223 | Learning to Generate Semantic Layouts for Higher Text-Image Correspondence in Text-to-Image Synthesis                                                | Minho Park                     | 2023-08-16     | cs.CV                                   | Existing text-to-image generation approaches have set high standards forphotorealism and text-image correspondence, largely benefiting from web-scaletext-image datasets, which can include up to 5~billion pairs. However,text-to-image generation models trained on domain-specific datasets, such asurban scenes, medical images, and faces, still suffer from low text-imagecorrespondence due to the lack of text-image pairs. Additionally, collectingbillions of text-image pairs for a specific domain can be time-consuming andcostly. Thus, ensuring high text-image correspondence without relying onweb-scale text-image datasets remains a challenging task. In this paper, wepresent a novel approach for enhancing text-image correspondence by leveragingavailable semantic layouts. Specifically, we propose a Gaussian-categoricaldiffusion process that simultaneously generates both images and correspondinglayout pairs. Our experiments reveal that we can guide text-to-image generationmodels to be aware of the semantics of different image regions, by training themodel to generate semantic labels for each pixel. We demonstrate that ourapproach achieves higher text-image correspondence compared to existingtext-to-image generation approaches in the Multi-Modal CelebA-HQ and theCityscapes dataset, where text-image pairs are scarce. Codes are available inthis https://pmh9960.github.io/research/GCDP                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.08157v1 |
| 224 | GPA-3D: Geometry-aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds                                    | Ziyu Li                        | 2023-08-16     | cs.CV                                   | LiDAR-based 3D detection has made great progress in recent years. However,the performance of 3D detectors is considerably limited when deployed in unseenenvironments, owing to the severe domain gap problem. Existing domain adaptive3D detection methods do not adequately consider the problem of thedistributional discrepancy in feature space, thereby hindering generalizationof detectors across domains. In this work, we propose a novel unsuperviseddomain adaptive \textbf{3D} detection framework, namely \textbf{G}eometry-aware\textbf{P}rototype \textbf{A}lignment (\textbf{GPA-3D}), which explicitlyleverages the intrinsic geometric relationship from point cloud objects toreduce the feature discrepancy, thus facilitating cross-domain transferring.Specifically, GPA-3D assigns a series of tailored and learnable prototypes topoint cloud objects with distinct geometric structures. Each prototype alignsBEV (bird's-eye-view) features derived from corresponding point cloud objectson source and target domains, reducing the distributional discrepancy andachieving better adaptation. The evaluation results obtained on variousbenchmarks, including Waymo, nuScenes and KITTI, demonstrate the superiority ofour GPA-3D over the state-of-the-art approaches for different adaptationscenarios. The MindSpore version code will be publicly available at\url{https://github.com/Liz66666/GPA3D}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.08140v1 |
| 225 | OmniZoomer: Learning to Move and Zoom in on Sphere at High-Resolution                                                                                | Zidong Cao                     | 2023-08-16     | cs.CV, cs.AI                            | Omnidirectional images (ODIs) have become increasingly popular, as theirlarge field-of-view (FoV) can offer viewers the chance to freely choose theview directions in immersive environments such as virtual reality. The M\"obiustransformation is typically employed to further provide the opportunity formovement and zoom on ODIs, but applying it to the image level often results inblurry effect and aliasing problem. In this paper, we propose a novel deeplearning-based approach, called \textbf{OmniZoomer}, to incorporate theM\"obius transformation into the network for movement and zoom on ODIs. Bylearning various transformed feature maps under different conditions, thenetwork is enhanced to handle the increasing edge curvatures, which alleviatesthe blurry effect. Moreover, to address the aliasing problem, we propose twokey components. Firstly, to compensate for the lack of pixels for describingcurves, we enhance the feature maps in the high-resolution (HR) space andcalculate the transformed index map with a spatial index generation module.Secondly, considering that ODIs are inherently represented in the sphericalspace, we propose a spherical resampling module that combines the index map andHR feature maps to transform the feature maps for better spherical correlation.The transformed feature maps are decoded to output a zoomed ODI. Experimentsshow that our method can produce HR and high-quality ODIs with the flexibilityto move and zoom in to the object of interest. Project page is available athttp://vlislab22.github.io/OmniZoomer/.                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.08114v2 |
| 226 | View Consistent Purification for Accurate Cross-View Localization                                                                                    | Shan Wang                      | 2023-08-16     | cs.CV                                   | This paper proposes a fine-grained self-localization method for outdoorrobotics that utilizes a flexible number of onboard cameras and readilyaccessible satellite images. The proposed method addresses limitations inexisting cross-view localization methods that struggle to handle noise sourcessuch as moving objects and seasonal variations. It is the first sparsevisual-only method that enhances perception in dynamic environments bydetecting view-consistent key points and their corresponding deep features fromground and satellite views, while removing off-the-ground objects andestablishing homography transformation between the two views. Moreover, theproposed method incorporates a spatial embedding approach that leverages cameraintrinsic and extrinsic information to reduce the ambiguity of purely visualmatching, leading to improved feature matching and overall pose estimationaccuracy. The method exhibits strong generalization and is robust toenvironmental changes, requiring only geo-poses as ground truth. Extensiveexperiments on the KITTI and Ford Multi-AV Seasonal datasets demonstrate thatour proposed method outperforms existing state-of-the-art methods, achievingmedian spatial accuracy errors below $0.5$ meters along the lateral andlongitudinal directions, and a median orientation accuracy error below 2degrees.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.08110v1 |
| 227 | Memory-and-Anticipation Transformer for Online Action Understanding                                                                                  | Jiahao Wang                    | 2023-08-15     | cs.CV                                   | Most existing forecasting systems are memory-based methods, which attempt tomimic human forecasting ability by employing various memory mechanisms and haveprogressed in temporal modeling for memory dependency. Nevertheless, an obviousweakness of this paradigm is that it can only model limited historicaldependence and can not transcend the past. In this paper, we rethink thetemporal dependence of event evolution and propose a novelmemory-anticipation-based paradigm to model an entire temporal structure,including the past, present, and future. Based on this idea, we presentMemory-and-Anticipation Transformer (MAT), a memory-anticipation-basedapproach, to address the online action detection and anticipation tasks. Inaddition, owing to the inherent superiority of MAT, it can process onlineaction detection and anticipation tasks in a unified manner. The proposed MATmodel is tested on four challenging benchmarks TVSeries, THUMOS'14, HDD, andEPIC-Kitchens-100, for online action detection and anticipation tasks, and itsignificantly outperforms all existing methods. Code is available athttps://github.com/Echo0125/Memory-and-Anticipation-Transformer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.07893v1 |
| 228 | ObjectSDF++: Improved Object-Compositional Neural Implicit Surfaces                                                                                  | Qianyi Wu                      | 2023-08-15     | cs.CV                                   | In recent years, neural implicit surface reconstruction has emerged as apopular paradigm for multi-view 3D reconstruction. Unlike traditionalmulti-view stereo approaches, the neural implicit surface-based methodsleverage neural networks to represent 3D scenes as signed distance functions(SDFs). However, they tend to disregard the reconstruction of individualobjects within the scene, which limits their performance and practicalapplications. To address this issue, previous work ObjectSDF introduced a niceframework of object-composition neural implicit surfaces, which utilizes 2Dinstance masks to supervise individual object SDFs. In this paper, we propose anew framework called ObjectSDF++ to overcome the limitations of ObjectSDF.First, in contrast to ObjectSDF whose performance is primarily restricted byits converted semantic field, the core component of our model is anocclusion-aware object opacity rendering formulation that directlyvolume-renders object opacity to be supervised with instance masks. Second, wedesign a novel regularization term for object distinction, which caneffectively mitigate the issue that ObjectSDF may result in unexpectedreconstruction in invisible regions due to the lack of constraint to preventcollisions. Our extensive experiments demonstrate that our novel framework notonly produces superior object reconstruction results but also significantlyimproves the quality of scene reconstruction. Code and more resources can befound in \url{https://qianyiwu.github.io/objectsdf++}                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.07868v2 |
| 229 | StyleDiffusion: Controllable Disentangled Style Transfer via Diffusion Models                                                                        | Zhizhong Wang                  | 2023-08-15     | cs.CV                                   | Content and style (C-S) disentanglement is a fundamental problem and criticalchallenge of style transfer. Existing approaches based on explicit definitions(e.g., Gram matrix) or implicit learning (e.g., GANs) are neither interpretablenor easy to control, resulting in entangled representations and less satisfyingresults. In this paper, we propose a new C-S disentangled framework for styletransfer without using previous assumptions. The key insight is to explicitlyextract the content information and implicitly learn the complementary styleinformation, yielding interpretable and controllable C-S disentanglement andstyle transfer. A simple yet effective CLIP-based style disentanglement losscoordinated with a style reconstruction prior is introduced to disentangle C-Sin the CLIP image space. By further leveraging the powerful style removal andgenerative ability of diffusion models, our framework achieves superior resultsthan state of the art and flexible C-S disentanglement and trade-off control.Our work provides new insights into the C-S disentanglement in style transferand demonstrates the potential of diffusion models for learningwell-disentangled C-S characteristics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.07863v1 |
| 230 | ImbSAM: A Closer Look at Sharpness-Aware Minimization in Class-Imbalanced Recognition                                                                | Yixuan Zhou                    | 2023-08-15     | cs.CV                                   | Class imbalance is a common challenge in real-world recognition tasks, wherethe majority of classes have few samples, also known as tail classes. Weaddress this challenge with the perspective of generalization and empiricallyfind that the promising Sharpness-Aware Minimization (SAM) fails to addressgeneralization issues under the class-imbalanced setting. Through investigatingthis specific type of task, we identify that its generalization bottleneckprimarily lies in the severe overfitting for tail classes with limited trainingdata. To overcome this bottleneck, we leverage class priors to restrict thegeneralization scope of the class-agnostic SAM and propose a class-awaresmoothness optimization algorithm named Imbalanced-SAM (ImbSAM). With theguidance of class priors, our ImbSAM specifically improves generalizationtargeting tail classes. We also verify the efficacy of ImbSAM on twoprototypical applications of class-imbalanced recognition: long-tailedclassification and semi-supervised anomaly detection, where our ImbSAMdemonstrates remarkable performance improvements for tail classes and anomaly.Our code implementation is available athttps://github.com/cool-xuan/Imbalanced_SAM.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.07815v1 |
| 231 | DiffV2S: Diffusion-based Video-to-Speech Synthesis with Vision-guided Speaker Embedding                                                              | Jeongsoo Choi                  | 2023-08-15     | cs.SD, cs.CV, cs.LG, eess.AS            | Recent research has demonstrated impressive results in video-to-speechsynthesis which involves reconstructing speech solely from visual input.However, previous works have struggled to accurately synthesize speech due to alack of sufficient guidance for the model to infer the correct content with theappropriate sound. To resolve the issue, they have adopted an extra speakerembedding as a speaking style guidance from a reference auditory information.Nevertheless, it is not always possible to obtain the audio information fromthe corresponding video input, especially during the inference time. In thispaper, we present a novel vision-guided speaker embedding extractor using aself-supervised pre-trained model and prompt tuning technique. In doing so, therich speaker embedding information can be produced solely from input visualinformation, and the extra audio information is not necessary during theinference time. Using the extracted vision-guided speaker embeddingrepresentations, we further develop a diffusion-based video-to-speech synthesismodel, so called DiffV2S, conditioned on those speaker embeddings and thevisual representation extracted from the input video. The proposed DiffV2S notonly maintains phoneme details contained in the input video frames, but alsocreates a highly intelligible mel-spectrogram in which the speaker identitiesof the multiple speakers are all preserved. Our experimental results show thatDiffV2S achieves the state-of-the-art performance compared to the previousvideo-to-speech synthesis technique.                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.07787v1 |
| 232 | Self-supervised Hypergraphs for Learning Multiple World Interpretations                                                                              | Alina Marcu                    | 2023-08-15     | cs.CV                                   | We present a method for learning multiple scene representations given a smalllabeled set, by exploiting the relationships between such representations inthe form of a multi-task hypergraph. We also show how we can use the hypergraphto improve a powerful pretrained VisTransformer model without any additionallabeled data. In our hypergraph, each node is an interpretation layer (e.g.,depth or segmentation) of the scene. Within each hyperedge, one or severalinput nodes predict the layer at the output node. Thus, each node could be aninput node in some hyperedges and an output node in others. In this way,multiple paths can reach the same node, to form ensembles from which we obtainrobust pseudolabels, which allow self-supervised learning in the hypergraph. Wetest different ensemble models and different types of hyperedges and showsuperior performance to other multi-task graph models in the field. We alsointroduce Dronescapes, a large video dataset captured with UAVs in differentcomplex real-world scenes, with multiple representations, suitable formulti-task learning.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.07615v2 |
| 233 | Story Visualization by Online Text Augmentation with Context Memory                                                                                  | Daechul Ahn                    | 2023-08-15     | cs.CV, cs.AI, cs.LG                     | Story visualization (SV) is a challenging text-to-image generation task forthe difficulty of not only rendering visual details from the text descriptionsbut also encoding a long-term context across multiple sentences. While priorefforts mostly focus on generating a semantically relevant image for eachsentence, encoding a context spread across the given paragraph to generatecontextually convincing images (e.g., with a correct character or with a properbackground of the scene) remains a challenge. To this end, we propose a novelmemory architecture for the Bi-directional Transformer framework with an onlinetext augmentation that generates multiple pseudo-descriptions as supplementarysupervision during training for better generalization to the language variationat inference. In extensive experiments on the two popular SV benchmarks, i.e.,the Pororo-SV and Flintstones-SV, the proposed method significantly outperformsthe state of the arts in various metrics including FID, character F1, frameaccuracy, BLEU-2/3, and R-precision with similar or less computationalcomplexity.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.07575v2 |
| 234 | 3DHacker: Spectrum-based Decision Boundary Generation for Hard-label 3D Point Cloud Attack                                                           | Yunbo Tao                      | 2023-08-15     | cs.CV                                   | With the maturity of depth sensors, the vulnerability of 3D point cloudmodels has received increasing attention in various applications such asautonomous driving and robot navigation. Previous 3D adversarial attackerseither follow the white-box setting to iteratively update the coordinateperturbations based on gradients, or utilize the output model logits toestimate noisy gradients in the black-box setting. However, these attackmethods are hard to be deployed in real-world scenarios since realistic 3Dapplications will not share any model details to users. Therefore, we explore amore challenging yet practical 3D attack setting, \textit{i.e.}, attackingpoint clouds with black-box hard labels, in which the attacker can only haveaccess to the prediction label of the input. To tackle this setting, we proposea novel 3D attack method, termed \textbf{3D} \textbf{H}ard-labelatt\textbf{acker} (\textbf{3DHacker}), based on the developed decision boundaryalgorithm to generate adversarial samples solely with the knowledge of classlabels. Specifically, to construct the class-aware model decision boundary,3DHacker first randomly fuses two point clouds of different classes in thespectral domain to craft their intermediate sample with high imperceptibility,then projects it onto the decision boundary via binary search. To restrict thefinal perturbation size, 3DHacker further introduces an iterative optimizationstrategy to move the intermediate sample along the decision boundary forgenerating adversarial point clouds with smallest trivial perturbations.Extensive evaluations show that, even in the challenging hard-label setting,3DHacker still competitively outperforms existing 3D attacks regarding theattack performance as well as adversary quality.                                                                                                                                | http://arxiv.org/abs/2308.07546v1 |
| 235 | DREAMWALKER: Mental Planning for Continuous Vision-Language Navigation                                                                               | Hanqing Wang                   | 2023-08-14     | cs.CV, cs.AI                            | VLN-CE is a recently released embodied task, where AI agents need to navigatea freely traversable environment to reach a distant target location, givenlanguage instructions. It poses great challenges due to the huge space ofpossible strategies. Driven by the belief that the ability to anticipate theconsequences of future actions is crucial for the emergence of intelligent andinterpretable planning behavior, we propose DREAMWALKER -- a world model basedVLN-CE agent. The world model is built to summarize the visual, topological,and dynamic properties of the complicated continuous environment into adiscrete, structured, and compact representation. DREAMWALKER can simulate andevaluate possible plans entirely in such internal abstract world, beforeexecuting costly actions. As opposed to existing model-free VLN-CE agentssimply making greedy decisions in the real world, which easily results inshortsighted behaviors, DREAMWALKER is able to make strategic planning throughlarge amounts of ``mental experiments.'' Moreover, the imagined futurescenarios reflect our agent's intention, making its decision-making processmore transparent. Extensive experiments and ablation studies on VLN-CE datasetconfirm the effectiveness of the proposed approach and outline fruitfuldirections for future work.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.07498v1 |
| 236 | Probabilistic MIMO U-Net: Efficient and Accurate Uncertainty Estimation for Pixel-wise Regression                                                    | Anton Baumann                  | 2023-08-14     | cs.CV, eess.IV                          | Uncertainty estimation in machine learning is paramount for enhancing thereliability and interpretability of predictive models, especially inhigh-stakes real-world scenarios. Despite the availability of numerous methods,they often pose a trade-off between the quality of uncertainty estimation andcomputational efficiency. Addressing this challenge, we present an adaptationof the Multiple-Input Multiple-Output (MIMO) framework -- an approachexploiting the overparameterization of deep neural networks -- for pixel-wiseregression tasks. Our MIMO variant expands the applicability of the approachfrom simple image classification to broader computer vision domains. For thatpurpose, we adapted the U-Net architecture to train multiple subnetworks withina single model, harnessing the overparameterization in deep neural networks.Additionally, we introduce a novel procedure for synchronizing subnetworkperformance within the MIMO framework. Our comprehensive evaluations of theresulting MIMO U-Net on two orthogonal datasets demonstrate comparable accuracyto existing models, superior calibration on in-distribution data, robustout-of-distribution detection capabilities, and considerable improvements inparameter size and inference time. Code available atgithub.com/antonbaumann/MIMO-Unet                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.07477v1 |
| 237 | PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects                                                                         | Jiayi Liu                      | 2023-08-14     | cs.CV, cs.AI, cs.GR                     | We address the task of simultaneous part-level reconstruction and motionparameter estimation for articulated objects. Given two sets of multi-viewimages of an object in two static articulation states, we decouple the movablepart from the static part and reconstruct shape and appearance while predictingthe motion parameters. To tackle this problem, we present PARIS: aself-supervised, end-to-end architecture that learns part-level implicit shapeand appearance models and optimizes motion parameters jointly without any 3Dsupervision, motion, or semantic annotation. Our experiments show that ourmethod generalizes better across object categories, and outperforms baselinesand prior work that are given 3D point clouds as input. Our approach improvesreconstruction relative to state-of-the-art baselines with a Chamfer-L1distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, andachieves 5% error rate for motion estimation across 10 object categories.  Video summary at: https://youtu.be/tDSrROPCgUc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.07391v1 |
| 238 | Group Pose: A Simple Baseline for End-to-End Multi-person Pose Estimation                                                                            | Huan Liu                       | 2023-08-14     | cs.CV                                   | In this paper, we study the problem of end-to-end multi-person poseestimation. State-of-the-art solutions adopt the DETR-like framework, andmainly develop the complex decoder, e.g., regarding pose estimation as keypointbox detection and combining with human detection in ED-Pose, hierarchicallypredicting with pose decoder and joint (keypoint) decoder in PETR. We present asimple yet effective transformer approach, named Group Pose. We simply regard$K$-keypoint pose estimation as predicting a set of $N\times K$ keypointpositions, each from a keypoint query, as well as representing each pose withan instance query for scoring $N$ pose predictions. Motivated by the intuitionthat the interaction, among across-instance queries of different types, is notdirectly helpful, we make a simple modification to decoder self-attention. Wereplace single self-attention over all the $N\times(K+1)$ queries with twosubsequent group self-attentions: (i) $N$ within-instance self-attention, witheach over $K$ keypoint queries and one instance query, and (ii) $(K+1)$same-type across-instance self-attention, each over $N$ queries of the sametype. The resulting decoder removes the interaction among across-instancetype-different queries, easing the optimization and thus improving theperformance. Experimental results on MS COCO and CrowdPose show that ourapproach without human box supervision is superior to previous methods withcomplex decoders, and even is slightly better than ED-Pose that uses human boxsupervision. $\href{https://github.com/Michel-liu/GroupPose-Paddle}{\rmPaddle}$ and $\href{https://github.com/Michel-liu/GroupPose}{\rm PyTorch}$ codeare available.                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.07313v1 |
| 239 | Context-Aware Planning and Environment-Aware Memory for Instruction Following Embodied Agents                                                        | Byeonghwi Kim                  | 2023-08-14     | cs.RO, cs.AI                            | Accomplishing household tasks requires to plan step-by-step actionsconsidering the consequences of previous actions. However, the state-of-the-artembodied agents often make mistakes in navigating the environment andinteracting with proper objects due to imperfect learning by imitating expertsor algorithmic planners without such knowledge. To improve both visualnavigation and object interaction, we propose to consider the consequence oftaken actions by CAPEAM (Context-Aware Planning and Environment-Aware Memory)that incorporates semantic context (e.g., appropriate objects to interact with)in a sequence of actions, and the changed spatial arrangement and states ofinteracted objects (e.g., location that the object has been moved to) ininferring the subsequent actions. We empirically show that the agent with theproposed CAPEAM achieves state-of-the-art performance in various metrics usinga challenging interactive instruction following benchmark in both seen andunseen environments by large margins (up to +10.70% in unseen env.).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.07241v3 |
| 240 | Distance Matters For Improving Performance Estimation Under Covariate Shift                                                                          | Mélanie Roschewitz             | 2023-08-14     | cs.CV, cs.LG                            | Performance estimation under covariate shift is a crucial component of safeAI model deployment, especially for sensitive use-cases. Recently, severalsolutions were proposed to tackle this problem, most leveraging modelpredictions or softmax confidence to derive accuracy estimates. However, underdataset shifts, confidence scores may become ill-calibrated if samples are toofar from the training distribution. In this work, we show that taking intoaccount distances of test samples to their expected training distribution cansignificantly improve performance estimation under covariate shift. Precisely,we introduce a "distance-check" to flag samples that lie too far from theexpected distribution, to avoid relying on their untrustworthy model outputs inthe accuracy estimation step. We demonstrate the effectiveness of this methodon 13 image classification tasks, across a wide-range of natural and syntheticdistribution shifts and hundreds of models, with a median relative MAEimprovement of 27% over the best baseline across all tasks, and SOTAperformance on 10 out of 13 tasks. Our code is publicly available athttps://github.com/melanibe/distance_matters_performance_estimation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.07223v1 |
| 241 | HyperSparse Neural Networks: Shifting Exploration to Exploitation through Adaptive Regularization                                                    | Patrick Glandorf               | 2023-08-14     | cs.CV                                   | Sparse neural networks are a key factor in developing resource-efficientmachine learning applications. We propose the novel and powerful sparselearning method Adaptive Regularized Training (ART) to compress dense intosparse networks. Instead of the commonly used binary mask during training toreduce the number of model weights, we inherently shrink weights close to zeroin an iterative manner with increasing weight regularization. Our methodcompresses the pre-trained model knowledge into the weights of highestmagnitude. Therefore, we introduce a novel regularization loss namedHyperSparse that exploits the highest weights while conserving the ability ofweight exploration. Extensive experiments on CIFAR and TinyImageNet show thatour method leads to notable performance gains compared to other sparsificationmethods, especially in extremely high sparsity regimes up to 99.8 percent modelsparsity. Additional investigations provide new insights into the patterns thatare encoded in weights with high magnitudes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.07163v2 |
| 242 | DELO: Deep Evidential LiDAR Odometry using Partial Optimal Transport                                                                                 | Sk Aziz Ali                    | 2023-08-14     | cs.CV                                   | Accurate, robust, and real-time LiDAR-based odometry (LO) is imperative formany applications like robot navigation, globally consistent 3D scene mapreconstruction, or safe motion-planning. Though LiDAR sensor is known for itsprecise range measurement, the non-uniform and uncertain point sampling densityinduce structural inconsistencies. Hence, existing supervised and unsupervisedpoint set registration methods fail to establish one-to-one matchingcorrespondences between LiDAR frames. We introduce a novel deep learning-basedreal-time (approx. 35-40ms per frame) LO method that jointly learns accurateframe-to-frame correspondences and model's predictive uncertainty (PU) asevidence to safe-guard LO predictions. In this work, we propose (i) partialoptimal transportation of LiDAR feature descriptor for robust LO estimation,(ii) joint learning of predictive uncertainty while learning odometry overdriving sequences, and (iii) demonstrate how PU can serve as evidence fornecessary pose-graph optimization when LO network is either under or overconfident. We evaluate our method on KITTI dataset and show competitiveperformance, even superior generalization ability over recent state-of-the-artapproaches. Source codes are available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.07153v1 |
| 243 | Diffusion Based Augmentation for Captioning and Retrieval in Cultural Heritage                                                                       | Dario Cioni                    | 2023-08-14     | cs.CV                                   | Cultural heritage applications and advanced machine learning models arecreating a fruitful synergy to provide effective and accessible ways ofinteracting with artworks. Smart audio-guides, personalized art-related contentand gamification approaches are just a few examples of how technology can beexploited to provide additional value to artists or exhibitions. Nonetheless,from a machine learning point of view, the amount of available artistic data isoften not enough to train effective models. Off-the-shelf computer visionmodules can still be exploited to some extent, yet a severe domain shift ispresent between art images and standard natural image datasets used to trainsuch models. As a result, this can lead to degraded performance. This paperintroduces a novel approach to address the challenges of limited annotated dataand domain shifts in the cultural heritage domain. By leveraging generativevision-language models, we augment art datasets by generating diversevariations of artworks conditioned on their captions. This augmentationstrategy enhances dataset diversity, bridging the gap between natural imagesand artworks, and improving the alignment of visual cues with knowledge fromgeneral-purpose datasets. The generated variations assist in training visionand language models with a deeper understanding of artistic characteristics andthat are able to generate better captions with appropriate jargon.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.07151v1 |
| 244 | CTP: Towards Vision-Language Continual Pretraining via Compatible Momentum Contrast and Topology Preservation                                        | Hongguang Zhu                  | 2023-08-14     | cs.CV, cs.AI, cs.MM                     | Vision-Language Pretraining (VLP) has shown impressive results on diversedownstream tasks by offline training on large-scale datasets. Regarding thegrowing nature of real-world data, such an offline training paradigm onever-expanding data is unsustainable, because models lack the continuallearning ability to accumulate knowledge constantly. However, most continuallearning studies are limited to uni-modal classification and existingmulti-modal datasets cannot simulate continual non-stationary data streamscenarios. To support the study of Vision-Language Continual Pretraining(VLCP), we first contribute a comprehensive and unified benchmark dataset P9Dwhich contains over one million product image-text pairs from 9 industries. Thedata from each industry as an independent task supports continual learning andconforms to the real-world long-tail nature to simulate pretraining on webdata. We comprehensively study the characteristics and challenges of VLCP, andpropose a new algorithm: Compatible momentum contrast with TopologyPreservation, dubbed CTP. The compatible momentum model absorbs the knowledgeof the current and previous-task models to flexibly update the modal feature.Moreover, Topology Preservation transfers the knowledge of embedding acrosstasks while preserving the flexibility of feature adjustment. The experimentalresults demonstrate our method not only achieves superior performance comparedwith other baselines but also does not bring an expensive training burden.Dataset and codes are available at https://github.com/KevinLight831/CTP.                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.07146v1 |
| 245 | Masked Motion Predictors are Strong 3D Action Representation Learners                                                                                | Yunyao Mao                     | 2023-08-14     | cs.CV                                   | In 3D human action recognition, limited supervised data makes it challengingto fully tap into the modeling potential of powerful networks such astransformers. As a result, researchers have been actively investigatingeffective self-supervised pre-training strategies. In this work, we show thatinstead of following the prevalent pretext task to perform maskedself-component reconstruction in human joints, explicit contextual motionmodeling is key to the success of learning effective feature representation for3D action recognition. Formally, we propose the Masked Motion Prediction (MAMP)framework. To be specific, the proposed MAMP takes as input the maskedspatio-temporal skeleton sequence and predicts the corresponding temporalmotion of the masked human joints. Considering the high temporal redundancy ofthe skeleton sequence, in our MAMP, the motion information also acts as anempirical semantic richness prior that guide the masking process, promotingbetter attention to semantically rich temporal regions. Extensive experimentson NTU-60, NTU-120, and PKU-MMD datasets show that the proposed MAMPpre-training substantially improves the performance of the adopted vanillatransformer, achieving state-of-the-art results without bells and whistles. Thesource code of our MAMP is available at https://github.com/maoyunyao/MAMP.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.07092v1 |
| 246 | S3IM: Stochastic Structural SIMilarity and Its Unreasonable Effectiveness for Neural Fields                                                          | Zeke Xie                       | 2023-08-14     | cs.CV, cs.LG                            | Recently, Neural Radiance Field (NeRF) has shown great success in renderingnovel-view images of a given scene by learning an implicit representation withonly posed RGB images. NeRF and relevant neural field methods (e.g., neuralsurface representation) typically optimize a point-wise loss and makepoint-wise predictions, where one data point corresponds to one pixel.Unfortunately, this line of research failed to use the collective supervisionof distant pixels, although it is known that pixels in an image or scene canprovide rich structural information. To the best of our knowledge, we are thefirst to design a nonlocal multiplex training paradigm for NeRF and relevantneural field methods via a novel Stochastic Structural SIMilarity (S3IM) lossthat processes multiple data points as a whole set instead of process multipleinputs independently. Our extensive experiments demonstrate the unreasonableeffectiveness of S3IM in improving NeRF and neural surface representation fornearly free. The improvements of quality metrics can be particularlysignificant for those relatively difficult tasks: e.g., the test MSE lossunexpectedly drops by more than 90% for TensoRF and DVGO over eight novel viewsynthesis tasks; a 198% F-score gain and a 64% Chamfer $L_{1}$ distancereduction for NeuS over eight surface reconstruction tasks. Moreover, S3IM isconsistently robust even with sparse inputs, corrupted images, and dynamicscenes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.07032v1 |
| 247 | Efficient Neural PDE-Solvers using Quantization Aware Training                                                                                       | Winfried van den Dool          | 2023-08-14     | cs.LG, cs.AI                            | In the past years, the application of neural networks as an alternative toclassical numerical methods to solve Partial Differential Equations has emergedas a potential paradigm shift in this century-old mathematical field. However,in terms of practical applicability, computational cost remains a substantialbottleneck. Classical approaches try to mitigate this challenge by limiting thespatial resolution on which the PDEs are defined. For neural PDE solvers, wecan do better: Here, we investigate the potential of state-of-the-artquantization methods on reducing computational costs. We show that quantizingthe network weights and activations can successfully lower the computationalcost of inference while maintaining performance. Our results on four standardPDE datasets and three network architectures show that quantization-awaretraining works across settings and three orders of FLOPs magnitudes. Finally,we empirically demonstrate that Pareto-optimality of computational cost vsperformance is almost always achieved only by incorporating quantization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.07350v1 |
| 248 | ACTIVE: Towards Highly Transferable 3D Physical Camouflage for Universal and Robust Vehicle Evasion                                                  | Naufal Suryanto                | 2023-08-14     | cs.CV                                   | Adversarial camouflage has garnered attention for its ability to attackobject detectors from any viewpoint by covering the entire object's surface.However, universality and robustness in existing methods often fall short asthe transferability aspect is often overlooked, thus restricting theirapplication only to a specific target with limited performance. To addressthese challenges, we present Adversarial Camouflage for Transferable andIntensive Vehicle Evasion (ACTIVE), a state-of-the-art physical camouflageattack framework designed to generate universal and robust adversarialcamouflage capable of concealing any 3D vehicle from detectors. Our frameworkincorporates innovative techniques to enhance universality and robustness,including a refined texture rendering that enables common texture applicationto different vehicles without being constrained to a specific texture map, anovel stealth loss that renders the vehicle undetectable, and a smooth andcamouflage loss to enhance the naturalness of the adversarial camouflage. Ourextensive experiments on 15 different models show that ACTIVE consistentlyoutperforms existing works on various public detectors, including the latestYOLOv7. Notably, our universality evaluations reveal promising transferabilityto other vehicle classes, tasks (segmentation models), and the real world, notjust other vehicles.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.07009v2 |
| 249 | Knowing Where to Focus: Event-aware Transformer for Video Grounding                                                                                  | Jinhyun Jang                   | 2023-08-14     | cs.CV, cs.LG                            | Recent DETR-based video grounding models have made the model directly predictmoment timestamps without any hand-crafted components, such as a pre-definedproposal or non-maximum suppression, by learning moment queries. However, theirinput-agnostic moment queries inevitably overlook an intrinsic temporalstructure of a video, providing limited positional information. In this paper,we formulate an event-aware dynamic moment query to enable the model to takethe input-specific content and positional information of the video intoaccount. To this end, we present two levels of reasoning: 1) Event reasoningthat captures distinctive event units constituting a given video using a slotattention mechanism; and 2) moment reasoning that fuses the moment queries witha given sentence through a gated fusion transformer layer and learnsinteractions between the moment queries and video-sentence representations topredict moment timestamps. Extensive experiments demonstrate the effectivenessand efficiency of the event-aware dynamic moment queries, outperformingstate-of-the-art approaches on several video grounding benchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.06947v1 |
| 250 | CBA: Improving Online Continual Learning via Continual Bias Adaptor                                                                                  | Quanziang Wang                 | 2023-08-14     | cs.LG, cs.CV                            | Online continual learning (CL) aims to learn new knowledge and consolidatepreviously learned knowledge from non-stationary data streams. Due to thetime-varying training setting, the model learned from a changing distributioneasily forgets the previously learned knowledge and biases toward the newlyreceived task. To address this problem, we propose a Continual Bias Adaptor(CBA) module to augment the classifier network to adapt to catastrophicdistribution change during training, such that the classifier network is ableto learn a stable consolidation of previously learned tasks. In the testingstage, CBA can be removed which introduces no additional computation cost andmemory overhead. We theoretically reveal the reason why the proposed method caneffectively alleviate catastrophic distribution shifts, and empiricallydemonstrate its effectiveness through extensive experiments based on fourrehearsal-based baselines and three public continual learning benchmarks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.06925v1 |
| 251 | Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization                                                         | Jungsoo Lee                    | 2023-08-14     | cs.CV                                   | Test-time adaptation (TTA) methods, which generally rely on the model'spredictions (e.g., entropy minimization) to adapt the source pretrained modelto the unlabeled target domain, suffer from noisy signals originating from 1)incorrect or 2) open-set predictions. Long-term stable adaptation is hamperedby such noisy signals, so training models without such error accumulation iscrucial for practical TTA. To address these issues, including open-set TTA, wepropose a simple yet effective sample selection method inspired by thefollowing crucial empirical finding. While entropy minimization compels themodel to increase the probability of its predicted label (i.e., confidencevalues), we found that noisy samples rather show decreased confidence values.To be more specific, entropy minimization attempts to raise the confidencevalues of an individual sample's prediction, but individual confidence valuesmay rise or fall due to the influence of signals from numerous otherpredictions (i.e., wisdom of crowds). Due to this fact, noisy signalsmisaligned with such 'wisdom of crowds', generally found in the correctsignals, fail to raise the individual confidence values of wrong samples,despite attempts to increase them. Based on such findings, we filter out thesamples whose confidence values are lower in the adapted model than in theoriginal model, as they are likely to be noisy. Our method is widely applicableto existing TTA methods and improves their long-term adaptation performance inboth image classification (e.g., 49.4% reduced error rates with TENT) andsemantic segmentation (e.g., 11.7% gain in mIoU with TENT).                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.06879v1 |
| 252 | Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning                                                                             | Lihe Yang                      | 2023-08-13     | cs.CV                                   | Semi-supervised learning is attracting blooming attention, due to its successin combining unlabeled data. To mitigate potentially incorrect pseudo labels,recent frameworks mostly set a fixed confidence threshold to discard uncertainsamples. This practice ensures high-quality pseudo labels, but incurs arelatively low utilization of the whole unlabeled set. In this work, our keyinsight is that these uncertain samples can be turned into certain ones, aslong as the confusion classes for the top-1 class are detected and removed.Invoked by this, we propose a novel method dubbed ShrinkMatch to learnuncertain samples. For each uncertain sample, it adaptively seeks a shrunkclass space, which merely contains the original top-1 class, as well asremaining less likely classes. Since the confusion ones are removed in thisspace, the re-calculated top-1 confidence can satisfy the pre-definedthreshold. We then impose a consistency regularization between a pair ofstrongly and weakly augmented samples in the shrunk space to strive fordiscriminative representations. Furthermore, considering the varied reliabilityamong uncertain samples and the gradually improved model during training, wecorrespondingly design two reweighting principles for our uncertain loss. Ourmethod exhibits impressive performance on widely adopted benchmarks. Code isavailable at https://github.com/LiheYoung/ShrinkMatch.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.06777v1 |
| 253 | Unsupervised Image Denoising in Real-World Scenarios via Self-Collaboration Parallel Generative Adversarial Branches                                 | Xin Lin                        | 2023-08-13     | eess.IV, cs.CV                          | Deep learning methods have shown remarkable performance in image denoising,particularly when trained on large-scale paired datasets. However, acquiringsuch paired datasets for real-world scenarios poses a significant challenge.Although unsupervised approaches based on generative adversarial networks offera promising solution for denoising without paired datasets, they are difficultin surpassing the performance limitations of conventional GAN-basedunsupervised frameworks without significantly modifying existing structures orincreasing the computational complexity of denoisers. To address this problem,we propose a SC strategy for multiple denoisers. This strategy can achievesignificant performance improvement without increasing the inference complexityof the GAN-based denoising framework. Its basic idea is to iteratively replacethe previous less powerful denoiser in the filter-guided noise extractionmodule with the current powerful denoiser. This process generates bettersynthetic clean-noisy image pairs, leading to a more powerful denoiser for thenext iteration. This baseline ensures the stability and effectiveness of thetraining network. The experimental results demonstrate the superiority of ourmethod over state-of-the-art unsupervised methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.06776v1 |
| 254 | Dual Meta-Learning with Longitudinally Generalized Regularization for One-Shot Brain Tissue Segmentation Across the Human Lifespan                   | Yongheng Sun                   | 2023-08-13     | cs.CV, cs.AI                            | Brain tissue segmentation is essential for neuroscience and clinical studies.However, segmentation on longitudinal data is challenging due to dynamic brainchanges across the lifespan. Previous researches mainly focus onself-supervision with regularizations and will lose longitudinal generalizationwhen fine-tuning on a specific age group. In this paper, we propose a dualmeta-learning paradigm to learn longitudinally consistent representations andpersist when fine-tuning. Specifically, we learn a plug-and-play featureextractor to extract longitudinal-consistent anatomical representations bymeta-feature learning and a well-initialized task head for fine-tuning bymeta-initialization learning. Besides, two class-aware regularizations areproposed to encourage longitudinal consistency. Experimental results on theiSeg2019 and ADNI datasets demonstrate the effectiveness of our method. Ourcode is available at https://github.com/ladderlab-xjtu/DuMeta.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.06774v1 |
| 255 | AerialVLN: Vision-and-Language Navigation for UAVs                                                                                                   | Shubo Liu                      | 2023-08-13     | cs.CV, cs.AI, cs.RO                     | Recently emerged Vision-and-Language Navigation (VLN) tasks have drawnsignificant attention in both computer vision and natural language processingcommunities. Existing VLN tasks are built for agents that navigate on theground, either indoors or outdoors. However, many tasks require intelligentagents to carry out in the sky, such as UAV-based goods delivery,traffic/security patrol, and scenery tour, to name a few. Navigating in the skyis more complicated than on the ground because agents need to consider theflying height and more complex spatial relationship reasoning. To fill this gapand facilitate research in this field, we propose a new task named AerialVLN,which is UAV-based and towards outdoor environments. We develop a 3D simulatorrendered by near-realistic pictures of 25 city-level scenarios. Our simulatorsupports continuous navigation, environment extension and configuration. Wealso proposed an extended baseline model based on the widely-usedcross-modal-alignment (CMA) navigation methods. We find that there is still asignificant gap between the baseline model and human performance, whichsuggests AerialVLN is a new challenging task. Dataset and code is available athttps://github.com/AirVLN/AirVLN.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.06735v1 |
| 256 | Compositional Feature Augmentation for Unbiased Scene Graph Generation                                                                               | Lin Li                         | 2023-08-13     | cs.CV                                   | Scene Graph Generation (SGG) aims to detect all the visual relation triplets<sub, pred, obj> in a given image. With the emergence of various advancedtechniques for better utilizing both the intrinsic and extrinsic information ineach relation triplet, SGG has achieved great progress over the recent years.However, due to the ubiquitous long-tailed predicate distributions, today's SGGmodels are still easily biased to the head predicates. Currently, the mostprevalent debiasing solutions for SGG are re-balancing methods, e.g., changingthe distributions of original training samples. In this paper, we argue thatall existing re-balancing strategies fail to increase the diversity of therelation triplet features of each predicate, which is critical for robust SGG.To this end, we propose a novel Compositional Feature Augmentation (CFA)strategy, which is the first unbiased SGG work to mitigate the bias issue fromthe perspective of increasing the diversity of triplet features. Specifically,we first decompose each relation triplet feature into two components: intrinsicfeature and extrinsic feature, which correspond to the intrinsiccharacteristics and extrinsic contexts of a relation triplet, respectively.Then, we design two different feature augmentation modules to enrich thefeature diversity of original relation triplets by replacing or mixing upeither their intrinsic or extrinsic features from other samples. Due to itsmodel-agnostic nature, CFA can be seamlessly incorporated into various SGGframeworks. Extensive ablations have shown that CFA achieves a newstate-of-the-art performance on the trade-off between different metrics.                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.06712v1 |
| 257 | Estimator Meets Equilibrium Perspective: A Rectified Straight Through Estimator for Binary Neural Networks Training                                  | Xiao-Ming Wu                   | 2023-08-13     | cs.CV                                   | Binarization of neural networks is a dominant paradigm in neural networkscompression. The pioneering work BinaryConnect uses Straight Through Estimator(STE) to mimic the gradients of the sign function, but it also causes thecrucial inconsistency problem. Most of the previous methods design differentestimators instead of STE to mitigate it. However, they ignore the fact thatwhen reducing the estimating error, the gradient stability will decreaseconcomitantly. These highly divergent gradients will harm the model trainingand increase the risk of gradient vanishing and gradient exploding. To fullytake the gradient stability into consideration, we present a new perspective tothe BNNs training, regarding it as the equilibrium between the estimating errorand the gradient stability. In this view, we firstly design two indicators toquantitatively demonstrate the equilibrium phenomenon. In addition, in order tobalance the estimating error and the gradient stability well, we revise theoriginal straight through estimator and propose a power function basedestimator, Rectified Straight Through Estimator (ReSTE for short). Comparing toother estimators, ReSTE is rational and capable of flexibly balancing theestimating error with the gradient stability. Extensive experiments on CIFAR-10and ImageNet datasets show that ReSTE has excellent performance and surpassesthe state-of-the-art methods without any auxiliary modules or losses.                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.06689v2 |
| 258 | Cyclic Test-Time Adaptation on Monocular Video for 3D Human Mesh Reconstruction                                                                      | Hyeongjin Nam                  | 2023-08-12     | cs.CV                                   | Despite recent advances in 3D human mesh reconstruction, domain gap betweentraining and test data is still a major challenge. Several prior works tacklethe domain gap problem via test-time adaptation that fine-tunes a networkrelying on 2D evidence (e.g., 2D human keypoints) from test images. However,the high reliance on 2D evidence during adaptation causes two major issues.First, 2D evidence induces depth ambiguity, preventing the learning of accurate3D human geometry. Second, 2D evidence is noisy or partially non-existentduring test time, and such imperfect 2D evidence leads to erroneous adaptation.To overcome the above issues, we introduce CycleAdapt, which cyclically adaptstwo networks: a human mesh reconstruction network (HMRNet) and a human motiondenoising network (MDNet), given a test video. In our framework, to alleviatehigh reliance on 2D evidence, we fully supervise HMRNet with generated 3Dsupervision targets by MDNet. Our cyclic adaptation scheme progressivelyelaborates the 3D supervision targets, which compensate for imperfect 2Devidence. As a result, our CycleAdapt achieves state-of-the-art performancecompared to previous test-time adaptation methods. The codes are available athttps://github.com/hygenie1228/CycleAdapt_RELEASE.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.06554v1 |
| 259 | Revisiting Vision Transformer from the View of Path Ensemble                                                                                         | Shuning Chang                  | 2023-08-12     | cs.CV                                   | Vision Transformers (ViTs) are normally regarded as a stack of transformerlayers. In this work, we propose a novel view of ViTs showing that they can beseen as ensemble networks containing multiple parallel paths with differentlengths. Specifically, we equivalently transform the traditional cascade ofmulti-head self-attention (MSA) and feed-forward network (FFN) into threeparallel paths in each transformer layer. Then, we utilize the identityconnection in our new transformer form and further transform the ViT into anexplicit multi-path ensemble network. From the new perspective, these pathsperform two functions: the first is to provide the feature for the classifierdirectly, and the second is to provide the lower-level feature representationfor subsequent longer paths. We investigate the influence of each path for thefinal prediction and discover that some paths even pull down the performance.Therefore, we propose the path pruning and EnsembleScale skills forimprovement, which cut out the underperforming paths and re-weight the ensemblecomponents, respectively, to optimize the path combination and make the shortpaths focus on providing high-quality representation for subsequent paths. Wealso demonstrate that our path combination strategies can help ViTs go deeperand act as high-pass filters to filter out partial low-frequency signals. Tofurther enhance the representation of paths served for subsequent paths,self-distillation is applied to transfer knowledge from the long paths to theshort paths. This work calls for more future research to explain and designViTs from new perspectives.                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.06548v1 |
| 260 | SegPrompt: Boosting Open-world Segmentation via Category-level Prompt Learning                                                                       | Muzhi Zhu                      | 2023-08-12     | cs.CV                                   | Current closed-set instance segmentation models rely on pre-defined classlabels for each mask during training and evaluation, largely limiting theirability to detect novel objects. Open-world instance segmentation (OWIS) modelsaddress this challenge by detecting unknown objects in a class-agnostic manner.However, previous OWIS approaches completely erase category information duringtraining to keep the model's ability to generalize to unknown objects. In thiswork, we propose a novel training mechanism termed SegPrompt that uses categoryinformation to improve the model's class-agnostic segmentation ability for bothknown and unknown categories. In addition, the previous OWIS training settingexposes the unknown classes to the training set and brings information leakage,which is unreasonable in the real world. Therefore, we provide a new open-worldbenchmark closer to a real-world scenario by dividing the dataset classes intoknown-seen-unseen parts. For the first time, we focus on the model's ability todiscover objects that never appear in the training set images.  Experiments show that SegPrompt can improve the overall and unseen detectionperformance by 5.6% and 6.1% in AR on our new benchmark without affecting theinference efficiency. We further demonstrate the effectiveness of our method onexisting cross-dataset transfer and strongly supervised settings, leading to5.5% and 12.3% relative improvement.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.06531v1 |
| 261 | BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation                                             | Miaoyu Li                      | 2023-08-12     | cs.CV                                   | Cross-modal Unsupervised Domain Adaptation (UDA) aims to exploit thecomplementarity of 2D-3D data to overcome the lack of annotation in a newdomain. However, UDA methods rely on access to the target domain duringtraining, meaning the trained model only works in a specific target domain. Inlight of this, we propose cross-modal learning under bird's-eye view for DomainGeneralization (DG) of 3D semantic segmentation, called BEV-DG. DG is morechallenging because the model cannot access the target domain during training,meaning it needs to rely on cross-modal learning to alleviate the domain gap.Since 3D semantic segmentation requires the classification of each point,existing cross-modal learning is directly conducted point-to-point, which issensitive to the misalignment in projections between pixels and points. To thisend, our approach aims to optimize domain-irrelevant representation modelingwith the aid of cross-modal learning under bird's-eye view. We proposeBEV-based Area-to-area Fusion (BAF) to conduct cross-modal learning underbird's-eye view, which has a higher fault tolerance for point-levelmisalignment. Furthermore, to model domain-irrelevant representations, wepropose BEV-driven Domain Contrastive Learning (BDCL) with the help ofcross-modal learning under bird's-eye view. We design three domaingeneralization settings based on three 3D datasets, and BEV-DG significantlyoutperforms state-of-the-art competitors with tremendous margins in allsettings.                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.06530v1 |
| 262 | One-bit Flip is All You Need: When Bit-flip Attack Meets Model Training                                                                              | Jianshuo Dong                  | 2023-08-12     | cs.CR, cs.AI, cs.CV, cs.LG              | Deep neural networks (DNNs) are widely deployed on real-world devices.Concerns regarding their security have gained great attention from researchers.Recently, a new weight modification attack called bit flip attack (BFA) wasproposed, which exploits memory fault inject techniques such as row hammer toattack quantized models in the deployment stage. With only a few bit flips, thetarget model can be rendered useless as a random guesser or even be implantedwith malicious functionalities. In this work, we seek to further reduce thenumber of bit flips. We propose a training-assisted bit flip attack, in whichthe adversary is involved in the training stage to build a high-risk model torelease. This high-risk model, obtained coupled with a corresponding maliciousmodel, behaves normally and can escape various detection methods. The resultson benchmark datasets show that an adversary can easily convert this high-riskbut normal model to a malicious one on victim's side by \textbf{flipping onlyone critical bit} on average in the deployment stage. Moreover, our attackstill poses a significant threat even when defenses are employed. The codes forreproducing main experiments are available at\url{https://github.com/jianshuod/TBA}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.07934v1 |
| 263 | Tiny and Efficient Model for the Edge Detection Generalization                                                                                       | Xavier Soria                   | 2023-08-12     | cs.CV, cs.LG                            | Most high-level computer vision tasks rely on low-level image operations astheir initial processes. Operations such as edge detection, image enhancement,and super-resolution, provide the foundations for higher level image analysis.In this work we address the edge detection considering three main objectives:simplicity, efficiency, and generalization since current state-of-the-art(SOTA) edge detection models are increased in complexity for better accuracy.To achieve this, we present Tiny and Efficient Edge Detector (TEED), a lightconvolutional neural network with only $58K$ parameters, less than $0.2$% ofthe state-of-the-art models. Training on the BIPED dataset takes $less than 30minutes$, with each epoch requiring $less than 5 minutes$. Our proposed modelis easy to train and it quickly converges within very first few epochs, whilethe predicted edge-maps are crisp and of high quality. Additionally, we proposea new dataset to test the generalization of edge detection, which comprisessamples from popular images used in edge detection and image segmentation. Thesource code is available in https://github.com/xavysp/TEED.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.06468v1 |
| 264 | Multi-Label Knowledge Distillation                                                                                                                   | Penghui Yang                   | 2023-08-12     | cs.LG, cs.AI, cs.CV                     | Existing knowledge distillation methods typically work by imparting theknowledge of output logits or intermediate feature maps from the teachernetwork to the student network, which is very successful in multi-classsingle-label learning. However, these methods can hardly be extended to themulti-label learning scenario, where each instance is associated with multiplesemantic labels, because the prediction probabilities do not sum to one andfeature maps of the whole example may ignore minor classes in such a scenario.In this paper, we propose a novel multi-label knowledge distillation method. Onone hand, it exploits the informative semantic knowledge from the logits bydividing the multi-label learning problem into a set of binary classificationproblems; on the other hand, it enhances the distinctiveness of the learnedfeature representations by leveraging the structural information of label-wiseembeddings. Experimental results on multiple benchmark datasets validate thatthe proposed method can avoid knowledge counteraction among labels, thusachieving superior performance against diverse comparing methods. Our code isavailable at: https://github.com/penghui-yang/L2D                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.06453v1 |
| 265 | FunnyBirds: A Synthetic Vision Dataset for a Part-Based Analysis of Explainable AI Methods                                                           | Robin Hesse                    | 2023-08-11     | cs.CV, cs.LG                            | The field of explainable artificial intelligence (XAI) aims to uncover theinner workings of complex deep neural models. While being crucial forsafety-critical domains, XAI inherently lacks ground-truth explanations, makingits automatic evaluation an unsolved problem. We address this challenge byproposing a novel synthetic vision dataset, named FunnyBirds, and accompanyingautomatic evaluation protocols. Our dataset allows performing semanticallymeaningful image interventions, e.g., removing individual object parts, whichhas three important implications. First, it enables analyzing explanations on apart level, which is closer to human comprehension than existing methods thatevaluate on a pixel level. Second, by comparing the model output for inputswith removed parts, we can estimate ground-truth part importances that shouldbe reflected in the explanations. Third, by mapping individual explanationsinto a common space of part importances, we can analyze a variety of differentexplanation types in a single common framework. Using our tools, we reportresults for 24 different combinations of neural models and XAI methods,demonstrating the strengths and weaknesses of the assessed methods in a fullyautomatic and systematic manner.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.06248v1 |
| 266 | Out-of-Distribution Detection for Monocular Depth Estimation                                                                                         | Julia Hornauer                 | 2023-08-11     | cs.CV                                   | In monocular depth estimation, uncertainty estimation approaches mainlytarget the data uncertainty introduced by image noise. In contrast to priorwork, we address the uncertainty due to lack of knowledge, which is relevantfor the detection of data not represented by the training distribution, theso-called out-of-distribution (OOD) data. Motivated by anomaly detection, wepropose to detect OOD images from an encoder-decoder depth estimation modelbased on the reconstruction error. Given the features extracted with the fixeddepth encoder, we train an image decoder for image reconstruction using onlyin-distribution data. Consequently, OOD images result in a high reconstructionerror, which we use to distinguish between in- and out-of-distribution samples.We built our experiments on the standard NYU Depth V2 and KITTI benchmarks asin-distribution data. Our post hoc method performs astonishingly well ondifferent models and outperforms existing uncertainty estimation approacheswithout modifying the trained encoder-decoder depth estimation model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.06072v1 |
| 267 | Cyclic-Bootstrap Labeling for Weakly Supervised Object Detection                                                                                     | Yufei Yin                      | 2023-08-11     | cs.CV                                   | Recent progress in weakly supervised object detection is featured by acombination of multiple instance detection networks (MIDN) and ordinal onlinerefinement. However, with only image-level annotation, MIDN inevitably assignshigh scores to some unexpected region proposals when generating pseudo labels.These inaccurate high-scoring region proposals will mislead the training ofsubsequent refinement modules and thus hamper the detection performance. Inthis work, we explore how to ameliorate the quality of pseudo-labeling in MIDN.Formally, we devise Cyclic-Bootstrap Labeling (CBL), a novel weakly supervisedobject detection pipeline, which optimizes MIDN with rank information from areliable teacher network. Specifically, we obtain this teacher network byintroducing a weighted exponential moving average strategy to take advantage ofvarious refinement modules. A novel class-specific ranking distillationalgorithm is proposed to leverage the output of weighted ensembled teachernetwork for distilling MIDN with rank information. As a result, MIDN is guidedto assign higher scores to accurate proposals among their neighboring ones,thus benefiting the subsequent pseudo labeling. Extensive experiments on theprevalent PASCAL VOC 2007 \& 2012 and COCO datasets demonstrate the superiorperformance of our CBL framework. Code will be available athttps://github.com/Yinyf0804/WSOD-CBL/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.05991v1 |
| 268 | TrajPAC: Towards Robustness Verification of Pedestrian Trajectory Prediction Models                                                                  | Liang Zhang                    | 2023-08-11     | cs.AI                                   | Robust pedestrian trajectory forecasting is crucial to developing safeautonomous vehicles. Although previous works have studied adversarialrobustness in the context of trajectory forecasting, some significant issuesremain unaddressed. In this work, we try to tackle these crucial problems.Firstly, the previous definitions of robustness in trajectory prediction areambiguous. We thus provide formal definitions for two kinds of robustness,namely label robustness and pure robustness. Secondly, as previous works failto consider robustness about all points in a disturbance interval, we utilise aprobably approximately correct (PAC) framework for robustness verification.Additionally, this framework can not only identify potential counterexamples,but also provides interpretable analyses of the original methods. Our approachis applied using a prototype tool named TrajPAC. With TrajPAC, we evaluate therobustness of four state-of-the-art trajectory prediction models --Trajectron++, MemoNet, AgentFormer, and MID -- on trajectories from five scenesof the ETH/UCY dataset and scenes of the Stanford Drone Dataset. Using ourframework, we also experimentally study various factors that could influencerobustness performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.05985v1 |
| 269 | Generalizing Event-Based Motion Deblurring in Real-World Scenarios                                                                                   | Xiang Zhang                    | 2023-08-11     | cs.CV                                   | Event-based motion deblurring has shown promising results by exploitinglow-latency events. However, current approaches are limited in their practicalusage, as they assume the same spatial resolution of inputs and specificblurriness distributions. This work addresses these limitations and aims togeneralize the performance of event-based deblurring in real-world scenarios.We propose a scale-aware network that allows flexible input spatial scales andenables learning from different temporal scales of motion blur. A two-stageself-supervised learning scheme is then developed to fit real-world datadistribution. By utilizing the relativity of blurriness, our approachefficiently ensures the restored brightness and structure of latent images andfurther generalizes deblurring performance to handle varying spatial andtemporal scales of motion blur in a self-distillation manner. Our method isextensively evaluated, demonstrating remarkable performance, and we alsointroduce a real-world dataset consisting of multi-scale blurry frames andevents to facilitate research in event-based deblurring.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.05932v1 |
| 270 | Collaborative Tracking Learning for Frame-Rate-Insensitive Multi-Object Tracking                                                                     | Yiheng Liu                     | 2023-08-11     | cs.CV                                   | Multi-object tracking (MOT) at low frame rates can reduce computational,storage and power overhead to better meet the constraints of edge devices. Manyexisting MOT methods suffer from significant performance degradation inlow-frame-rate videos due to significant location and appearance changesbetween adjacent frames. To this end, we propose to explore collaborativetracking learning (ColTrack) for frame-rate-insensitive MOT in a query-basedend-to-end manner. Multiple historical queries of the same target jointly trackit with richer temporal descriptions. Meanwhile, we insert an informationrefinement module between every two temporal blocking decoders to better fusetemporal clues and refine features. Moreover, a tracking object consistencyloss is proposed to guide the interaction between historical queries. Extensiveexperimental results demonstrate that in high-frame-rate videos, ColTrackobtains higher performance than state-of-the-art methods on large-scaledatasets Dancetrack and BDD100K, and outperforms the existing end-to-endmethods on MOT17. More importantly, ColTrack has a significant advantage overstate-of-the-art methods in low-frame-rate videos, which allows it to obtainfaster processing speeds by reducing frame-rate requirements while maintaininghigher performance. Code will be released athttps://github.com/yolomax/ColTrack                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.05911v1 |
| 271 | SegDA: Maximum Separable Segment Mask with Pseudo Labels for Domain Adaptive Semantic Segmentation                                                   | Anant Khandelwal               | 2023-08-10     | cs.CV                                   | Unsupervised Domain Adaptation (UDA) aims to solve the problem of labelscarcity of the target domain by transferring the knowledge from the label richsource domain. Usually, the source domain consists of synthetic images forwhich the annotation is easily obtained using the well known computer graphicstechniques. However, obtaining annotation for real world images (target domain)require lot of manual annotation effort and is very time consuming because itrequires per pixel annotation. To address this problem we propose SegDA moduleto enhance transfer performance of UDA methods by learning the maximumseparable segment representation. This resolves the problem of identifyingvisually similar classes like pedestrian/rider, sidewalk/road etc. We leveragedEquiangular Tight Frame (ETF) classifier inspired from Neural Collapse formaximal separation between segment classes. This causes the source domain pixelrepresentation to collapse to a single vector forming a simplex vertices whichare aligned to the maximal separable ETF classifier. We use this phenomenon topropose the novel architecture for domain adaptation of segment representationfor target domain. Additionally, we proposed to estimate the noise in labellingthe target domain images and update the decoder for noise correction whichencourages the discovery of pixels for classes not identified in pseudo labels.We have used four UDA benchmarks simulating synthetic-to-real,daytime-to-nighttime, clear-to-adverse weather scenarios. Our proposed approachoutperforms +2.2 mIoU on GTA -> Cityscapes, +2.0 mIoU on Synthia -> Cityscapes,+5.9 mIoU on Cityscapes -> DarkZurich, +2.6 mIoU on Cityscapes -> ACDC.                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.05851v1 |
| 272 | PlankAssembly: Robust 3D Reconstruction from Three Orthographic Views with Learnt Shape Programs                                                     | Wentao Hu                      | 2023-08-10     | cs.CV, cs.GR                            | In this paper, we develop a new method to automatically convert 2D linedrawings from three orthographic views into 3D CAD models. Existing methods forthis problem reconstruct 3D models by back-projecting the 2D observations into3D space while maintaining explicit correspondence between the input andoutput. Such methods are sensitive to errors and noises in the input, thusoften fail in practice where the input drawings created by human designers areimperfect. To overcome this difficulty, we leverage the attention mechanism ina Transformer-based sequence generation model to learn flexible mappingsbetween the input and output. Further, we design shape programs which aresuitable for generating the objects of interest to boost the reconstructionaccuracy and facilitate CAD modeling applications. Experiments on a newbenchmark dataset show that our method significantly outperforms existing oneswhen the inputs are noisy or incomplete.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.05744v1 |
| 273 | FrozenRecon: Pose-free 3D Scene Reconstruction with Frozen Depth Models                                                                              | Guangkai Xu                    | 2023-08-10     | cs.CV                                   | 3D scene reconstruction is a long-standing vision task. Existing approachescan be categorized into geometry-based and learning-based methods. The formerleverages multi-view geometry but can face catastrophic failures due to thereliance on accurate pixel correspondence across views. The latter wasproffered to mitigate these issues by learning 2D or 3D representationdirectly. However, without a large-scale video or 3D training data, it canhardly generalize to diverse real-world scenarios due to the presence of tensof millions or even billions of optimization parameters in the deep network.Recently, robust monocular depth estimation models trained with large-scaledatasets have been proven to possess weak 3D geometry prior, but they areinsufficient for reconstruction due to the unknown camera parameters, theaffine-invariant property, and inter-frame inconsistency. Here, we propose anovel test-time optimization approach that can transfer the robustness ofaffine-invariant depth models such as LeReS to challenging diverse scenes whileensuring inter-frame consistency, with only dozens of parameters to optimizeper video frame. Specifically, our approach involves freezing the pre-trainedaffine-invariant depth model's depth predictions, rectifying them by optimizingthe unknown scale-shift values with a geometric consistency alignment module,and employing the resulting scale-consistent depth maps to robustly obtaincamera poses and achieve dense scene reconstruction, even in low-textureregions. Experiments show that our method achieves state-of-the-artcross-dataset reconstruction on five zero-shot testing datasets.                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.05733v1 |
| 274 | Hard No-Box Adversarial Attack on Skeleton-Based Human Action Recognition with Skeleton-Motion-Informed Gradient                                     | Zhengzhi Lu                    | 2023-08-10     | cs.CV, cs.AI, cs.LG                     | Recently, methods for skeleton-based human activity recognition have beenshown to be vulnerable to adversarial attacks. However, these attack methodsrequire either the full knowledge of the victim (i.e. white-box attacks),access to training data (i.e. transfer-based attacks) or frequent model queries(i.e. black-box attacks). All their requirements are highly restrictive,raising the question of how detrimental the vulnerability is. In this paper, weshow that the vulnerability indeed exists. To this end, we consider a newattack task: the attacker has no access to the victim model or the trainingdata or labels, where we coin the term hard no-box attack. Specifically, wefirst learn a motion manifold where we define an adversarial loss to compute anew gradient for the attack, named skeleton-motion-informed (SMI) gradient. Ourgradient contains information of the motion dynamics, which is different fromexisting gradient-based attack methods that compute the loss gradient assumingeach dimension in the data is independent. The SMI gradient can augment manygradient-based attack methods, leading to a new family of no-box attackmethods. Extensive evaluation and comparison show that our method imposes areal threat to existing classifiers. They also show that the SMI gradientimproves the transferability and imperceptibility of adversarial samples inboth no-box and transfer-based black-box settings.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.05681v2 |
| 275 | 2D3D-MATR: 2D-3D Matching Transformer for Detection-free Registration between Images and Point Clouds                                                | Minhao Li                      | 2023-08-10     | cs.CV                                   | The commonly adopted detect-then-match approach to registration findsdifficulties in the cross-modality cases due to the incompatible keypointdetection and inconsistent feature description. We propose, 2D3D-MATR, adetection-free method for accurate and robust registration between images andpoint clouds. Our method adopts a coarse-to-fine pipeline where it firstcomputes coarse correspondences between downsampled patches of the input imageand the point cloud and then extends them to form dense correspondences betweenpixels and points within the patch region. The coarse-level patch matching isbased on transformer which jointly learns global contextual constraints withself-attention and cross-modality correlations with cross-attention. To resolvethe scale ambiguity in patch matching, we construct a multi-scale pyramid foreach image patch and learn to find for each point patch the best matching imagepatch at a proper resolution level. Extensive experiments on two publicbenchmarks demonstrate that 2D3D-MATR outperforms the previous state-of-the-artP2-Net by around $20$ percentage points on inlier ratio and over $10$ points onregistration recall. Our code and models are available athttps://github.com/minhaolee/2D3DMATR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.05667v2 |
| 276 | AD-CLIP: Adapting Domains in Prompt Space Using CLIP                                                                                                 | Mainak Singha                  | 2023-08-10     | cs.CV                                   | Although deep learning models have shown impressive performance on supervisedlearning tasks, they often struggle to generalize well when the training(source) and test (target) domains differ. Unsupervised domain adaptation (DA)has emerged as a popular solution to this problem. However, current DAtechniques rely on visual backbones, which may lack semantic richness. Despitethe potential of large-scale vision-language foundation models like CLIP, theireffectiveness for DA has yet to be fully explored. To address this gap, weintroduce AD-CLIP, a domain-agnostic prompt learning strategy for CLIP thataims to solve the DA problem in the prompt space. We leverage the frozen visionbackbone of CLIP to extract both image style (domain) and content information,which we apply to learn prompt tokens. Our prompts are designed to bedomain-invariant and class-generalizable, by conditioning prompt learning onimage style and content features simultaneously. We use standard supervisedcontrastive learning in the source domain, while proposing an entropyminimization strategy to align domains in the embedding space given the targetdomain data. We also consider a scenario where only target domain samples areavailable during testing, without any source domain data, and propose across-domain style mapping network to hallucinate domain-agnostic tokens. Ourextensive experiments on three benchmark DA datasets demonstrate theeffectiveness of AD-CLIP compared to existing literature.                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.05659v1 |
| 277 | Robust Asymmetric Loss for Multi-Label Long-Tailed Learning                                                                                          | Wongi Park                     | 2023-08-10     | cs.CV                                   | In real medical data, training samples typically show long-taileddistributions with multiple labels. Class distribution of the medical data hasa long-tailed shape, in which the incidence of different diseases is quitevaried, and at the same time, it is not unusual for images taken fromsymptomatic patients to be multi-label diseases. Therefore, in this paper, weconcurrently address these two issues by putting forth a robust asymmetric losson the polynomial function. Since our loss tackles both long-tailed andmulti-label classification problems simultaneously, it leads to a complexdesign of the loss function with a large number of hyper-parameters. Although amodel can be highly fine-tuned due to a large number of hyper-parameters, it isdifficult to optimize all hyper-parameters at the same time, and there might bea risk of overfitting a model. Therefore, we regularize the loss function usingthe Hill loss approach, which is beneficial to be less sensitive against thenumerous hyper-parameters so that it reduces the risk of overfitting the model.For this reason, the proposed loss is a generic method that can be applied tomost medical image classification tasks and does not make the training processmore time-consuming. We demonstrate that the proposed robust asymmetric lossperforms favorably against the long-tailed with multi-label medical imageclassification in addition to the various long-tailed single-label datasets.Notably, our method achieves Top-5 results on the CXR-LT dataset of the ICCVCVAMD 2023 competition. We opensource our implementation of the robustasymmetric loss in the public repository: https://github.com/kalelpark/RAL.                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.05542v1 |
| 278 | Is there progress in activity progress prediction?                                                                                                   | Frans de Boer                  | 2023-08-10     | cs.CV                                   | Activity progress prediction aims to estimate what percentage of an activityhas been completed. Currently this is done with machine learning approaches,trained and evaluated on complicated and realistic video datasets. The videosin these datasets vary drastically in length and appearance. And some of theactivities have unanticipated developments, making activity progressiondifficult to estimate. In this work, we examine the results obtained byexisting progress prediction methods on these datasets. We find that currentprogress prediction methods seem not to extract useful visual information forthe progress prediction task. Therefore, these methods fail to exceed simpleframe-counting baselines. We design a precisely controlled dataset for activityprogress prediction and on this synthetic dataset we show that the consideredmethods can make use of the visual information, when this directly relates tothe progress prediction. We conclude that the progress prediction task isill-posed on the currently used real-world datasets. Moreover, to fairlymeasure activity progression we advise to consider a, simple but effective,frame-counting baseline.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.05533v1 |
| 279 | Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation                                            | Xu Zheng                       | 2023-08-10     | cs.CV                                   | Endeavors have been recently made to transfer knowledge from the labeledpinhole image domain to the unlabeled panoramic image domain via UnsupervisedDomain Adaptation (UDA). The aim is to tackle the domain gaps caused by thestyle disparities and distortion problem from the non-uniformly distributedpixels of equirectangular projection (ERP). Previous works typically focus ontransferring knowledge based on geometric priors with specially designedmulti-branch network architectures. As a result, considerable computationalcosts are induced, and meanwhile, their generalization abilities are profoundlyhindered by the variation of distortion among pixels. In this paper, we findthat the pixels' neighborhood regions of the ERP indeed introduce lessdistortion. Intuitively, we propose a novel UDA framework that can effectivelyaddress the distortion problems for panoramic semantic segmentation. Incomparison, our method is simpler, easier to implement, and morecomputationally efficient. Specifically, we propose distortion-aware attention(DA) capturing the neighboring pixel distribution without using any geometricconstraints. Moreover, we propose a class-wise feature aggregation (CFA) moduleto iteratively update the feature representations with a memory bank. As such,the feature similarity between two domains can be consistently optimized.Extensive experiments show that our method achieves new state-of-the-artperformance while remarkably reducing 80% parameters.                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.05493v1 |
| 280 | SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data                                       | Mohammad Zohaib                | 2023-08-10     | cs.CV, cs.GR, cs.RO                     | This paper proposes a new method to infer keypoints from arbitrary objectcategories in practical scenarios where point cloud data (PCD) are noisy,down-sampled and arbitrarily rotated. Our proposed model adheres to thefollowing principles: i) keypoints inference is fully unsupervised (noannotation given), ii) keypoints position error should be low and resilient toPCD perturbations (robustness), iii) keypoints should not change their indexesfor the intra-class objects (semantic coherence), iv) keypoints should be closeto or proximal to PCD surface (compactness). We achieve these desiderata byproposing a new self-supervised training strategy for keypoints estimation thatdoes not assume any a priori knowledge of the object class, and a modelarchitecture with coupled auxiliary losses that promotes the desired keypointsproperties. We compare the keypoints estimated by the proposed approach withthose of the state-of-the-art unsupervised approaches. The experiments showthat our approach outperforms by estimating keypoints with improved coverage(+9.41%) while being semantically consistent (+4.66%) that best characterizesthe object's 3D shape for downstream tasks. Code and data are available at:https://github.com/IITPAVIS/SC3K                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.05410v1 |
| 281 | Pseudo-label Alignment for Semi-supervised Instance Segmentation                                                                                     | Jie Hu                         | 2023-08-10     | cs.CV                                   | Pseudo-labeling is significant for semi-supervised instance segmentation,which generates instance masks and classes from unannotated images forsubsequent training. However, in existing pipelines, pseudo-labels that containvaluable information may be directly filtered out due to mismatches in classand mask quality. To address this issue, we propose a novel framework, calledpseudo-label aligning instance segmentation (PAIS), in this paper. In PAIS, wedevise a dynamic aligning loss (DALoss) that adjusts the weights ofsemi-supervised loss terms with varying class and mask score pairs. Throughextensive experiments conducted on the COCO and Cityscapes datasets, wedemonstrate that PAIS is a promising framework for semi-supervised instancesegmentation, particularly in cases where labeled data is severely limited.Notably, with just 1\% labeled data, PAIS achieves 21.2 mAP (based onMask-RCNN) and 19.9 mAP (based on K-Net) on the COCO dataset, outperforming thecurrent state-of-the-art model, \ie, NoisyBoundary with 7.7 mAP, by a margin ofover 12 points. Code is available at: \url{https://github.com/hujiecpp/PAIS}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.05359v1 |
| 282 | Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri                                                           | Fabio Quattrini                | 2023-08-09     | cs.CV, cs.DL                            | Recent advancements in Digital Document Restoration (DDR) have led tosignificant breakthroughs in analyzing highly damaged written artifacts. Amongthose, there has been an increasing interest in applying ArtificialIntelligence techniques for virtually unwrapping and automatically detectingink on the Herculaneum papyri collection. This collection consists ofcarbonized scrolls and fragments of documents, which have been digitized viaX-ray tomography to allow the development of ad-hoc deep learning-based DDRsolutions. In this work, we propose a modification of the Fast FourierConvolution operator for volumetric data and apply it in a segmentationarchitecture for ink detection on the challenging Herculaneum papyri,demonstrating its suitability via deep experimental analysis. To encourage theresearch on this task and the application of the proposed operator to othertasks involving volumetric data, we will release our implementation(https://github.com/aimagelab/vffc)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.05070v1 |
| 283 | Foreground Object Search by Distilling Composite Image Feature                                                                                       | Bo Zhang                       | 2023-08-09     | cs.CV                                   | Foreground object search (FOS) aims to find compatible foreground objects fora given background image, producing realistic composite image. We observe thatcompetitive retrieval performance could be achieved by using a discriminator topredict the compatibility of composite image, but this approach hasunaffordable time cost. To this end, we propose a novel FOS method viadistilling composite feature (DiscoFOS). Specifically, the abovementioneddiscriminator serves as teacher network. The student network employs twoencoders to extract foreground feature and background feature. Theirinteraction output is enforced to match the composite image feature from theteacher network. Additionally, previous works did not release their datasets,so we contribute two datasets for FOS task: S-FOSD dataset with syntheticcomposite images and R-FOSD dataset with real composite images. Extensiveexperiments on our two datasets demonstrate the superiority of the proposedmethod over previous approaches. The dataset and code are available athttps://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.04990v1 |
| 284 | SelectNAdapt: Support Set Selection for Few-Shot Domain Adaptation                                                                                   | Youssef Dawoud                 | 2023-08-09     | cs.CV                                   | Generalisation of deep neural networks becomes vulnerable when distributionshifts are encountered between train (source) and test (target) domain data.Few-shot domain adaptation mitigates this issue by adapting deep neuralnetworks pre-trained on the source domain to the target domain using a randomlyselected and annotated support set from the target domain. This paper arguesthat randomly selecting the support set can be further improved for effectivelyadapting the pre-trained source models to the target domain. Alternatively, wepropose SelectNAdapt, an algorithm to curate the selection of the target domainsamples, which are then annotated and included in the support set. Inparticular, for the K-shot adaptation problem, we first leverageself-supervision to learn features of the target domain data. Then, we proposea per-class clustering scheme of the learned target domain features and selectK representative target samples using a distance-based scoring function.Finally, we bring our selection setup towards a practical ground by relying onpseudo-labels for clustering semantically similar target domain samples. Ourexperiments show promising results on three few-shot domain adaptationbenchmarks for image recognition compared to related approaches and thestandard random selection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.04946v1 |
| 285 | Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection                                                                                | Tetiana Gula                   | 2023-08-09     | cs.CV, cs.AI                            | Anomaly detection (AD) in images, identifying significant deviations fromnormality, is a critical issue in computer vision. This paper introduces anovel approach to dimensionality reduction for AD using pre-trainedconvolutional neural network (CNN) that incorporate EfficientNet models. Weinvestigate the importance of component selection and propose two types of treesearch approaches, both employing a greedy strategy, for optimal eigencomponentselection. Our study conducts three main experiments to evaluate theeffectiveness of our approach. The first experiment explores the influence oftest set performance on component choice, the second experiment examines theperformance when we train on one anomaly type and evaluate on all other types,and the third experiment investigates the impact of using a minimum number ofimages for training and selecting them based on anomaly types. Our approachaims to find the optimal subset of components that deliver the highestperformance score, instead of focusing solely on the proportion of varianceexplained by each component and also understand the components behaviour indifferent settings. Our results indicate that the proposed method surpassesboth Principal Component Analysis (PCA) and Negated Principal ComponentAnalysis (NPCA) in terms of detection accuracy, even when using fewercomponents. Thus, our approach provides a promising alternative to conventionaldimensionality reduction techniques in AD, and holds potential to enhance theefficiency and effectiveness of AD systems.                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2308.04944v1 |
| 286 | JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition                             | Lucian Bicsi                   | 2023-08-09     | cs.CV, cs.LG                            | We propose JEDI, a multi-dataset semi-supervised learning method, whichefficiently combines knowledge from multiple experts, learned on differentdatasets, to train and improve the performance of individual, per dataset,student models. Our approach achieves this by addressing two important problemsin current machine learning research: generalization across datasets andlimitations of supervised training due to scarcity of labeled data. We startwith an arbitrary number of experts, pretrained on their own specific dataset,which form the initial set of student models. The teachers are immediatelyderived by concatenating the feature representations from the penultimatelayers of the students. We then train all models in a student-teachersemi-supervised learning scenario until convergence. In our efficient approach,student-teacher training is carried out jointly and end-to-end, showing thatboth students and teachers improve their generalization capacity duringtraining. We validate our approach on four video action recognition datasets.By simultaneously considering all datasets within a unified semi-supervisedsetting, we demonstrate significant improvements over the initial experts.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2308.04934v1 |
| 287 | WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields                                                                                         | Muyu Xu                        | 2023-08-09     | cs.CV                                   | Neural Radiance Field (NeRF) has shown impressive performance in novel viewsynthesis via implicit scene representation. However, it usually suffers frompoor scalability as requiring densely sampled images for each new scene.Several studies have attempted to mitigate this problem by integratingMulti-View Stereo (MVS) technique into NeRF while they still entail acumbersome fine-tuning process for new scenes. Notably, the rendering qualitywill drop severely without this fine-tuning process and the errors mainlyappear around the high-frequency features. In the light of this observation, wedesign WaveNeRF, which integrates wavelet frequency decomposition into MVS andNeRF to achieve generalizable yet high-quality synthesis without any per-sceneoptimization. To preserve high-frequency information when generating 3D featurevolumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integratingthe discrete wavelet transform into the classical cascade MVS, whichdisentangles high-frequency information explicitly. With that, disentangledfrequency features can be injected into classic NeRF via a novel hybrid neuralrenderer to yield faithful high-frequency details, and an intuitivefrequency-guided sampling strategy can be designed to suppress artifacts aroundhigh-frequency regions. Extensive experiments over three widely studiedbenchmarks show that WaveNeRF achieves superior generalizable radiance fieldmodeling when only given three images as input.                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.04826v1 |
| 288 | HyperCoil-Recon: A Hypernetwork-based Adaptive Coil Configuration Task Switching Network for MRI Reconstruction                                      | Sriprabha Ramanarayanan        | 2023-08-09     | eess.IV, cs.CV                          | Parallel imaging, a fast MRI technique, involves dynamic adjustments based onthe configuration i.e. number, positioning, and sensitivity of the coils withrespect to the anatomy under study. Conventional deep learning-based imagereconstruction models have to be trained or fine-tuned for each configuration,posing a barrier to clinical translation, given the lack of computationalresources and machine learning expertise for clinicians to train models atdeployment. Joint training on diverse datasets learns a single weight set thatmight underfit to deviated configurations. We propose, HyperCoil-Recon, ahypernetwork-based coil configuration task-switching network for multi-coil MRIreconstruction that encodes varying configurations of the numbers of coils in amulti-tasking perspective, posing each configuration as a task. Thehypernetworks infer and embed task-specific weights into the reconstructionnetwork, 1) effectively utilizing the contextual knowledge of common andvarying image features among the various fields-of-view of the coils, and 2)enabling generality to unseen configurations at test time. Experiments revealthat our approach 1) adapts on the fly to various unseen configurations up to32 coils when trained on lower numbers (i.e. 7 to 11) of randomly varyingcoils, and to 120 deviated unseen configurations when trained on 18configurations in a single model, 2) matches the performance of coilconfiguration-specific models, and 3) outperforms configuration-invariantmodels with improvement margins of around 1 dB / 0.03 and 0.3 dB / 0.02 in PSNR/ SSIM for knee and brain data. Our code is available athttps://github.com/sriprabhar/HyperCoil-Recon                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.04821v1 |
| 289 | PointMBF: A Multi-scale Bidirectional Fusion Network for Unsupervised RGB-D Point Cloud Registration                                                 | Mingzhi Yuan                   | 2023-08-09     | cs.CV                                   | Point cloud registration is a task to estimate the rigid transformationbetween two unaligned scans, which plays an important role in many computervision applications. Previous learning-based works commonly focus on supervisedregistration, which have limitations in practice. Recently, with the advance ofinexpensive RGB-D sensors, several learning-based works utilize RGB-D data toachieve unsupervised registration. However, most of existing unsupervisedmethods follow a cascaded design or fuse RGB-D data in a unidirectional manner,which do not fully exploit the complementary information in the RGB-D data. Toleverage the complementary information more effectively, we propose a networkimplementing multi-scale bidirectional fusion between RGB images and pointclouds generated from depth images. By bidirectionally fusing visual andgeometric features in multi-scales, more distinctive deep features forcorrespondence estimation can be obtained, making our registration moreaccurate. Extensive experiments on ScanNet and 3DMatch demonstrate that ourmethod achieves new state-of-the-art performance. Code will be released athttps://github.com/phdymz/PointMBF                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.04782v1 |
| 290 | Objects do not disappear: Video object detection by single-frame object location anticipation                                                        | Xin Liu                        | 2023-08-09     | cs.CV                                   | Objects in videos are typically characterized by continuous smooth motion. Weexploit continuous smooth motion in three ways. 1) Improved accuracy by usingobject motion as an additional source of supervision, which we obtain byanticipating object locations from a static keyframe. 2) Improved efficiency byonly doing the expensive feature computations on a small subset of all frames.Because neighboring video frames are often redundant, we only compute featuresfor a single static keyframe and predict object locations in subsequent frames.3) Reduced annotation cost, where we only annotate the keyframe and use smoothpseudo-motion between keyframes. We demonstrate computational efficiency,annotation efficiency, and improved mean average precision compared to thestate-of-the-art on four datasets: ImageNet VID, EPIC KITCHENS-55,YouTube-BoundingBoxes, and Waymo Open dataset. Our source code is available athttps://github.com/L-KID/Videoobject-detection-by-location-anticipation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.04770v1 |
| 291 | Bird's-Eye-View Scene Graph for Vision-Language Navigation                                                                                           | Rui Liu                        | 2023-08-09     | cs.CV, cs.AI                            | Vision-language navigation (VLN), which entails an agent to navigate 3Denvironments following human instructions, has shown great advances. However,current agents are built upon panoramic observations, which hinders theirability to perceive 3D scene geometry and easily leads to ambiguous selectionof panoramic view. To address these limitations, we present a BEV Scene Graph(BSG), which leverages multi-step BEV representations to encode scene layoutsand geometric cues of indoor environment under the supervision of 3D detection.During navigation, BSG builds a local BEV representation at each step andmaintains a BEV-based global scene map, which stores and organizes all theonline collected local BEV representations according to their topologicalrelations. Based on BSG, the agent predicts a local BEV grid-level decisionscore and a global graph-level decision score, combined with a sub-viewselection score on panoramic views, for more accurate action prediction. Ourapproach significantly outperforms state-of-the-art methods on REVERIE, R2R,and R4R, showing the potential of BEV perception in VLN.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.04758v2 |
| 292 | GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization                                                                        | Hao Fang                       | 2023-08-09     | cs.CV, cs.CR                            | Federated Learning (FL) has recently emerged as a promising distributedmachine learning framework to preserve clients' privacy, by allowing multipleclients to upload the gradients calculated from their local data to a centralserver. Recent studies find that the exchanged gradients also take the risk ofprivacy leakage, e.g., an attacker can invert the shared gradients and recoversensitive data against an FL system by leveraging pre-trained generativeadversarial networks (GAN) as prior knowledge. However, performing gradientinversion attacks in the latent space of the GAN model limits their expressionability and generalizability. To tackle these challenges, we propose\textbf{G}radient \textbf{I}nversion over \textbf{F}eature \textbf{D}omains(GIFD), which disassembles the GAN model and searches the feature domains ofthe intermediate layers. Instead of optimizing only over the initial latentcode, we progressively change the optimized layer, from the initial latentspace to intermediate layers closer to the output images. In addition, wedesign a regularizer to avoid unreal image generation by adding a small ${l_1}$ball constraint to the searching range. We also extend GIFD to theout-of-distribution (OOD) setting, which weakens the assumption that thetraining sets of GANs and FL tasks obey the same data distribution. Extensiveexperiments demonstrate that our method can achieve pixel-level reconstructionand is superior to the existing methods. Notably, GIFD also shows greatgeneralizability under different defense strategy settings and batch sizes.                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.04699v1 |
| 293 | Which Tokens to Use? Investigating Token Reduction in Vision Transformers                                                                            | Joakim Bruslund Haurum         | 2023-08-09     | cs.CV                                   | Since the introduction of the Vision Transformer (ViT), researchers havesought to make ViTs more efficient by removing redundant information in theprocessed tokens. While different methods have been explored to achieve thisgoal, we still lack understanding of the resulting reduction patterns and howthose patterns differ across token reduction methods and datasets. To closethis gap, we set out to understand the reduction patterns of 10 different tokenreduction methods using four image classification datasets. By systematicallycomparing these methods on the different classification tasks, we find that theTop-K pruning method is a surprisingly strong baseline. Through in-depthanalysis of the different methods, we determine that: the reduction patternsare generally not consistent when varying the capacity of the backbone model,the reduction patterns of pruning-based methods significantly differ from fixedradial patterns, and the reduction patterns of pruning-based methods arecorrelated across classification datasets. Finally we report that thesimilarity of reduction patterns is a moderate-to-strong proxy for modelperformance. Project page at https://vap.aau.dk/tokens.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.04657v1 |
| 294 | Rendering Humans from Object-Occluded Monocular Videos                                                                                               | Tiange Xiang                   | 2023-08-08     | cs.CV                                   | 3D understanding and rendering of moving humans from monocular videos is achallenging task. Despite recent progress, the task remains difficult inreal-world scenarios, where obstacles may block the camera view and causepartial occlusions in the captured videos. Existing methods cannot handle suchdefects due to two reasons. First, the standard rendering strategy relies onpoint-point mapping, which could lead to dramatic disparities between thevisible and occluded areas of the body. Second, the naive direct regressionapproach does not consider any feasibility criteria (ie, prior information) forrendering under occlusions. To tackle the above drawbacks, we present OccNeRF,a neural rendering method that achieves better rendering of humans in severelyoccluded scenes. As direct solutions to the two drawbacks, we proposesurface-based rendering by integrating geometry and visibility priors. Wevalidate our method on both simulated and real-world occlusions and demonstrateour method's superiority.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.04622v1 |
| 295 | FocalFormer3D : Focusing on Hard Instance for 3D Object Detection                                                                                    | Yilun Chen                     | 2023-08-08     | cs.CV                                   | False negatives (FN) in 3D object detection, {\em e.g.}, missing predictionsof pedestrians, vehicles, or other obstacles, can lead to potentially dangeroussituations in autonomous driving. While being fatal, this issue is understudiedin many current 3D detection methods. In this work, we propose Hard InstanceProbing (HIP), a general pipeline that identifies \textit{FN} in a multi-stagemanner and guides the models to focus on excavating difficult instances. For 3Dobject detection, we instantiate this method as FocalFormer3D, a simple yeteffective detector that excels at excavating difficult objects and improvingprediction recall. FocalFormer3D features a multi-stage query generation todiscover hard objects and a box-level transformer decoder to efficientlydistinguish objects from massive object candidates. Experimental results on thenuScenes and Waymo datasets validate the superior performance of FocalFormer3D.The advantage leads to strong performance on both detection and tracking, inboth LiDAR and multi-modal settings. Notably, FocalFormer3D achieves a 70.5 mAPand 73.9 NDS on nuScenes detection benchmark, while the nuScenes trackingbenchmark shows 72.1 AMOTA, both ranking 1st place on the nuScenes LiDARleaderboard. Our code is available at\url{https://github.com/NVlabs/FocalFormer3D}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.04556v1 |
| 296 | Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation                                                                                 | Shuangrui Ding                 | 2023-08-08     | cs.CV                                   | Transformers have become the primary backbone of the computer visioncommunity due to their impressive performance. However, the unfriendlycomputation cost impedes their potential in the video recognition domain. Tooptimize the speed-accuracy trade-off, we propose Semantic-aware TemporalAccumulation score (STA) to prune spatio-temporal tokens integrally. STA scoreconsiders two critical factors: temporal redundancy and semantic importance.The former depicts a specific region based on whether it is a new occurrence ora seen entity by aggregating token-to-token similarity in consecutive frameswhile the latter evaluates each token based on its contribution to the overallprediction. As a result, tokens with higher scores of STA carry more temporalredundancy as well as lower semantics thus being pruned. Based on the STAscore, we are able to progressively prune the tokens without introducing anyadditional parameters or requiring further re-training. We directly apply theSTA module to off-the-shelf ViT and VideoSwin backbones, and the empiricalresults on Kinetics-400 and Something-Something V2 achieve over 30% computationreduction with a negligible ~0.2% accuracy drop. The code is released athttps://github.com/Mark12Ding/STA.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2308.04549v1 |
| 297 | Estimation of Human Condition at Disaster Site Using Aerial Drone Images                                                                             | Tomoki Arai                    | 2023-08-08     | cs.CV                                   | Drones are being used to assess the situation in various disasters. In thisstudy, we investigate a method to automatically estimate the damage status ofpeople based on their actions in aerial drone images in order to understanddisaster sites faster and save labor. We constructed a new dataset of aerialimages of human actions in a hypothetical disaster that occurred in an urbanarea, and classified the human damage status using 3D ResNet. The resultsshowed that the status with characteristic human actions could be classifiedwith a recall rate of more than 80%, while other statuses with similar humanactions could only be classified with a recall rate of about 50%. In addition,a cloud-based VR presentation application suggested the effectiveness of usingdrones to understand the disaster site and estimate the human condition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.04535v1 |
| 298 | Unsupervised Camouflaged Object Segmentation as Domain Adaptation                                                                                    | Yi Zhang                       | 2023-08-08     | cs.CV                                   | Deep learning for unsupervised image segmentation remains challenging due tothe absence of human labels. The common idea is to train a segmentation head,with the supervision of pixel-wise pseudo-labels generated based on therepresentation of self-supervised backbones. By doing so, the model performancedepends much on the distance between the distributions of target datasets andthe pre-training dataset (e.g., ImageNet). In this work, we investigate a newtask, namely unsupervised camouflaged object segmentation (UCOS), where thetarget objects own a common rarely-seen attribute, i.e., camouflage.Unsurprisingly, we find that the state-of-the-art unsupervised models strugglein adapting UCOS, due to the domain gap between the properties of generic andcamouflaged objects. To this end, we formulate the UCOS as a source-freeunsupervised domain adaptation task (UCOS-DA), where both source labels andtarget labels are absent during the whole model training process. Specifically,we define a source model consisting of self-supervised vision transformerspre-trained on ImageNet. On the other hand, the target domain includes a simplelinear layer (i.e., our target model) and unlabeled camouflaged objects. Wethen design a pipeline for foreground-background-contrastive self-adversarialdomain adaptation, to achieve robust UCOS. As a result, our baseline modelachieves superior segmentation performance when compared with competingunsupervised models on the UCOS benchmark, with the training set which's scaleis only one tenth of the supervised COS counterpart.                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.04528v1 |
| 299 | Person Re-Identification without Identification via Event Anonymization                                                                              | Shafiq Ahmad                   | 2023-08-08     | cs.CV                                   | Wide-scale use of visual surveillance in public spaces puts individualprivacy at stake while increasing resource consumption (energy, bandwidth, andcomputation). Neuromorphic vision sensors (event-cameras) have been recentlyconsidered a valid solution to the privacy issue because they do not capturedetailed RGB visual information of the subjects in the scene. However, recentdeep learning architectures have been able to reconstruct images from eventcameras with high fidelity, reintroducing a potential threat to privacy forevent-based vision applications. In this paper, we aim to anonymizeevent-streams to protect the identity of human subjects against such imagereconstruction attacks. To achieve this, we propose an end-to-end networkarchitecture jointly optimized for the twofold objective of preserving privacyand performing a downstream task such as person ReId. Our network learns toscramble events, enforcing the degradation of images recovered from the privacyattacker. In this work, we also bring to the community the first everevent-based person ReId dataset gathered to evaluate the performance of ourapproach. We validate our approach with extensive experiments and reportresults on the synthetic event data simulated from the publicly availableSoftBio dataset and our proposed Event-ReId dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.04402v4 |
| 300 | All-pairs Consistency Learning for Weakly Supervised Semantic Segmentation                                                                           | Weixuan Sun                    | 2023-08-08     | cs.CV                                   | In this work, we propose a new transformer-based regularization to betterlocalize objects for Weakly supervised semantic segmentation (WSSS). Inimage-level WSSS, Class Activation Map (CAM) is adopted to generate objectlocalization as pseudo segmentation labels. To address the partial activationissue of the CAMs, consistency regularization is employed to maintainactivation intensity invariance across various image augmentations. However,such methods ignore pair-wise relations among regions within each CAM, whichcapture context and should also be invariant across image views. To this end,we propose a new all-pairs consistency regularization (ACR). Given a pair ofaugmented views, our approach regularizes the activation intensities between apair of augmented views, while also ensuring that the affinity across regionswithin each view remains consistent. We adopt vision transformers as theself-attention mechanism naturally embeds pair-wise affinity. This enables usto simply regularize the distance between the attention matrices of augmentedimage pairs. Additionally, we introduce a novel class-wise localization methodthat leverages the gradients of the class token. Our method can be seamlesslyintegrated into existing WSSS methods using transformers without modifying thearchitectures. We evaluate our method on PASCAL VOC and MS COCO datasets. Ourmethod produces noticeably better class localization maps (67.3% mIoU on PASCALVOC train), resulting in superior WSSS performances.                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.04321v1 |
| 301 | Enhancing Adversarial Robustness in Low-Label Regime via Adaptively Weighted Regularization and Knowledge Distillation                               | Dongyoon Yang                  | 2023-08-08     | cs.LG, cs.AI                            | Adversarial robustness is a research area that has recently received a lot ofattention in the quest for trustworthy artificial intelligence. However, recentworks on adversarial robustness have focused on supervised learning where it isassumed that labeled data is plentiful. In this paper, we investigatesemi-supervised adversarial training where labeled data is scarce. We derivetwo upper bounds for the robust risk and propose a regularization term forunlabeled data motivated by these two upper bounds. Then, we develop asemi-supervised adversarial training algorithm that combines the proposedregularization term with knowledge distillation using a semi-supervised teacher(i.e., a teacher model trained using a semi-supervised learning algorithm). Ourexperiments show that our proposed algorithm achieves state-of-the-artperformance with significant margins compared to existing algorithms. Inparticular, compared to supervised learning algorithms, performance of ourproposed algorithm is not much worse even when the amount of labeled data isvery small. For example, our algorithm with only 8\% labeled data is comparableto supervised adversarial training algorithms that use all labeled data, bothin terms of standard and robust accuracies on CIFAR-10.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2308.04061v1 |
| 302 | An Empirical Analysis of Range for 3D Object Detection                                                                                               | Neehar Peri                    | 2023-08-08     | cs.CV, cs.RO                            | LiDAR-based 3D detection plays a vital role in autonomous navigation.Surprisingly, although autonomous vehicles (AVs) must detect both near-fieldobjects (for collision avoidance) and far-field objects (for longer-termplanning), contemporary benchmarks focus only on near-field 3D detection.However, AVs must detect far-field objects for safe navigation. In this paper,we present an empirical analysis of far-field 3D detection using the long-rangedetection dataset Argoverse 2.0 to better understand the problem, and share thefollowing insight: near-field LiDAR measurements are dense and optimallyencoded by small voxels, while far-field measurements are sparse and are betterencoded with large voxels. We exploit this observation to build a collection ofrange experts tuned for near-vs-far field detection, and propose simpletechniques to efficiently ensemble models for long-range detection that improveefficiency by 33% and boost accuracy by 3.2% CDS.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.04054v1 |
| 303 | Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning                                                                           | Hanjae Kim                     | 2023-08-08     | cs.CV                                   | Compositional zero-shot learning (CZSL) aims to recognize unseen compositionswith prior knowledge of known primitives (attribute and object). Previous worksfor CZSL often suffer from grasping the contextuality between attribute andobject, as well as the discriminability of visual features, and the long-taileddistribution of real-world compositional data. We propose a simple and scalableframework called Composition Transformer (CoT) to address these issues. CoTemploys object and attribute experts in distinctive manners to generaterepresentative embeddings, using the visual network hierarchically. The objectexpert extracts representative object embeddings from the final layer in abottom-up manner, while the attribute expert makes attribute embeddings in atop-down manner with a proposed object-guided attention module that modelscontextuality explicitly. To remedy biased prediction caused by imbalanced datadistribution, we develop a simple minority attribute augmentation (MAA) thatsynthesizes virtual samples by mixing two images and oversampling minorityattribute classes. Our method achieves SoTA performance on several benchmarks,including MIT-States, C-GQA, and VAW-CZSL. We also demonstrate theeffectiveness of CoT in improving visual discrimination and addressing themodel bias from the imbalanced data distribution. The code is available athttps://github.com/HanjaeKim98/CoT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.04016v1 |
| 304 | Coarse-to-Fine: Learning Compact Discriminative Representation for Single-Stage Image Retrieval                                                      | Yunquan Zhu                    | 2023-08-08     | cs.CV                                   | Image retrieval targets to find images from a database that are visuallysimilar to the query image. Two-stage methods following retrieve-and-rerankparadigm have achieved excellent performance, but their separate local andglobal modules are inefficient to real-world applications. To better trade-offretrieval efficiency and accuracy, some approaches fuse global and localfeature into a joint representation to perform single-stage image retrieval.However, they are still challenging due to various situations to tackle,$e.g.$, background, occlusion and viewpoint. In this work, we design aCoarse-to-Fine framework to learn Compact Discriminative representation (CFCD)for end-to-end single-stage image retrieval-requiring only image-level labels.Specifically, we first design a novel adaptive softmax-based loss whichdynamically tunes its scale and margin within each mini-batch and increasesthem progressively to strengthen supervision during training and intra-classcompactness. Furthermore, we propose a mechanism which attentively selectsprominent local descriptors and infuse fine-grained semantic relations into theglobal representation by a hard negative sampling strategy to optimizeinter-class distinctiveness at a global scale. Extensive experimental resultshave demonstrated the effectiveness of our method, which achievesstate-of-the-art single-stage image retrieval performance on benchmarks such asRevisited Oxford and Revisited Paris. Code is available athttps://github.com/bassyess/CFCD.                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.04008v1 |
| 305 | PARTNER: Level up the Polar Representation for LiDAR 3D Object Detection                                                                             | Ming Nie                       | 2023-08-08     | cs.CV                                   | Recently, polar-based representation has shown promising properties inperceptual tasks. In addition to Cartesian-based approaches, which separatepoint clouds unevenly, representing point clouds as polar grids has beenrecognized as an alternative due to (1) its advantage in robust performanceunder different resolutions and (2) its superiority in streaming-basedapproaches. However, state-of-the-art polar-based detection methods inevitablysuffer from the feature distortion problem because of the non-uniform divisionof polar representation, resulting in a non-negligible performance gap comparedto Cartesian-based approaches. To tackle this issue, we present PARTNER, anovel 3D object detector in the polar coordinate. PARTNER alleviates thedilemma of feature distortion with global representation re-alignment andfacilitates the regression by introducing instance-level geometric informationinto the detection head. Extensive experiments show overwhelming advantages instreaming-based detection and different resolutions. Furthermore, our methodoutperforms the previous polar-based works with remarkable margins of 3.68% and9.15% on Waymo and ONCE validation set, thus achieving competitive results overthe state-of-the-art methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.03982v1 |
| 306 | CheXFusion: Effective Fusion of Multi-View Features using Transformers for Long-Tailed Chest X-Ray Classification                                    | Dongkyun Kim                   | 2023-08-08     | cs.CV, cs.AI                            | Medical image classification poses unique challenges due to the long-taileddistribution of diseases, the co-occurrence of diagnostic findings, and themultiple views available for each study or patient. This paper introduces oursolution to the ICCV CVAMD 2023 Shared Task on CXR-LT: Multi-Label Long-TailedClassification on Chest X-Rays. Our approach introduces CheXFusion, atransformer-based fusion module incorporating multi-view images. The fusionmodule, guided by self-attention and cross-attention mechanisms, efficientlyaggregates multi-view features while considering label co-occurrence.Furthermore, we explore data balancing and self-training methods to optimizethe model's performance. Our solution achieves state-of-the-art results with0.372 mAP in the MIMIC-CXR test set, securing 1st place in the competition. Oursuccess in the task underscores the significance of considering multi-viewsettings, class imbalance, and label co-occurrence in medical imageclassification. Public code is available athttps://github.com/dongkyuk/CXR-LT-public-solution                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.03968v1 |
| 307 | Deterministic Neural Illumination Mapping for Efficient Auto-White Balance Correction                                                                | Furkan Kınlı                   | 2023-08-07     | cs.CV                                   | Auto-white balance (AWB) correction is a critical operation in image signalprocessors for accurate and consistent color correction across variousillumination scenarios. This paper presents a novel and efficient AWBcorrection method that achieves at least 35 times faster processing withequivalent or superior performance on high-resolution images for the currentstate-of-the-art methods. Inspired by deterministic color style transfer, ourapproach introduces deterministic illumination color mapping, leveraginglearnable projection matrices for both canonical illumination form andAWB-corrected output. It involves feeding high-resolution images andcorresponding latent representations into a mapping module to derive acanonical form, followed by another mapping module that maps the pixel valuesto those for the corrected version. This strategy is designed asresolution-agnostic and also enables seamless integration of any pre-trainedAWB network as the backbone. Experimental results confirm the effectiveness ofour approach, revealing significant performance improvements and reduced timecomplexity compared to state-of-the-art methods. Our method provides anefficient deep learning-based AWB correction solution, promising real-time,high-quality color correction for digital imaging applications. Source code isavailable at https://github.com/birdortyedi/DeNIM/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2308.03939v1 |
| 308 | ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals | Milad Sikaroudi                | 2023-08-07     | cs.CV, cs.AI                            | We propose an exhaustive methodology that leverages all levels of featureabstraction, targeting an enhancement in the generalizability of imageclassification to unobserved hospitals. Our approach incorporatesaugmentation-based self-supervision with common distribution shifts inhistopathology scenarios serving as the pretext task. This enables us to deriveinvariant features from training images without relying on training labels,thereby covering different abstraction levels. Moving onto the subsequentabstraction level, we employ a domain alignment module to facilitate furtherextraction of invariant features across varying training hospitals. Torepresent the highly specific features of participating hospitals, an encoderis trained to classify hospital labels, independent of their diagnostic labels.The features from each of these encoders are subsequently disentangled tominimize redundancy and segregate the features. This representation, whichspans a broad spectrum of semantic information, enables the development of amodel demonstrating increased robustness to unseen images from disparatedistributions. Experimental results from the PACS dataset (a domaingeneralization benchmark), a synthetic dataset created by applyinghistopathology-specific jitters to the MHIST dataset (defining differentdomains with varied distribution shifts), and a Renal Cell Carcinoma datasetderived from four image repositories from TCGA, collectively indicate that ourproposed model is adept at managing varying levels of image granularity. Thus,it shows improved generalizability when faced with new, out-of-distributionhospital images.                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.03936v2 |
| 309 | TIJO: Trigger Inversion with Joint Optimization for Defending Multimodal Backdoored Models                                                           | Indranil Sur                   | 2023-08-07     | cs.CV                                   | We present a Multimodal Backdoor Defense technique TIJO (Trigger Inversionusing Joint Optimization). Recent work arXiv:2112.07668 has demonstratedsuccessful backdoor attacks on multimodal models for the Visual QuestionAnswering task. Their dual-key backdoor trigger is split across two modalities(image and text), such that the backdoor is activated if and only if thetrigger is present in both modalities. We propose TIJO that defends againstdual-key attacks through a joint optimization that reverse-engineers thetrigger in both the image and text modalities. This joint optimization ischallenging in multimodal models due to the disconnected nature of the visualpipeline which consists of an offline feature extractor, whose output is thenfused with the text using a fusion module. The key insight enabling the jointoptimization in TIJO is that the trigger inversion needs to be carried out inthe object detection box feature space as opposed to the pixel space. Wedemonstrate the effectiveness of our method on the TrojVQA benchmark, whereTIJO improves upon the state-of-the-art unimodal methods from an AUC of 0.6 to0.92 on multimodal dual-key backdoors. Furthermore, our method also improvesupon the unimodal baselines on unimodal backdoors. We present ablation studiesand qualitative results to provide insights into our algorithm such as thecritical importance of overlaying the inverted feature triggers on all visualfeatures during trigger inversion. The prototype implementation of TIJO isavailable at https://github.com/SRI-CSL/TIJO.                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.03906v1 |
| 310 | Guarding the Guardians: Automated Analysis of Online Child Sexual Abuse                                                                              | Juanita Puentes                | 2023-08-07     | cs.AI                                   | Online violence against children has increased globally recently, demandingurgent attention. Competent authorities manually analyze abuse complaints tocomprehend crime dynamics and identify patterns. However, the manual analysisof these complaints presents a challenge because it exposes analysts to harmfulcontent during the review process. Given these challenges, we present a novelsolution, an automated tool designed to analyze children's sexual abuse reportscomprehensively. By automating the analysis process, our tool significantlyreduces the risk of exposure to harmful content by categorizing the reports onthree dimensions: Subject, Degree of Criminality, and Damage. Furthermore,leveraging our multidisciplinary team's expertise, we introduce a novelapproach to annotate the collected data, enabling a more in-depth analysis ofthe reports. This approach improves the comprehension of fundamental patternsand trends, enabling law enforcement agencies and policymakers to createfocused strategies in the fight against children's violence.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.03880v2 |
| 311 | From Sky to the Ground: A Large-scale Benchmark and Simple Baseline Towards Real Rain Removal                                                        | Yun Guo                        | 2023-08-07     | cs.CV                                   | Learning-based image deraining methods have made great progress. However, thelack of large-scale high-quality paired training samples is the main bottleneckto hamper the real image deraining (RID). To address this dilemma and advanceRID, we construct a Large-scale High-quality Paired real rain benchmark(LHP-Rain), including 3000 video sequences with 1 million high-resolution(1920*1080) frame pairs. The advantages of the proposed dataset over theexisting ones are three-fold: rain with higher-diversity and larger-scale,image with higher-resolution and higher-quality ground-truth. Specifically, thereal rains in LHP-Rain not only contain the classical rainstreak/veiling/occlusion in the sky, but also the \textbf{splashing on theground} overlooked by deraining community. Moreover, we propose a novel robustlow-rank tensor recovery model to generate the GT with better separating thestatic background from the dynamic rain. In addition, we design a simpletransformer-based single image deraining baseline, which simultaneously utilizethe self-attention and cross-layer attention within the image and rain layerwith discriminative feature representation. Extensive experiments verify thesuperiority of the proposed dataset and deraining method over state-of-the-art.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.03867v2 |
| 312 | 3D Motion Magnification: Visualizing Subtle Motions with Time Varying Radiance Fields                                                                | Brandon Y. Feng                | 2023-08-07     | cs.CV                                   | Motion magnification helps us visualize subtle, imperceptible motion.However, prior methods only work for 2D videos captured with a fixed camera. Wepresent a 3D motion magnification method that can magnify subtle motions fromscenes captured by a moving camera, while supporting novel view rendering. Werepresent the scene with time-varying radiance fields and leverage the Eulerianprinciple for motion magnification to extract and amplify the variation of theembedding of a fixed point over time. We study and validate our proposedprinciple for 3D motion magnification using both implicit and tri-plane-basedradiance fields as our underlying 3D scene representation. We evaluate theeffectiveness of our method on both synthetic and real-world scenes capturedunder various camera setups.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.03757v1 |
| 313 | Learning Concise and Descriptive Attributes for Visual Recognition                                                                                   | An Yan                         | 2023-08-07     | cs.CV                                   | Recent advances in foundation models present new opportunities forinterpretable visual recognition -- one can first query Large Language Models(LLMs) to obtain a set of attributes that describe each class, then applyvision-language models to classify images via these attributes. Pioneering workshows that querying thousands of attributes can achieve performance competitivewith image features. However, our further investigation on 8 datasets revealsthat LLM-generated attributes in a large quantity perform almost the same asrandom words. This surprising finding suggests that significant noise may bepresent in these attributes. We hypothesize that there exist subsets ofattributes that can maintain the classification performance with much smallersizes, and propose a novel learning-to-search method to discover those concisesets of attributes. As a result, on the CUB dataset, our method achievesperformance close to that of massive LLM-generated attributes (e.g., 10kattributes for CUB), yet using only 32 attributes in total to distinguish 200bird species. Furthermore, our new paradigm demonstrates several additionalbenefits: higher interpretability and interactivity for humans, and the abilityto summarize knowledge for a recognition task.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2308.03685v1 |
| 314 | Dual Aggregation Transformer for Image Super-Resolution                                                                                              | Zheng Chen                     | 2023-08-07     | cs.CV                                   | Transformer has recently gained considerable popularity in low-level visiontasks, including image super-resolution (SR). These networks utilizeself-attention along different dimensions, spatial or channel, and achieveimpressive performance. This inspires us to combine the two dimensions inTransformer for a more powerful representation capability. Based on the aboveidea, we propose a novel Transformer model, Dual Aggregation Transformer (DAT),for image SR. Our DAT aggregates features across spatial and channeldimensions, in the inter-block and intra-block dual manner. Specifically, wealternately apply spatial and channel self-attention in consecutive Transformerblocks. The alternate strategy enables DAT to capture the global context andrealize inter-block feature aggregation. Furthermore, we propose the adaptiveinteraction module (AIM) and the spatial-gate feed-forward network (SGFN) toachieve intra-block feature aggregation. AIM complements two self-attentionmechanisms from corresponding dimensions. Meanwhile, SGFN introduces additionalnon-linear spatial information in the feed-forward network. Extensiveexperiments show that our DAT surpasses current methods. Code and models areobtainable at https://github.com/zhengchen1999/DAT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.03364v2 |
| 315 | Source-free Domain Adaptive Human Pose Estimation                                                                                                    | Qucheng Peng                   | 2023-08-06     | cs.CV, cs.AI, cs.LG                     | Human Pose Estimation (HPE) is widely used in various fields, includingmotion analysis, healthcare, and virtual reality. However, the great expensesof labeled real-world datasets present a significant challenge for HPE. Toovercome this, one approach is to train HPE models on synthetic datasets andthen perform domain adaptation (DA) on real-world data. Unfortunately, existingDA methods for HPE neglect data privacy and security by using both source andtarget data in the adaptation process. To this end, we propose a new task,named source-free domain adaptive HPE, which aims to address the challenges ofcross-domain learning of HPE without access to source data during theadaptation process. We further propose a novel framework that consists of threemodels: source model, intermediate model, and target model, which explores thetask from both source-protect and target-relevant perspectives. Thesource-protect module preserves source information more effectively whileresisting noise, and the target-relevant module reduces the sparsity of spatialrepresentations by building a novel spatial probability space, andpose-specific contrastive learning and information maximization are proposed onthe basis of this space. Comprehensive experiments on several domain adaptiveHPE benchmarks show that the proposed method outperforms existing approaches bya considerable margin. The codes are available athttps://github.com/davidpengucf/SFDAHPE.                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.03202v4 |
| 316 | FireFly A Synthetic Dataset for Ember Detection in Wildfire                                                                                          | Yue Hu                         | 2023-08-06     | cs.CV, cs.LG, I.4                       | This paper presents "FireFly", a synthetic dataset for ember detectioncreated using Unreal Engine 4 (UE4), designed to overcome the current lack ofember-specific training resources. To create the dataset, we present a toolthat allows the automated generation of the synthetic labeled dataset withadjustable parameters, enabling data diversity from various environmentalconditions, making the dataset both diverse and customizable based on userrequirements. We generated a total of 19,273 frames that have been used toevaluate FireFly on four popular object detection models. Further to minimizehuman intervention, we leveraged a trained model to create a semi-automaticlabeling process for real-life ember frames. Moreover, we demonstrated an up to8.57% improvement in mean Average Precision (mAP) in real-world wildfirescenarios compared to models trained exclusively on a small real dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.03164v1 |
| 317 | CGBA: Curvature-aware Geometric Black-box Attack                                                                                                     | Md Farhamdur Reza              | 2023-08-06     | cs.CV                                   | Decision-based black-box attacks often necessitate a large number of queriesto craft an adversarial example. Moreover, decision-based attacks based onquerying boundary points in the estimated normal vector direction often sufferfrom inefficiency and convergence issues. In this paper, we propose a novelquery-efficient curvature-aware geometric decision-based black-box attack(CGBA) that conducts boundary search along a semicircular path on a restricted2D plane to ensure finding a boundary point successfully irrespective of theboundary curvature. While the proposed CGBA attack can work effectively for anarbitrary decision boundary, it is particularly efficient in exploiting the lowcurvature to craft high-quality adversarial examples, which is widely seen andexperimentally verified in commonly used classifiers under non-targetedattacks. In contrast, the decision boundaries often exhibit higher curvatureunder targeted attacks. Thus, we develop a new query-efficient variant, CGBA-H,that is adapted for the targeted attack. In addition, we further design analgorithm to obtain a better initial boundary point at the expense of someextra queries, which considerably enhances the performance of the targetedattack. Extensive experiments are conducted to evaluate the performance of ourproposed methods against some well-known classifiers on the ImageNet andCIFAR10 datasets, demonstrating the superiority of CGBA and CGBA-H overstate-of-the-art non-targeted and targeted attacks, respectively. The sourcecode is available at https://github.com/Farhamdur/CGBA.                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.03163v1 |
| 318 | Learning Fine-Grained Features for Pixel-wise Video Correspondences                                                                                  | Rui Li                         | 2023-08-06     | cs.CV                                   | Video analysis tasks rely heavily on identifying the pixels from differentframes that correspond to the same visual target. To tackle this problem,recent studies have advocated feature learning methods that aim to learndistinctive representations to match the pixels, especially in aself-supervised fashion. Unfortunately, these methods have difficulties fortiny or even single-pixel visual targets. Pixel-wise video correspondences weretraditionally related to optical flows, which however lead to deterministiccorrespondences and lack robustness on real-world videos. We address theproblem of learning features for establishing pixel-wise correspondences.Motivated by optical flows as well as the self-supervised feature learning, wepropose to use not only labeled synthetic videos but also unlabeled real-worldvideos for learning fine-grained representations in a holistic framework. Weadopt an adversarial learning scheme to enhance the generalization ability ofthe learned features. Moreover, we design a coarse-to-fine framework to pursuehigh computational efficiency. Our experimental results on a series ofcorrespondence-based tasks demonstrate that the proposed method outperformsstate-of-the-art rivals in both accuracy and efficiency.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2308.03040v1 |
| 319 | Generative Approach for Probabilistic Human Mesh Recovery using Diffusion Models                                                                     | Hanbyel Cho                    | 2023-08-05     | cs.CV                                   | This work focuses on the problem of reconstructing a 3D human body mesh froma given 2D image. Despite the inherent ambiguity of the task of human meshrecovery, most existing works have adopted a method of regressing a singleoutput. In contrast, we propose a generative approach framework, called"Diffusion-based Human Mesh Recovery (Diff-HMR)" that takes advantage of thedenoising diffusion process to account for multiple plausible outcomes. Duringthe training phase, the SMPL parameters are diffused from ground-truthparameters to random distribution, and Diff-HMR learns the reverse process ofthis diffusion. In the inference phase, the model progressively refines thegiven random SMPL parameters into the corresponding parameters that align withthe input image. Diff-HMR, being a generative approach, is capable ofgenerating diverse results for the same input image as the input noise varies.We conduct validation experiments, and the results demonstrate that theproposed framework effectively models the inherent ambiguity of the task ofhuman mesh recovery in a probabilistic manner. The code is available athttps://github.com/hanbyel0105/Diff-HMR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.02963v2 |
| 320 | An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability                                                               | Bin Chen                       | 2023-08-05     | cs.CV                                   | While the transferability property of adversarial examples allows theadversary to perform black-box attacks (i.e., the attacker has no knowledgeabout the target model), the transfer-based adversarial attacks have gainedgreat attention. Previous works mostly study gradient variation or imagetransformations to amplify the distortion on critical parts of inputs. Thesemethods can work on transferring across models with limited differences, i.e.,from CNNs to CNNs, but always fail in transferring across models with widedifferences, such as from CNNs to ViTs. Alternatively, model ensembleadversarial attacks are proposed to fuse outputs from surrogate models withdiverse architectures to get an ensemble loss, making the generated adversarialexample more likely to transfer to other models as it can fool multiple modelsconcurrently. However, existing ensemble attacks simply fuse the outputs of thesurrogate models evenly, thus are not efficacious to capture and amplify theintrinsic transfer information of adversarial examples. In this paper, wepropose an adaptive ensemble attack, dubbed AdaEA, to adaptively control thefusion of the outputs from each model, via monitoring the discrepancy ratio oftheir contributions towards the adversarial objective. Furthermore, an extradisparity-reduced filter is introduced to further synchronize the updatedirection. As a result, we achieve considerable improvement over the existingensemble attacks on various datasets, and the proposed AdaEA can also boostexisting transfer-based attacks, which further demonstrates its efficacy andversatility.                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2308.02897v1 |
| 321 | Sketch and Text Guided Diffusion Model for Colored Point Cloud Generation                                                                            | Zijie Wu                       | 2023-08-05     | cs.CV, cs.MM                            | Diffusion probabilistic models have achieved remarkable success in textguided image generation. However, generating 3D shapes is still challenging dueto the lack of sufficient data containing 3D models along with theirdescriptions. Moreover, text based descriptions of 3D shapes are inherentlyambiguous and lack details. In this paper, we propose a sketch and text guidedprobabilistic diffusion model for colored point cloud generation thatconditions the denoising process jointly with a hand drawn sketch of the objectand its textual description. We incrementally diffuse the point coordinates andcolor values in a joint diffusion process to reach a Gaussian distribution.Colored point cloud generation thus amounts to learning the reverse diffusionprocess, conditioned by the sketch and text, to iteratively recover the desiredshape and color. Specifically, to learn effective sketch-text embedding, ourmodel adaptively aggregates the joint embedding of text prompt and the sketchbased on a capsule attention network. Our model uses staged diffusion togenerate the shape and then assign colors to different parts conditioned on theappearance prompt while preserving precise shapes from the first stage. Thisgives our model the flexibility to extend to multiple tasks, such as appearancere-editing and part segmentation. Experimental results demonstrate that ourmodel outperforms recent state-of-the-art in point cloud generation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.02874v1 |
| 322 | FB-BEV: BEV Representation from Forward-Backward View Transformations                                                                                | Zhiqi Li                       | 2023-08-04     | cs.CV                                   | View Transformation Module (VTM), where transformations happen betweenmulti-view image features and Bird-Eye-View (BEV) representation, is a crucialstep in camera-based BEV perception systems. Currently, the two most prominentVTM paradigms are forward projection and backward projection. Forwardprojection, represented by Lift-Splat-Shoot, leads to sparsely projected BEVfeatures without post-processing. Backward projection, with BEVFormer being anexample, tends to generate false-positive BEV features from incorrectprojections due to the lack of utilization on depth. To address the abovelimitations, we propose a novel forward-backward view transformation module.Our approach compensates for the deficiencies in both existing methods,allowing them to enhance each other to obtain higher quality BEVrepresentations mutually. We instantiate the proposed module with FB-BEV, whichachieves a new state-of-the-art result of 62.4% NDS on the nuScenes test set.Code and models are available at https://github.com/NVlabs/FB-BEV.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2308.02236v2 |
| 323 | Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation                                        | Jinyuan Liu                    | 2023-08-04     | cs.CV                                   | Multi-modality image fusion and segmentation play a vital role in autonomousdriving and robotic operation. Early efforts focus on boosting the performancefor only one task, \emph{e.g.,} fusion or segmentation, making it hard toreach~`Best of Both Worlds'. To overcome this issue, in this paper, we proposea \textbf{M}ulti-\textbf{i}nteractive \textbf{F}eature learning architecturefor image fusion and \textbf{Seg}mentation, namely SegMiF, and exploitdual-task correlation to promote the performance of both tasks. The SegMiF isof a cascade structure, containing a fusion sub-network and a commonly usedsegmentation sub-network. By slickly bridging intermediate features between twocomponents, the knowledge learned from the segmentation task can effectivelyassist the fusion task. Also, the benefited fusion network supports thesegmentation one to perform more pretentiously. Besides, a hierarchicalinteractive attention block is established to ensure fine-grained mapping ofall the vital information between two tasks, so that the modality/semanticfeatures can be fully mutual-interactive. In addition, a dynamic weight factoris introduced to automatically adjust the corresponding weights of each task,which can balance the interactive feature correspondence and break through thelimitation of laborious tuning. Furthermore, we construct a smart multi-wavebinocular imaging system and collect a full-time multi-modality benchmark with15 annotated pixel-level categories for image fusion and segmentation.Extensive experiments on several public datasets and our benchmark demonstratethat the proposed method outputs visually appealing fused images and performaveragely $7.66\%$ higher segmentation mIoU in the real-world scene than thestate-of-the-art approaches. The source code and benchmark are available at\url{https://github.com/JinyuanLiu-CV/SegMiF}.                            | http://arxiv.org/abs/2308.02097v1 |
| 324 | LiDAR-Camera Panoptic Segmentation via Geometry-Consistent and Semantic-Aware Alignment                                                              | Zhiwei Zhang                   | 2023-08-03     | cs.CV, cs.AI                            | 3D panoptic segmentation is a challenging perception task that requires bothsemantic segmentation and instance segmentation. In this task, we notice thatimages could provide rich texture, color, and discriminative information, whichcan complement LiDAR data for evident performance improvement, but their fusionremains a challenging problem. To this end, we propose LCPS, the firstLiDAR-Camera Panoptic Segmentation network. In our approach, we conductLiDAR-Camera fusion in three stages: 1) an Asynchronous Compensation PixelAlignment (ACPA) module that calibrates the coordinate misalignment caused byasynchronous problems between sensors; 2) a Semantic-Aware Region Alignment(SARA) module that extends the one-to-one point-pixel mapping to one-to-manysemantic relations; 3) a Point-to-Voxel feature Propagation (PVP) module thatintegrates both geometric and semantic fusion information for the entire pointcloud. Our fusion strategy improves about 6.9% PQ performance over theLiDAR-only baseline on NuScenes dataset. Extensive quantitative and qualitativeexperiments further demonstrate the effectiveness of our novel framework. Thecode will be released at https://github.com/zhangzw12319/lcps.git.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2308.01686v2 |
| 325 | Get the Best of Both Worlds: Improving Accuracy and Transferability by Grassmann Class Representation                                                | Haoqi Wang                     | 2023-08-03     | cs.CV                                   | We generalize the class vectors found in neural networks to linear subspaces(i.e.~points in the Grassmann manifold) and show that the Grassmann ClassRepresentation (GCR) enables the simultaneous improvement in accuracy andfeature transferability. In GCR, each class is a subspace and the logit isdefined as the norm of the projection of a feature onto the class subspace. Weintegrate Riemannian SGD into deep learning frameworks such that classsubspaces in a Grassmannian are jointly optimized with the rest modelparameters. Compared to the vector form, the representative capability ofsubspaces is more powerful. We show that on ImageNet-1K, the top-1 error ofResNet50-D, ResNeXt50, Swin-T and Deit3-S are reduced by 5.6%, 4.5%, 3.0% and3.5%, respectively. Subspaces also provide freedom for features to vary and weobserved that the intra-class feature variability grows when the subspacedimension increases. Consequently, we found the quality of GCR features isbetter for downstream tasks. For ResNet50-D, the average linear transferaccuracy across 6 datasets improves from 77.98% to 79.70% compared to thestrong baseline of vanilla softmax. For Swin-T, it improves from 81.5% to 83.4%and for Deit3, it improves from 73.8% to 81.4%. With these encouraging results,we believe that more applications could benefit from the Grassmann classrepresentation. Code is released at https://github.com/innerlee/GCR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2308.01547v1 |
| 326 | Efficient neural supersampling on a novel gaming dataset                                                                                             | Antoine Mercier                | 2023-08-03     | cs.CV, cs.GR, cs.LG                     | Real-time rendering for video games has become increasingly challenging dueto the need for higher resolutions, framerates and photorealism. Supersamplinghas emerged as an effective solution to address this challenge. Our workintroduces a novel neural algorithm for supersampling rendered content that is4 times more efficient than existing methods while maintaining the same levelof accuracy. Additionally, we introduce a new dataset which provides auxiliarymodalities such as motion vectors and depth generated using graphics renderingfeatures like viewport jittering and mipmap biasing at different resolutions.We believe that this dataset fills a gap in the current dataset landscape andcan serve as a valuable resource to help measure progress in the field andadvance the state-of-the-art in super-resolution techniques for gaming content.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.01483v1 |
| 327 | Body Knowledge and Uncertainty Modeling for Monocular 3D Human Body Reconstruction                                                                   | Yufei Zhang                    | 2023-08-01     | cs.CV                                   | While 3D body reconstruction methods have made remarkable progress recently,it remains difficult to acquire the sufficiently accurate and numerous 3Dsupervisions required for training. In this paper, we propose \textbf{KNOWN}, aframework that effectively utilizes body \textbf{KNOW}ledge andu\textbf{N}certainty modeling to compensate for insufficient 3D supervisions.KNOWN exploits a comprehensive set of generic body constraints derived fromwell-established body knowledge. These generic constraints precisely andexplicitly characterize the reconstruction plausibility and enable 3Dreconstruction models to be trained without any 3D data. Moreover, existingmethods typically use images from multiple datasets during training, which canresult in data noise (\textit{e.g.}, inconsistent joint annotation) and dataimbalance (\textit{e.g.}, minority images representing unusual poses orcaptured from challenging camera views). KNOWN solves these problems through anovel probabilistic framework that models both aleatoric and epistemicuncertainty. Aleatoric uncertainty is encoded in a robust NegativeLog-Likelihood (NLL) training loss, while epistemic uncertainty is used toguide model refinement. Experiments demonstrate that KNOWN's bodyreconstruction outperforms prior weakly-supervised approaches, particularly onthe challenging minority images.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.00799v1 |
| 328 | ELFNet: Evidential Local-global Fusion for Stereo Matching                                                                                           | Jieming Lou                    | 2023-08-01     | cs.CV                                   | Although existing stereo matching models have achieved continuousimprovement, they often face issues related to trustworthiness due to theabsence of uncertainty estimation. Additionally, effectively leveragingmulti-scale and multi-view knowledge of stereo pairs remains unexplored. Inthis paper, we introduce the \textbf{E}vidential \textbf{L}ocal-global\textbf{F}usion (ELF) framework for stereo matching, which endows bothuncertainty estimation and confidence-aware fusion with trustworthy heads.Instead of predicting the disparity map alone, our model estimates anevidential-based disparity considering both aleatoric and epistemicuncertainties. With the normal inverse-Gamma distribution as a bridge, theproposed framework realizes intra evidential fusion of multi-level predictionsand inter evidential fusion between cost-volume-based and transformer-basedstereo matching. Extensive experimental results show that the proposedframework exploits multi-view information effectively and achievesstate-of-the-art overall performance both on accuracy and cross-domaingeneralization.  The codes are available at https://github.com/jimmy19991222/ELFNet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2308.00728v1 |
| 329 | DriveAdapter: Breaking the Coupling Barrier of Perception and Planning in End-to-End Autonomous Driving                                              | Xiaosong Jia                   | 2023-08-01     | cs.RO, cs.CV                            | End-to-end autonomous driving aims to build a fully differentiable systemthat takes raw sensor data as inputs and directly outputs the plannedtrajectory or control signals of the ego vehicle. State-of-the-art methodsusually follow the `Teacher-Student' paradigm. The Teacher model usesprivileged information (ground-truth states of surrounding agents and mapelements) to learn the driving strategy. The student model only has access toraw sensor data and conducts behavior cloning on the data collected by theteacher model. By eliminating the noise of the perception part during planninglearning, state-of-the-art works could achieve better performance withsignificantly less data compared to those coupled ones.  However, under the current Teacher-Student paradigm, the student model stillneeds to learn a planning head from scratch, which could be challenging due tothe redundant and noisy nature of raw sensor inputs and the casual confusionissue of behavior cloning. In this work, we aim to explore the possibility ofdirectly adopting the strong teacher model to conduct planning while lettingthe student model focus more on the perception part. We find that even equippedwith a SOTA perception model, directly letting the student model learn therequired inputs of the teacher model leads to poor driving performance, whichcomes from the large distribution gap between predicted privileged inputs andthe ground-truth.  To this end, we propose DriveAdapter, which employs adapters with the featurealignment objective function between the student (perception) and teacher(planning) modules. Additionally, since the pure learning-based teacher modelitself is imperfect and occasionally breaks safety rules, we propose a methodof action-guided feature learning with a mask for those imperfect teacherfeatures to further inject the priors of hand-crafted rules into the learningprocess. | http://arxiv.org/abs/2308.00398v2 |
| 330 | Improving Generalization of Adversarial Training via Robust Critical Fine-Tuning                                                                     | Kaijie Zhu                     | 2023-08-01     | cs.LG, cs.CV                            | Deep neural networks are susceptible to adversarial examples, posing asignificant security risk in critical applications. Adversarial Training (AT)is a well-established technique to enhance adversarial robustness, but it oftencomes at the cost of decreased generalization ability. This paper proposesRobustness Critical Fine-Tuning (RiFT), a novel approach to enhancegeneralization without compromising adversarial robustness. The core idea ofRiFT is to exploit the redundant capacity for robustness by fine-tuning theadversarially trained model on its non-robust-critical module. To do so, weintroduce module robust criticality (MRC), a measure that evaluates thesignificance of a given module to model robustness under worst-case weightperturbations. Using this measure, we identify the module with the lowest MRCvalue as the non-robust-critical module and fine-tune its weights to obtainfine-tuned weights. Subsequently, we linearly interpolate between theadversarially trained weights and fine-tuned weights to derive the optimalfine-tuned model weights. We demonstrate the efficacy of RiFT on ResNet18,ResNet34, and WideResNet34-10 models trained on CIFAR10, CIFAR100, andTiny-ImageNet datasets. Our experiments show that \method can significantlyimprove both generalization and out-of-distribution robustness by around 1.5%while maintaining or even slightly enhancing adversarial robustness. Code isavailable at https://github.com/microsoft/robustlearn.                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2308.02533v1 |
| 331 | Deep Image Harmonization with Learnable Augmentation                                                                                                 | Li Niu                         | 2023-08-01     | cs.CV                                   | The goal of image harmonization is adjusting the foreground appearance in acomposite image to make the whole image harmonious. To construct pairedtraining images, existing datasets adopt different ways to adjust theillumination statistics of foregrounds of real images to produce syntheticcomposite images. However, different datasets have considerable domain gap andthe performances on small-scale datasets are limited by insufficient trainingdata. In this work, we explore learnable augmentation to enrich theillumination diversity of small-scale datasets for better harmonizationperformance. In particular, our designed SYthetic COmposite Network (SycoNet)takes in a real image with foreground mask and a random vector to learnsuitable color transformation, which is applied to the foreground of this realimage to produce a synthetic composite image. Comprehensive experimentsdemonstrate the effectiveness of our proposed learnable augmentation for imageharmonization. The code of SycoNet is released athttps://github.com/bcmi/SycoNet-Adaptive-Image-Harmonization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2308.00376v1 |
| 332 | Deep Image Harmonization with Globally Guided Feature Transformation and Relation Distillation                                                       | Li Niu                         | 2023-08-01     | cs.CV                                   | Given a composite image, image harmonization aims to adjust the foregroundillumination to be consistent with background. Previous methods have exploredtransforming foreground features to achieve competitive performance. In thiswork, we show that using global information to guide foreground featuretransformation could achieve significant improvement. Besides, we propose totransfer the foreground-background relation from real images to compositeimages, which can provide intermediate supervision for the transformed encoderfeatures. Additionally, considering the drawbacks of existing harmonizationdatasets, we also contribute a ccHarmony dataset which simulates the naturalillumination variation. Extensive experiments on iHarmony4 and our contributeddataset demonstrate the superiority of our method. Our ccHarmony dataset isreleased at https://github.com/bcmi/Image-Harmonization-Dataset-ccHarmony.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2308.00356v1 |
| 333 | Online Prototype Learning for Online Continual Learning                                                                                              | Yujie Wei                      | 2023-08-01     | cs.CV                                   | Online continual learning (CL) studies the problem of learning continuouslyfrom a single-pass data stream while adapting to new data and mitigatingcatastrophic forgetting. Recently, by storing a small subset of old data,replay-based methods have shown promising performance. Unlike previous methodsthat focus on sample storage or knowledge distillation against catastrophicforgetting, this paper aims to understand why the online learning models failto generalize well from a new perspective of shortcut learning. We identifyshortcut learning as the key limiting factor for online CL, where the learnedfeatures may be biased, not generalizable to new tasks, and may have an adverseimpact on knowledge distillation. To tackle this issue, we present the onlineprototype learning (OnPro) framework for online CL. First, we propose onlineprototype equilibrium to learn representative features against shortcutlearning and discriminative features to avoid class confusion, ultimatelyachieving an equilibrium status that separates all seen classes well whilelearning new classes. Second, with the feedback of online prototypes, we devisea novel adaptive prototypical feedback mechanism to sense the classes that areeasily misclassified and then enhance their boundaries. Extensive experimentalresults on widely-used benchmark datasets demonstrate the superior performanceof OnPro over the state-of-the-art baseline methods. Source code is availableat https://github.com/weilllllls/OnPro.                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.00301v1 |
| 334 | Revisiting the Parameter Efficiency of Adapters from the Perspective of Precision Redundancy                                                         | Shibo Jie                      | 2023-07-31     | cs.CV                                   | Current state-of-the-art results in computer vision depend in part onfine-tuning large pre-trained vision models. However, with the exponentialgrowth of model sizes, the conventional full fine-tuning, which needs to storea individual network copy for each tasks, leads to increasingly huge storageand transmission overhead. Adapter-based Parameter-Efficient Tuning (PET)methods address this challenge by tuning lightweight adapters inserted into thefrozen pre-trained models. In this paper, we investigate how to make adapterseven more efficient, reaching a new minimum size required to store atask-specific fine-tuned network. Inspired by the observation that theparameters of adapters converge at flat local minima, we find that adapters areresistant to noise in parameter space, which means they are also resistant tolow numerical precision. To train low-precision adapters, we propose acomputational-efficient quantization method which minimizes the quantizationerror. Through extensive experiments, we find that low-precision adaptersexhibit minimal performance degradation, and even 1-bit precision is sufficientfor adapters. The experimental results demonstrate that 1-bit adaptersoutperform all other PET methods on both the VTAB-1K benchmark and few-shotFGVC tasks, while requiring the smallest storage size. Our findings show, forthe first time, the significant potential of quantization techniques in PET,providing a general solution to enhance the parameter efficiency ofadapter-based PET methods. Code: https://github.com/JieShibo/PETL-ViT                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2307.16867v1 |
| 335 | UniVTG: Towards Unified Video-Language Temporal Grounding                                                                                            | Kevin Qinghong Lin             | 2023-07-31     | cs.CV                                   | Video Temporal Grounding (VTG), which aims to ground target clips from videos(such as consecutive intervals or disjoint shots) according to custom languagequeries (e.g., sentences or words), is key for video browsing on social media.Most methods in this direction develop taskspecific models that are trainedwith type-specific labels, such as moment retrieval (time interval) andhighlight detection (worthiness curve), which limits their abilities togeneralize to various VTG tasks and labels. In this paper, we propose to Unifythe diverse VTG labels and tasks, dubbed UniVTG, along three directions:Firstly, we revisit a wide range of VTG labels and tasks and define a unifiedformulation. Based on this, we develop data annotation schemes to createscalable pseudo supervision. Secondly, we develop an effective and flexiblegrounding model capable of addressing each task and making full use of eachlabel. Lastly, thanks to the unified framework, we are able to unlock temporalgrounding pretraining from large-scale diverse labels and develop strongergrounding abilities e.g., zero-shot grounding. Extensive experiments on threetasks (moment retrieval, highlight detection and video summarization) acrossseven datasets (QVHighlights, Charades-STA, TACoS, Ego4D, YouTube Highlights,TVSum, and QFVS) demonstrate the effectiveness and flexibility of our proposedframework. The codes are available at https://github.com/showlab/UniVTG.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.16715v2 |
| 336 | DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation                                                                       | Runyang Feng                   | 2023-07-31     | cs.CV                                   | Denoising diffusion probabilistic models that were initially proposed forrealistic image generation have recently shown success in various perceptiontasks (e.g., object detection and image segmentation) and are increasinglygaining attention in computer vision. However, extending such models tomulti-frame human pose estimation is non-trivial due to the presence of theadditional temporal dimension in videos. More importantly, learningrepresentations that focus on keypoint regions is crucial for accuratelocalization of human joints. Nevertheless, the adaptation of thediffusion-based methods remains unclear on how to achieve such objective. Inthis paper, we present DiffPose, a novel diffusion architecture that formulatesvideo-based human pose estimation as a conditional heatmap generation problem.First, to better leverage temporal information, we propose SpatioTemporalRepresentation Learner which aggregates visual evidences across frames and usesthe resulting features in each denoising step as a condition. In addition, wepresent a mechanism called Lookup-based MultiScale Feature Interaction thatdetermines the correlations between local joints and global contexts acrossmultiple scales. This mechanism generates delicate representations that focuson keypoint regions. Altogether, by extending diffusion models, we show twounique characteristics from DiffPose on pose estimation task: (i) the abilityto combine multiple sets of pose estimates to improve prediction accuracy,particularly for challenging joints, and (ii) the ability to adjust the numberof iterative steps for feature refinement without retraining the model.DiffPose sets new state-of-the-art results on three benchmarks: PoseTrack2017,PoseTrack2018, and PoseTrack21.                                                                                                                                              | http://arxiv.org/abs/2307.16687v2 |
| 337 | Guiding Image Captioning Models Toward More Specific Captions                                                                                        | Simon Kornblith                | 2023-07-31     | cs.CV, cs.LG                            | Image captioning is conventionally formulated as the task of generatingcaptions for images that match the distribution of reference image-captionpairs. However, reference captions in standard captioning datasets are shortand may not uniquely identify the images they describe. These problems arefurther exacerbated when models are trained directly on image-alt text pairscollected from the internet. In this work, we show that it is possible togenerate more specific captions with minimal changes to the training process.We implement classifier-free guidance for an autoregressive captioning model byfine-tuning it to estimate both conditional and unconditional distributionsover captions. The guidance scale applied at decoding controls a trade-offbetween maximizing $p(\mathrm{caption}|\mathrm{image})$ and$p(\mathrm{image}|\mathrm{caption})$. Compared to standard greedy decoding,decoding with a guidance scale of 2 substantially improves reference-freemetrics such as CLIPScore (0.808 vs. 0.775) and caption$\to$image retrievalperformance in the CLIP embedding space (recall@1 44.6% vs. 26.5%), but worsensstandard reference-based captioning metrics (e.g., CIDEr 78.6 vs 126.1). Wefurther explore the use of language models to guide the decoding process,obtaining small improvements over the Pareto frontier of reference-free vs.reference-based captioning metrics that arises from classifier-free guidance,and substantially improving the quality of captions generated from a modeltrained only on minimally curated web data.                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.16686v1 |
| 338 | Transferable Decoding with Visual Entities for Zero-Shot Image Captioning                                                                            | Junjie Fei                     | 2023-07-31     | cs.CV, cs.CL                            | Image-to-text generation aims to describe images using natural language.Recently, zero-shot image captioning based on pre-trained vision-languagemodels (VLMs) and large language models (LLMs) has made significant progress.However, we have observed and empirically demonstrated that these methods aresusceptible to modality bias induced by LLMs and tend to generate descriptionscontaining objects (entities) that do not actually exist in the image butfrequently appear during training (i.e., object hallucination). In this paper,we propose ViECap, a transferable decoding model that leverages entity-awaredecoding to generate descriptions in both seen and unseen scenarios. ViECapincorporates entity-aware hard prompts to guide LLMs' attention toward thevisual entities present in the image, enabling coherent caption generationacross diverse scenes. With entity-aware hard prompts, ViECap is capable ofmaintaining performance when transferring from in-domain to out-of-domainscenarios. Extensive experiments demonstrate that ViECap sets a newstate-of-the-art cross-domain (transferable) captioning and performscompetitively in-domain captioning compared to previous VLMs-based zero-shotmethods. Our code is available at: https://github.com/FeiElysia/ViECap                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.16525v1 |
| 339 | Towards General Low-Light Raw Noise Synthesis and Modeling                                                                                           | Feng Zhang                     | 2023-07-31     | cs.CV, cs.MM, eess.IV                   | Modeling and synthesizing low-light raw noise is a fundamental problem forcomputational photography and image processing applications. Although mostrecent works have adopted physics-based models to synthesize noise, thesignal-independent noise in low-light conditions is far more complicated andvaries dramatically across camera sensors, which is beyond the description ofthese models. To address this issue, we introduce a new perspective tosynthesize the signal-independent noise by a generative model. Specifically, wesynthesize the signal-dependent and signal-independent noise in a physics- andlearning-based manner, respectively. In this way, our method can be consideredas a general model, that is, it can simultaneously learn different noisecharacteristics for different ISO levels and generalize to various sensors.Subsequently, we present an effective multi-scale discriminator termed Fouriertransformer discriminator (FTD) to distinguish the noise distributionaccurately. Additionally, we collect a new low-light raw denoising (LRD)dataset for training and benchmarking. Qualitative validation shows that thenoise generated by our proposed noise model can be highly similar to the realnoise in terms of distribution. Furthermore, extensive denoising experimentsdemonstrate that our method performs favorably against state-of-the-art methodson different sensors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.16508v2 |
| 340 | DRAW: Defending Camera-shooted RAW against Image Manipulation                                                                                        | Xiaoxiao Hu                    | 2023-07-31     | cs.CV, cs.MM, eess.IV                   | RAW files are the initial measurement of scene radiance widely used in mostcameras, and the ubiquitously-used RGB images are converted from RAW datathrough Image Signal Processing (ISP) pipelines. Nowadays, digital images arerisky of being nefariously manipulated. Inspired by the fact that innateimmunity is the first line of body defense, we propose DRAW, a novel scheme ofdefending images against manipulation by protecting their sources, i.e.,camera-shooted RAWs. Specifically, we design a lightweight Multi-frequencyPartial Fusion Network (MPF-Net) friendly to devices with limited computingresources by frequency learning and partial feature fusion. It introducesinvisible watermarks as protective signal into the RAW data. The protectioncapability can not only be transferred into the rendered RGB images regardlessof the applied ISP pipeline, but also is resilient to post-processingoperations such as blurring or compression. Once the image is manipulated, wecan accurately identify the forged areas with a localization network. Extensiveexperiments on several famous RAW datasets, e.g., RAISE, FiveK and SIDD,indicate the effectiveness of our method. We hope that this technique can beused in future cameras as an option for image protection, which couldeffectively restrict image manipulation at the source.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.16418v1 |
| 341 | JOTR: 3D Joint Contrastive Learning with Transformers for Occluded Human Mesh Recovery                                                               | Jiahao Li                      | 2023-07-31     | cs.CV                                   | In this study, we focus on the problem of 3D human mesh recovery from asingle image under obscured conditions. Most state-of-the-art methods aim toimprove 2D alignment technologies, such as spatial averaging and 2D jointsampling. However, they tend to neglect the crucial aspect of 3D alignment byimproving 3D representations. Furthermore, recent methods struggle to separatethe target human from occlusion or background in crowded scenes as theyoptimize the 3D space of target human with 3D joint coordinates as localsupervision. To address these issues, a desirable method would involve aframework for fusing 2D and 3D features and a strategy for optimizing the 3Dspace globally. Therefore, this paper presents 3D JOint contrastive learningwith TRansformers (JOTR) framework for handling occluded 3D human meshrecovery. Our method includes an encoder-decoder transformer architecture tofuse 2D and 3D representations for achieving 2D$\&$3D aligned results in acoarse-to-fine manner and a novel 3D joint contrastive learning approach foradding explicitly global supervision for the 3D feature space. The contrastivelearning approach includes two contrastive losses: joint-to-joint contrast forenhancing the similarity of semantically similar voxels (i.e., human joints),and joint-to-non-joint contrast for ensuring discrimination from others (e.g.,occlusions and background). Qualitative and quantitative analyses demonstratethat our method outperforms state-of-the-art competitors on bothocclusion-specific and standard benchmarks, significantly improving thereconstruction of occluded humans.                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.16377v2 |
| 342 | Fusing VHR Post-disaster Aerial Imagery and LiDAR Data for Roof Classification in the Caribbean                                                      | Isabelle Tingzon               | 2023-07-30     | cs.CV                                   | Accurate and up-to-date information on building characteristics is essentialfor vulnerability assessment; however, the high costs and long timeframesassociated with conducting traditional field surveys can be an obstacle toobtaining critical exposure datasets needed for disaster risk management. Inthis work, we leverage deep learning techniques for the automatedclassification of roof characteristics from very high-resolution orthophotosand airborne LiDAR data obtained in Dominica following Hurricane Maria in 2017.We demonstrate that the fusion of multimodal earth observation data performsbetter than using any single data source alone. Using our proposed methods, weachieve F1 scores of 0.93 and 0.92 for roof type and roof materialclassification, respectively. This work is intended to help governments producemore timely building information to improve resilience and disaster response inthe Caribbean.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.16177v3 |
| 343 | Rapid Flood Inundation Forecast Using Fourier Neural Operator                                                                                        | Alexander Y. Sun               | 2023-07-29     | physics.flu-dyn, cs.LG                  | Flood inundation forecast provides critical information for emergencyplanning before and during flood events. Real time flood inundation forecasttools are still lacking. High-resolution hydrodynamic modeling has become moreaccessible in recent years, however, predicting flood extents at the street andbuilding levels in real-time is still computationally demanding. Here wepresent a hybrid process-based and data-driven machine learning (ML) approachfor flood extent and inundation depth prediction. We used the Fourier neuraloperator (FNO), a highly efficient ML method, for surrogate modeling. The FNOmodel is demonstrated over an urban area in Houston (Texas, U.S.) by trainingusing simulated water depths (in 15-min intervals) from six historical stormevents and then tested over two holdout events. Results show FNO outperformsthe baseline U-Net model. It maintains high predictability at all lead timestested (up to 3 hrs) and performs well when applying to new sites, suggestingstrong generalization skill.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.16090v1 |
| 344 | XMem++: Production-level Video Segmentation From Few Annotated Frames                                                                                | Maksym Bekuzarov               | 2023-07-29     | cs.CV, cs.GR                            | Despite advancements in user-guided video segmentation, extracting complexobjects consistently for highly complex scenes is still a labor-intensive task,especially for production. It is not uncommon that a majority of frames need tobe annotated. We introduce a novel semi-supervised video object segmentation(SSVOS) model, XMem++, that improves existing memory-based models, with apermanent memory module. Most existing methods focus on single frameannotations, while our approach can effectively handle multiple user-selectedframes with varying appearances of the same object or region. Our method canextract highly consistent results while keeping the required number of frameannotations low. We further introduce an iterative and attention-based framesuggestion mechanism, which computes the next best frame for annotation. Ourmethod is real-time and does not require retraining after each user input. Wealso introduce a new dataset, PUMaVOS, which covers new challenging use casesnot found in previous benchmarks. We demonstrate SOTA performance onchallenging (partial and multi-class) segmentation scenarios as well as longvideos, while ensuring significantly fewer frame annotations than any existingmethod. Project page: https://max810.github.io/xmem2-project-page/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.15958v2 |
| 345 | CMDA: Cross-Modality Domain Adaptation for Nighttime Semantic Segmentation                                                                           | Ruihao Xia                     | 2023-07-29     | cs.CV                                   | Most nighttime semantic segmentation studies are based on domain adaptationapproaches and image input. However, limited by the low dynamic range ofconventional cameras, images fail to capture structural details and boundaryinformation in low-light conditions. Event cameras, as a new form of visionsensors, are complementary to conventional cameras with their high dynamicrange. To this end, we propose a novel unsupervised Cross-Modality DomainAdaptation (CMDA) framework to leverage multi-modality (Images and Events)information for nighttime semantic segmentation, with only labels on daytimeimages. In CMDA, we design the Image Motion-Extractor to extract motioninformation and the Image Content-Extractor to extract content information fromimages, in order to bridge the gap between different modalities (Images toEvents) and domains (Day to Night). Besides, we introduce the first image-eventnighttime semantic segmentation dataset. Extensive experiments on both thepublic image dataset and the proposed image-event dataset demonstrate theeffectiveness of our proposed approach. We open-source our code, models, anddataset at https://github.com/XiaRho/CMDA.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.15942v1 |
| 346 | Effective Whole-body Pose Estimation with Two-stages Distillation                                                                                    | Zhendong Yang                  | 2023-07-29     | cs.CV                                   | Whole-body pose estimation localizes the human body, hand, face, and footkeypoints in an image. This task is challenging due to multi-scale body parts,fine-grained localization for low-resolution regions, and data scarcity.Meanwhile, applying a highly efficient and accurate pose estimator to widelyhuman-centric understanding and generation tasks is urgent. In this work, wepresent a two-stage pose \textbf{D}istillation for \textbf{W}hole-body\textbf{P}ose estimators, named \textbf{DWPose}, to improve their effectivenessand efficiency. The first-stage distillation designs a weight-decay strategywhile utilizing a teacher's intermediate feature and final logits with bothvisible and invisible keypoints to supervise the student from scratch. Thesecond stage distills the student model itself to further improve performance.Different from the previous self-knowledge distillation, this stage finetunesthe student's head with only 20% training time as a plug-and-play trainingstrategy. For data limitations, we explore the UBody dataset that containsdiverse facial expressions and hand gestures for real-life applications.Comprehensive experiments show the superiority of our proposed simple yeteffective methods. We achieve new state-of-the-art performance onCOCO-WholeBody, significantly boosting the whole-body AP of RTMPose-l from64.8% to 66.5%, even surpassing RTMPose-x teacher with 65.3% AP. We release aseries of models with different sizes, from tiny to large, for satisfyingvarious downstream tasks. Our codes and models are available athttps://github.com/IDEA-Research/DWPose.                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2307.15880v2 |
| 347 | What can Discriminator do? Towards Box-free Ownership Verification of Generative Adversarial Network                                                 | Ziheng Huang                   | 2023-07-29     | cs.CV, cs.CR                            | In recent decades, Generative Adversarial Network (GAN) and its variants haveachieved unprecedented success in image synthesis. However, well-trained GANsare under the threat of illegal steal or leakage. The prior studies on remoteownership verification assume a black-box setting where the defender can querythe suspicious model with specific inputs, which we identify is not enough forgeneration tasks. To this end, in this paper, we propose a novel IP protectionscheme for GANs where ownership verification can be done by checking outputsonly, without choosing the inputs (i.e., box-free setting). Specifically, wemake use of the unexploited potential of the discriminator to learn ahypersphere that captures the unique distribution learned by the pairedgenerator. Extensive evaluations on two popular GAN tasks and more than 10 GANarchitectures demonstrate our proposed scheme to effectively verify theownership. Our proposed scheme shown to be immune to popular input-basedremoval attacks and robust against other existing attacks. The source code andmodels are available athttps://github.com/AbstractTeen/gan_ownership_verification                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2307.15860v1 |
| 348 | Context-VQA: Towards Context-Aware and Purposeful Visual Question Answering                                                                          | Nandita Naik                   | 2023-07-28     | cs.CL, cs.CV                            | Visual question answering (VQA) has the potential to make the Internet moreaccessible in an interactive way, allowing people who cannot see images to askquestions about them. However, multiple studies have shown that people who areblind or have low-vision prefer image explanations that incorporate the contextin which an image appears, yet current VQA datasets focus on images inisolation. We argue that VQA models will not fully succeed at meeting people'sneeds unless they take context into account. To further motivate and analyzethe distinction between different contexts, we introduce Context-VQA, a VQAdataset that pairs images with contexts, specifically types of websites (e.g.,a shopping website). We find that the types of questions vary systematicallyacross contexts. For example, images presented in a travel context garner 2times more "Where?" questions, and images on social media and news garner 2.8and 1.8 times more "Who?" questions than the average. We also find that contexteffects are especially important when participants can't see the image. Theseresults demonstrate that context affects the types of questions asked and thatVQA models should be context-sensitive to better meet people's needs,especially in accessibility settings.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.15745v2 |
| 349 | MeMOTR: Long-Term Memory-Augmented Transformer for Multi-Object Tracking                                                                             | Ruopeng Gao                    | 2023-07-28     | cs.CV                                   | As a video task, Multiple Object Tracking (MOT) is expected to capturetemporal information of targets effectively. Unfortunately, most existingmethods only explicitly exploit the object features between adjacent frames,while lacking the capacity to model long-term temporal information. In thispaper, we propose MeMOTR, a long-term memory-augmented Transformer formulti-object tracking. Our method is able to make the same object's trackembedding more stable and distinguishable by leveraging long-term memoryinjection with a customized memory-attention layer. This significantly improvesthe target association ability of our model. Experimental results on DanceTrackshow that MeMOTR impressively surpasses the state-of-the-art method by 7.9% and13.0% on HOTA and AssA metrics, respectively. Furthermore, our model alsooutperforms other Transformer-based methods on association performance on MOT17and generalizes well on BDD100K. Code is available athttps://github.com/MCG-NJU/MeMOTR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.15700v2 |
| 350 | Scaling Data Generation in Vision-and-Language Navigation                                                                                            | Zun Wang                       | 2023-07-28     | cs.CV, cs.AI, cs.CL, cs.LG              | Recent research in language-guided visual navigation has demonstrated asignificant demand for the diversity of traversable environments and thequantity of supervision for training generalizable agents. To tackle the commondata scarcity issue in existing vision-and-language navigation datasets, wepropose an effective paradigm for generating large-scale data for learning,which applies 1200+ photo-realistic environments from HM3D and Gibson datasetsand synthesizes 4.9 million instruction trajectory pairs using fully-accessibleresources on the web. Importantly, we investigate the influence of eachcomponent in this paradigm on the agent's performance and study how toadequately apply the augmented data to pre-train and fine-tune an agent. Thanksto our large-scale dataset, the performance of an existing agent can be pushedup (+11% absolute with regard to previous SoTA) to a significantly new best of80% single-run success rate on the R2R test split by simple imitation learning.The long-lasting generalization gap between navigating in seen and unseenenvironments is also reduced to less than 1% (versus 8% in the previous bestmethod). Moreover, our paradigm also facilitates different models to achievenew state-of-the-art navigation results on CVDN, REVERIE, and R2R in continuousenvironments.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2307.15644v2 |
| 351 | Beating Backdoor Attack at Its Own Game                                                                                                              | Min Liu                        | 2023-07-28     | cs.LG, cs.CR, cs.CV                     | Deep neural networks (DNNs) are vulnerable to backdoor attack, which does notaffect the network's performance on clean data but would manipulate the networkbehavior once a trigger pattern is added. Existing defense methods have greatlyreduced attack success rate, but their prediction accuracy on clean data stilllags behind a clean model by a large margin. Inspired by the stealthiness andeffectiveness of backdoor attack, we propose a simple but highly effectivedefense framework which injects non-adversarial backdoors targeting poisonedsamples. Following the general steps in backdoor attack, we detect a small setof suspected samples and then apply a poisoning strategy to them. Thenon-adversarial backdoor, once triggered, suppresses the attacker's backdoor onpoisoned data, but has limited influence on clean data. The defense can becarried out during data preprocessing, without any modification to the standardend-to-end training pipeline. We conduct extensive experiments on multiplebenchmarks with different architectures and representative attacks. Resultsdemonstrate that our method achieves state-of-the-art defense effectivenesswith by far the lowest performance drop on clean data. Considering thesurprising defense ability displayed by our framework, we call for moreattention to utilizing backdoor for backdoor defense. Code is available athttps://github.com/damianliumin/non-adversarial_backdoor.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.15539v3 |
| 352 | Uncertainty-aware Unsupervised Multi-Object Tracking                                                                                                 | Kai Liu                        | 2023-07-28     | cs.CV                                   | Without manually annotated identities, unsupervised multi-object trackers areinferior to learning reliable feature embeddings. It causes thesimilarity-based inter-frame association stage also be error-prone, where anuncertainty problem arises. The frame-by-frame accumulated uncertainty preventstrackers from learning the consistent feature embedding against time variation.To avoid this uncertainty problem, recent self-supervised techniques areadopted, whereas they failed to capture temporal relations. The interframeuncertainty still exists. In fact, this paper argues that though theuncertainty problem is inevitable, it is possible to leverage the uncertaintyitself to improve the learned consistency in turn. Specifically, anuncertainty-based metric is developed to verify and rectify the riskyassociations. The resulting accurate pseudo-tracklets boost learning thefeature consistency. And accurate tracklets can incorporate temporalinformation into spatial transformation. This paper proposes a tracklet-guidedaugmentation strategy to simulate tracklets' motion, which adopts ahierarchical uncertainty-based sampling mechanism for hard sample mining. Theultimate unsupervised MOT framework, namely U2MOT, is proven effective onMOT-Challenges and VisDrone-MOT benchmark. U2MOT achieves a SOTA performanceamong the published supervised and unsupervised trackers.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.15409v1 |
| 353 | Supervised Homography Learning with Realistic Dataset Generation                                                                                     | Hai Jiang                      | 2023-07-28     | cs.CV                                   | In this paper, we propose an iterative framework, which consists of twophases: a generation phase and a training phase, to generate realistic trainingdata and yield a supervised homography network. In the generation phase, givenan unlabeled image pair, we utilize the pre-estimated dominant plane masks andhomography of the pair, along with another sampled homography that serves asground truth to generate a new labeled training pair with realistic motion. Inthe training phase, the generated data is used to train the supervisedhomography network, in which the training data is refined via a contentconsistency module and a quality assessment module. Once an iteration isfinished, the trained network is used in the next data generation phase toupdate the pre-estimated homography. Through such an iterative strategy, thequality of the dataset and the performance of the network can be gradually andsimultaneously improved. Experimental results show that our method achievesstate-of-the-art performance and existing supervised methods can be alsoimproved based on the generated dataset. Code and dataset are available athttps://github.com/JianghaiSCU/RealSH.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.15353v2 |
| 354 | TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts                                                       | Hanrong Ye                     | 2023-07-28     | cs.CV                                   | Learning discriminative task-specific features simultaneously for multipledistinct tasks is a fundamental problem in multi-task learning. Recentstate-of-the-art models consider directly decoding task-specific features fromone shared task-generic feature (e.g., feature from a backbone layer), andutilize carefully designed decoders to produce multi-task features. However, asthe input feature is fully shared and each task decoder also shares decodingparameters for different input samples, it leads to a static feature decodingprocess, producing less discriminative task-specific representations. To tacklethis limitation, we propose TaskExpert, a novel multi-task mixture-of-expertsmodel that enables learning multiple representative task-generic feature spacesand decoding task-specific features in a dynamic manner. Specifically,TaskExpert introduces a set of expert networks to decompose the backbonefeature into several representative task-generic features. Then, thetask-specific features are decoded by using dynamic task-specific gatingnetworks operating on the decomposed task-generic features. Furthermore, toestablish long-range modeling of the task-specific representations fromdifferent layers of TaskExpert, we design a multi-task feature memory thatupdates at each layer and acts as an additional feature expert for dynamictask-specific feature decoding. Extensive experiments demonstrate that ourTaskExpert clearly outperforms previous best-performing methods on all 9metrics of two competitive multi-task learning benchmarks for visual sceneunderstanding (i.e., PASCAL-Context and NYUD-v2). Codes and models will be madepublicly available at https://github.com/prismformore/Multi-Task-Transformer                                                                                                                                                                            | http://arxiv.org/abs/2307.15324v1 |
| 355 | PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization                                                                   | Junhyeong Cho                  | 2023-07-27     | cs.CV, cs.AI, cs.CL, cs.LG              | In a joint vision-language space, a text feature (e.g., from "a photo of adog") could effectively represent its relevant image features (e.g., from dogphotos). Also, a recent study has demonstrated the cross-modal transferabilityphenomenon of this joint space. From these observations, we proposePromptStyler which simulates various distribution shifts in the joint space bysynthesizing diverse styles via prompts without using any images to deal withsource-free domain generalization. The proposed method learns to generate avariety of style features (from "a S* style of a") via learnable style wordvectors for pseudo-words S*. To ensure that learned styles do not distortcontent information, we force style-content features (from "a S* style of a[class]") to be located nearby their corresponding content features (from"[class]") in the joint vision-language space. After learning style wordvectors, we train a linear classifier using synthesized style-content features.PromptStyler achieves the state of the art on PACS, VLCS, OfficeHome andDomainNet, even though it does not require any images for training.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.15199v2 |
| 356 | To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation                                                                             | Marc Botet Colomer             | 2023-07-27     | cs.CV                                   | The goal of Online Domain Adaptation for semantic segmentation is to handleunforeseeable domain changes that occur during deployment, like sudden weatherevents. However, the high computational costs associated with brute-forceadaptation make this paradigm unfeasible for real-world applications. In thispaper we propose HAMLET, a Hardware-Aware Modular Least Expensive Trainingframework for real-time domain adaptation. Our approach includes ahardware-aware back-propagation orchestration agent (HAMT) and a dedicateddomain-shift detector that enables active control over when and how the modelis adapted (LT). Thanks to these advancements, our approach is capable ofperforming semantic segmentation while simultaneously adapting at more than29FPS on a single consumer-grade GPU. Our framework's encouraging accuracy andspeed trade-off is demonstrated on OnDA and SHIFT benchmarks throughexperimental results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.15063v2 |
| 357 | Learning Depth Estimation for Transparent and Mirror Surfaces                                                                                        | Alex Costanzino                | 2023-07-27     | cs.CV                                   | Inferring the depth of transparent or mirror (ToM) surfaces represents a hardchallenge for either sensors, algorithms, or deep networks. We propose a simplepipeline for learning to estimate depth properly for such surfaces with neuralnetworks, without requiring any ground-truth annotation. We unveil how toobtain reliable pseudo labels by in-painting ToM objects in images andprocessing them with a monocular depth estimation model. These labels can beused to fine-tune existing monocular or stereo networks, to let them learn howto deal with ToM surfaces. Experimental results on the Booster dataset show thedramatic improvements enabled by our remarkably simple proposal.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.15052v1 |
| 358 | Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models                                                           | Kecheng Zheng                  | 2023-07-27     | cs.CV                                   | Prompt tuning and adapter tuning have shown great potential in transferringpre-trained vision-language models (VLMs) to various downstream tasks. In thiswork, we design a new type of tuning method, termed as regularized mask tuning,which masks the network parameters through a learnable selection. Inspired byneural pathways, we argue that the knowledge required by a downstream taskalready exists in the pre-trained weights but just gets concealed in theupstream pre-training stage. To bring the useful knowledge back into light, wefirst identify a set of parameters that are important to a given downstreamtask, then attach a binary mask to each parameter, and finally optimize thesemasks on the downstream data with the parameters frozen. When updating themask, we introduce a novel gradient dropout strategy to regularize theparameter selection, in order to prevent the model from forgetting oldknowledge and overfitting the downstream data. Experimental results on 11datasets demonstrate the consistent superiority of our method over previousalternatives. It is noteworthy that we manage to deliver 18.73% performanceimprovement compared to the zero-shot CLIP via masking an average of only 2.56%parameters. Furthermore, our method is synergistic with most existingparameter-efficient tuning methods and can boost the performance on top ofthem. Project page can be found here (https://wuw2019.github.io/R-AMT/).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.15049v2 |
| 359 | Diverse Inpainting and Editing with GAN Inversion                                                                                                    | Ahmet Burak Yildirim           | 2023-07-27     | cs.CV                                   | Recent inversion methods have shown that real images can be inverted intoStyleGAN's latent space and numerous edits can be achieved on those imagesthanks to the semantically rich feature representations of well-trained GANmodels. However, extensive research has also shown that image inversion ischallenging due to the trade-off between high-fidelity reconstruction andeditability. In this paper, we tackle an even more difficult task, invertingerased images into GAN's latent space for realistic inpaintings and editings.Furthermore, by augmenting inverted latent codes with different latent samples,we achieve diverse inpaintings. Specifically, we propose to learn an encoderand mixing network to combine encoded features from erased images withStyleGAN's mapped features from random samples. To encourage the mixing networkto utilize both inputs, we train the networks with generated data via a novelset-up. We also utilize higher-rate features to prevent color inconsistenciesbetween the inpainted and unerased parts. We run extensive experiments andcompare our method with state-of-the-art inversion and inpainting methods.Qualitative metrics and visual comparisons show significant improvements.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2307.15033v1 |
| 360 | Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models                                                                                 | Ziyi Wang                      | 2023-07-27     | cs.CV, cs.AI, cs.LG                     | With the overwhelming trend of mask image modeling led by MAE, generativepre-training has shown a remarkable potential to boost the performance offundamental models in 2D vision. However, in 3D vision, the over-reliance onTransformer-based backbones and the unordered nature of point clouds haverestricted the further development of generative pre-training. In this paper,we propose a novel 3D-to-2D generative pre-training method that is adaptable toany point cloud model. We propose to generate view images from differentinstructed poses via the cross-attention mechanism as the pre-training scheme.Generating view images has more precise supervision than its point cloudcounterpart, thus assisting 3D backbones to have a finer comprehension of thegeometrical structure and stereoscopic relations of the point cloud.Experimental results have proved the superiority of our proposed 3D-to-2Dgenerative pre-training over previous pre-training methods. Our method is alsoeffective in boosting the performance of architecture-oriented approaches,achieving state-of-the-art performance when fine-tuning on ScanObjectNNclassification and ShapeNetPart segmentation tasks. Code is available athttps://github.com/wangzy22/TAP.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.14971v1 |
| 361 | Towards Deeply Unified Depth-aware Panoptic Segmentation with Bi-directional Guidance Learning                                                       | Junwen He                      | 2023-07-27     | cs.CV                                   | Depth-aware panoptic segmentation is an emerging topic in computer visionwhich combines semantic and geometric understanding for more robust sceneinterpretation. Recent works pursue unified frameworks to tackle this challengebut mostly still treat it as two individual learning tasks, which limits theirpotential for exploring cross-domain information. We propose a deeply unifiedframework for depth-aware panoptic segmentation, which performs jointsegmentation and depth estimation both in a per-segment manner with identicalobject queries. To narrow the gap between the two tasks, we further design ageometric query enhancement method, which is able to integrate scene geometryinto object queries using latent representations. In addition, we propose abi-directional guidance learning approach to facilitate cross-task featurelearning by taking advantage of their mutual relations. Our method sets the newstate of the art for depth-aware panoptic segmentation on both Cityscapes-DVPSand SemKITTI-DVPS datasets. Moreover, our guidance learning approach is shownto deliver performance improvement even under incomplete supervision labels.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.14786v2 |
| 362 | Gloss-free Sign Language Translation: Improving from Visual-Language Pretraining                                                                     | Benjia Zhou                    | 2023-07-27     | cs.CV                                   | Sign Language Translation (SLT) is a challenging task due to its cross-domainnature, involving the translation of visual-gestural language to text. Manyprevious methods employ an intermediate representation, i.e., gloss sequences,to facilitate SLT, thus transforming it into a two-stage task of sign languagerecognition (SLR) followed by sign language translation (SLT). However, thescarcity of gloss-annotated sign language data, combined with the informationbottleneck in the mid-level gloss representation, has hindered the furtherdevelopment of the SLT task. To address this challenge, we propose a novelGloss-Free SLT based on Visual-Language Pretraining (GFSLT-VLP), which improvesSLT by inheriting language-oriented prior knowledge from pre-trained models,without any gloss annotation assistance. Our approach involves two stages: (i)integrating Contrastive Language-Image Pre-training (CLIP) with maskedself-supervised learning to create pre-tasks that bridge the semantic gapbetween visual and textual representations and restore masked sentences, and(ii) constructing an end-to-end architecture with an encoder-decoder-likestructure that inherits the parameters of the pre-trained Visual Encoder andText Decoder from the first stage. The seamless combination of these noveldesigns forms a robust sign language representation and significantly improvesgloss-free sign language translation. In particular, we have achievedunprecedented improvements in terms of BLEU-4 score on the PHOENIX14T dataset(>+5) and the CSL-Daily dataset (>+3) compared to state-of-the-art gloss-freeSLT methods. Furthermore, our approach also achieves competitive results on thePHOENIX14T dataset when compared with most of the gloss-based methods. Our codeis available at https://github.com/zhoubenjia/GFSLT-VLP.                                                                                        | http://arxiv.org/abs/2307.14768v1 |
| 363 | Test Time Adaptation for Blind Image Quality Assessment                                                                                              | Subhadeep Roy                  | 2023-07-27     | cs.CV, eess.IV                          | While the design of blind image quality assessment (IQA) algorithms hasimproved significantly, the distribution shift between the training and testingscenarios often leads to a poor performance of these methods at inference time.This motivates the study of test time adaptation (TTA) techniques to improvetheir performance at inference time. Existing auxiliary tasks and lossfunctions used for TTA may not be relevant for quality-aware adaptation of thepre-trained model. In this work, we introduce two novel quality-relevantauxiliary tasks at the batch and sample levels to enable TTA for blind IQA. Inparticular, we introduce a group contrastive loss at the batch level and arelative rank loss at the sample level to make the model quality aware andadapt to the target data. Our experiments reveal that even using a small batchof images from the test distribution helps achieve significant improvement inperformance by updating the batch normalization statistics of the source model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.14735v1 |
| 364 | P2C: Self-Supervised Point Cloud Completion from Single Partial Clouds                                                                               | Ruikai Cui                     | 2023-07-27     | cs.CV, cs.GR                            | Point cloud completion aims to recover the complete shape based on a partialobservation. Existing methods require either complete point clouds or multiplepartial observations of the same object for learning. In contrast to previousapproaches, we present Partial2Complete (P2C), the first self-supervisedframework that completes point cloud objects using training samples consistingof only a single incomplete point cloud per object. Specifically, our frameworkgroups incomplete point clouds into local patches as input and predicts maskedpatches by learning prior information from different partial objects. We alsopropose Region-Aware Chamfer Distance to regularize shape mismatch withoutlimiting completion capability, and devise the Normal Consistency Constraint toincorporate a local planarity assumption, encouraging the recovered shapesurface to be continuous and complete. In this way, P2C no longer needsmultiple observations or complete point clouds as ground truth. Instead,structural cues are learned from a category-specific dataset to completepartial point clouds of objects. We demonstrate the effectiveness of ourapproach on both synthetic ShapeNet data and real-world ScanNet data, showingthat P2C produces comparable results to methods trained with complete shapes,and outperforms methods learned with multiple partial observations. Code isavailable at https://github.com/CuiRuikai/Partial2Complete.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.14726v1 |
| 365 | Pre-training Vision Transformers with Very Limited Synthesized Images                                                                                | Ryo Nakamura                   | 2023-07-27     | cs.CV                                   | Formula-driven supervised learning (FDSL) is a pre-training method thatrelies on synthetic images generated from mathematical formulae such asfractals. Prior work on FDSL has shown that pre-training vision transformers onsuch synthetic datasets can yield competitive accuracy on a wide range ofdownstream tasks. These synthetic images are categorized according to theparameters in the mathematical formula that generate them. In the present work,we hypothesize that the process for generating different instances for the samecategory in FDSL, can be viewed as a form of data augmentation. We validatethis hypothesis by replacing the instances with data augmentation, which meanswe only need a single image per category. Our experiments shows that thisone-instance fractal database (OFDB) performs better than the original datasetwhere instances were explicitly generated. We further scale up OFDB to 21,000categories and show that it matches, or even surpasses, the model pre-trainedon ImageNet-21k in ImageNet-1k fine-tuning. The number of images in OFDB is21k, whereas ImageNet-21k has 14M. This opens new possibilities forpre-training vision transformers with much smaller datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2307.14710v2 |
| 366 | Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging via Optimization Trajectory Distillation                                                | Jianan Fan                     | 2023-07-27     | cs.CV                                   | The success of automated medical image analysis depends on large-scale andexpert-annotated training sets. Unsupervised domain adaptation (UDA) has beenraised as a promising approach to alleviate the burden of labeled datacollection. However, they generally operate under the closed-set adaptationsetting assuming an identical label set between the source and target domains,which is over-restrictive in clinical practice where new classes commonly existacross datasets due to taxonomic inconsistency. While several methods have beenpresented to tackle both domain shifts and incoherent label sets, none of themtake into account the common characteristics of the two issues and consider thelearning dynamics along network training. In this work, we propose optimizationtrajectory distillation, a unified approach to address the two technicalchallenges from a new perspective. It exploits the low-rank nature of gradientspace and devises a dual-stream distillation algorithm to regularize thelearning dynamics of insufficiently annotated domain and classes with theexternal guidance obtained from reliable sources. Our approach resolves theissue of inadequate navigation along network optimization, which is the majorobstacle in the taxonomy adaptive cross-domain adaptation scenario. We evaluatethe proposed method extensively on several tasks towards various endpoints withclinical and open-world significance. The results demonstrate its effectivenessand improvements over previous methods.                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2307.14709v1 |
| 367 | 360VOT: A New Benchmark Dataset for Omnidirectional Visual Object Tracking                                                                           | Huajian Huang                  | 2023-07-27     | cs.CV                                   | 360{\deg} images can provide an omnidirectional field of view which isimportant for stable and long-term scene perception. In this paper, we explore360{\deg} images for visual object tracking and perceive new challenges causedby large distortion, stitching artifacts, and other unique attributes of360{\deg} images. To alleviate these problems, we take advantage of novelrepresentations of target localization, i.e., bounding field-of-view, and thenintroduce a general 360 tracking framework that can adopt typical trackers foromnidirectional tracking. More importantly, we propose a new large-scaleomnidirectional tracking benchmark dataset, 360VOT, in order to facilitatefuture research. 360VOT contains 120 sequences with up to 113K high-resolutionframes in equirectangular projection. The tracking targets cover 32 categoriesin diverse scenarios. Moreover, we provide 4 types of unbiased ground truth,including (rotated) bounding boxes and (rotated) bounding field-of-views, aswell as new metrics tailored for 360{\deg} images which allow for the accurateevaluation of omnidirectional tracking performance. Finally, we extensivelyevaluated 20 state-of-the-art visual trackers and provided a new baseline forfuture comparisons. Homepage: https://360vot.hkustvgd.com                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2307.14630v1 |
| 368 | NeRF-Det: Learning Geometry-Aware Volumetric Representation for Multi-View 3D Object Detection                                                       | Chenfeng Xu                    | 2023-07-27     | cs.CV                                   | We present NeRF-Det, a novel method for indoor 3D detection with posed RGBimages as input. Unlike existing indoor 3D detection methods that struggle tomodel scene geometry, our method makes novel use of NeRF in an end-to-endmanner to explicitly estimate 3D geometry, thereby improving 3D detectionperformance. Specifically, to avoid the significant extra latency associatedwith per-scene optimization of NeRF, we introduce sufficient geometry priors toenhance the generalizability of NeRF-MLP. Furthermore, we subtly connect thedetection and NeRF branches through a shared MLP, enabling an efficientadaptation of NeRF to detection and yielding geometry-aware volumetricrepresentations for 3D detection. Our method outperforms state-of-the-arts by3.9 mAP and 3.1 mAP on the ScanNet and ARKITScenes benchmarks, respectively. Weprovide extensive analysis to shed light on how NeRF-Det works. As a result ofour joint-training design, NeRF-Det is able to generalize well to unseen scenesfor object detection, view synthesis, and depth estimation tasks withoutrequiring per-scene optimization. Code is available at\url{https://github.com/facebookresearch/NeRF-Det}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.14620v1 |
| 369 | TextManiA: Enriching Visual Feature by Text-driven Manifold Augmentation                                                                             | Moon Ye-Bin                    | 2023-07-27     | cs.CV                                   | Recent label mix-based augmentation methods have shown their effectiveness ingeneralization despite their simplicity, and their favorable effects are oftenattributed to semantic-level augmentation. However, we found that they arevulnerable to highly skewed class distribution, because scarce data classes arerarely sampled for inter-class perturbation. We propose TextManiA, atext-driven manifold augmentation method that semantically enriches visualfeature spaces, regardless of data distribution. TextManiA augments visual datawith intra-class semantic perturbation by exploiting easy-to-understandvisually mimetic words, i.e., attributes. To this end, we bridge between thetext representation and a target visual feature space, and propose an efficientvector augmentation. To empirically support the validity of our design, wedevise two visualization-based analyses and show the plausibility of the bridgebetween two different modality spaces. Our experiments demonstrate thatTextManiA is powerful in scarce samples with class imbalance as well as evendistribution. We also show compatibility with the label mix-based approaches inevenly distributed scarce data.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.14611v2 |
| 370 | Clustering based Point Cloud Representation Learning for 3D Analysis                                                                                 | Tuo Feng                       | 2023-07-27     | cs.CV, cs.AI                            | Point cloud analysis (such as 3D segmentation and detection) is a challengingtask, because of not only the irregular geometries of many millions ofunordered points, but also the great variations caused by depth, viewpoint,occlusion, etc. Current studies put much focus on the adaption of neuralnetworks to the complex geometries of point clouds, but are blind to afundamental question: how to learn an appropriate point embedding space that isaware of both discriminative semantics and challenging variations? As aresponse, we propose a clustering based supervised learning scheme for pointcloud analysis. Unlike current de-facto, scene-wise training paradigm, ouralgorithm conducts within-class clustering on the point embedding space forautomatically discovering subclass patterns which are latent yet representativeacross scenes. The mined patterns are, in turn, used to repaint the embeddingspace, so as to respect the underlying distribution of the entire trainingdataset and improve the robustness to the variations. Our algorithm isprincipled and readily pluggable to modern point cloud segmentation networksduring training, without extra overhead during testing. With various 3D networkarchitectures (i.e., voxel-based, point-based, Transformer-based, automaticallysearched), our algorithm shows notable improvements on famous point cloudsegmentation datasets (i.e.,2.0-2.6% on single-scan and 2.0-2.2% multi-scan ofSemanticKITTI, 1.8-1.9% on S3DIS, in terms of mIoU). Our algorithm alsodemonstrates utility in 3D detection, showing 2.0-3.4% mAP gains on KITTI.                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2307.14605v1 |
| 371 | MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation                                                                           | Rajeev Yasarla                 | 2023-07-26     | cs.CV                                   | We propose MAMo, a novel memory and attention frame-work for monocular videodepth estimation. MAMo can augment and improve any single-image depthestimation networks into video depth estimation models, enabling them to takeadvantage of the temporal information to predict more accurate depth. In MAMo,we augment model with memory which aids the depth prediction as the modelstreams through the video. Specifically, the memory stores learned visual anddisplacement tokens of the previous time instances. This allows the depthnetwork to cross-reference relevant features from the past when predictingdepth on the current frame. We introduce a novel scheme to continuously updatethe memory, optimizing it to keep tokens that correspond with both the past andthe present visual information. We adopt attention-based approach to processmemory features where we first learn the spatio-temporal relation among theresultant visual and displacement memory tokens using self-attention module.Further, the output features of self-attention are aggregated with the currentvisual features through cross-attention. The cross-attended features arefinally given to a decoder to predict depth on the current frame. Throughextensive experiments on several benchmarks, including KITTI, NYU-Depth V2, andDDAD, we show that MAMo consistently improves monocular depth estimationnetworks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo videodepth estimation provides higher accuracy with lower latency, when omparing toSOTA cost-volume-based video depth models.                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.14336v1 |
| 372 | ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation                                                                                   | Görkay Aydemir                 | 2023-07-26     | cs.CV, cs.RO                            | Forecasting future trajectories of agents in complex traffic scenes requiresreliable and efficient predictions for all agents in the scene. However,existing methods for trajectory prediction are either inefficient or sacrificeaccuracy. To address this challenge, we propose ADAPT, a novel approach forjointly predicting the trajectories of all agents in the scene with dynamicweight learning. Our approach outperforms state-of-the-art methods in bothsingle-agent and multi-agent settings on the Argoverse and Interactiondatasets, with a fraction of their computational overhead. We attribute theimprovement in our performance: first, to the adaptive head augmenting themodel capacity without increasing the model size; second, to our design choicesin the endpoint-conditioned prediction, reinforced by gradient stopping. Ouranalyses show that ADAPT can focus on each agent with adaptive prediction,allowing for accurate predictions efficiently. https://KUIS-AI.github.io/adapt                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2307.14187v1 |
| 373 | Creative Birds: Self-Supervised Single-View 3D Style Transfer                                                                                        | Renke Wang                     | 2023-07-26     | cs.CV                                   | In this paper, we propose a novel method for single-view 3D style transferthat generates a unique 3D object with both shape and texture transfer. Ourfocus lies primarily on birds, a popular subject in 3D reconstruction, forwhich no existing single-view 3D transfer methods have been developed.Themethod we propose seeks to generate a 3D mesh shape and texture of a bird fromtwo single-view images. To achieve this, we introduce a novel shape transfergenerator that comprises a dual residual gated network (DRGNet), and amulti-layer perceptron (MLP). DRGNet extracts the features of source and targetimages using a shared coordinate gate unit, while the MLP generates spatialcoordinates for building a 3D mesh. We also introduce a semantic UV texturetransfer module that implements textural style transfer using semantic UVsegmentation, which ensures consistency in the semantic meaning of thetransferred regions. This module can be widely adapted to many existingapproaches. Finally, our method constructs a novel 3D bird using adifferentiable renderer. Experimental results on the CUB dataset verify thatour method achieves state-of-the-art performance on the single-view 3D styletransfer task. Code is available in https://github.com/wrk226/creative_birds.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2307.14127v2 |
| 374 | Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models                                               | Dong Lu                        | 2023-07-26     | cs.CV                                   | Vision-language pre-training (VLP) models have shown vulnerability toadversarial examples in multimodal tasks. Furthermore, malicious adversariescan be deliberately transferred to attack other black-box models. However,existing work has mainly focused on investigating white-box attacks. In thispaper, we present the first study to investigate the adversarialtransferability of recent VLP models. We observe that existing methods exhibitmuch lower transferability, compared to the strong attack performance inwhite-box settings. The transferability degradation is partly caused by theunder-utilization of cross-modal interactions. Particularly, unlike unimodallearning, VLP models rely heavily on cross-modal interactions and themultimodal alignments are many-to-many, e.g., an image can be described invarious natural languages. To this end, we propose a highly transferableSet-level Guidance Attack (SGA) that thoroughly leverages modality interactionsand incorporates alignment-preserving augmentation with cross-modal guidance.Experimental results demonstrate that SGA could generate adversarial examplesthat can strongly transfer across different VLP models on multiple downstreamvision-language tasks. On image-text retrieval, SGA significantly enhances theattack success rate for transfer attacks from ALBEF to TCL by a large margin(at least 9.78% and up to 30.21%), compared to the state-of-the-art.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.14061v1 |
| 375 | 3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability                                                         | Ruowei Wang                    | 2023-07-26     | cs.CV                                   | Shape generation is the practice of producing 3D shapes as variousrepresentations for 3D content creation. Previous studies on 3D shapegeneration have focused on shape quality and structure, without or lessconsidering the importance of semantic information. Consequently, suchgenerative models often fail to preserve the semantic consistency of shapestructure or enable manipulation of the semantic attributes of shapes duringgeneration. In this paper, we proposed a novel semantic generative model named3D Semantic Subspace Traverser that utilizes semantic attributes forcategory-specific 3D shape generation and editing. Our method utilizes implicitfunctions as the 3D shape representation and combines a novel latent-space GANwith a linear subspace model to discover semantic dimensions in the locallatent space of 3D shapes. Each dimension of the subspace corresponds to aparticular semantic attribute, and we can edit the attributes of generatedshapes by traversing the coefficients of those dimensions. Experimental resultsdemonstrate that our method can produce plausible shapes with complexstructures and enable the editing of semantic attributes. The code and trainedmodels are available athttps://github.com/TrepangCat/3D_Semantic_Subspace_Traverser                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.14051v4 |
| 376 | Controllable Guide-Space for Generalizable Face Forgery Detection                                                                                    | Ying Guo                       | 2023-07-26     | cs.CV                                   | Recent studies on face forgery detection have shown satisfactory performancefor methods involved in training datasets, but are not ideal enough for unknowndomains. This motivates many works to improve the generalization, butforgery-irrelevant information, such as image background and identity, stillexists in different domain features and causes unexpected clustering, limitingthe generalization. In this paper, we propose a controllable guide-space (GS)method to enhance the discrimination of different forgery domains, so as toincrease the forgery relevance of features and thereby improve thegeneralization. The well-designed guide-space can simultaneously achieve boththe proper separation of forgery domains and the large distance betweenreal-forgery domains in an explicit and controllable manner. Moreover, forbetter discrimination, we use a decoupling module to weaken the interference offorgery-irrelevant correlations between domains. Furthermore, we makeadjustments to the decision boundary manifold according to the clusteringdegree of the same domain features within the neighborhood. Extensiveexperiments in multiple in-domain and cross-domain settings confirm that ourmethod can achieve state-of-the-art generalization.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2307.14039v1 |
| 377 | AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception                                                | Dingkang Yang                  | 2023-07-26     | cs.CV                                   | Driver distraction has become a significant cause of severe traffic accidentsover the past decade. Despite the growing development of vision-driven drivermonitoring systems, the lack of comprehensive perception datasets restrictsroad safety and traffic security. In this paper, we present an AssIstiveDriving pErception dataset (AIDE) that considers context information bothinside and outside the vehicle in naturalistic scenarios. AIDE facilitatesholistic driver monitoring through three distinctive characteristics, includingmulti-view settings of driver and scene, multi-modal annotations of face, body,posture, and gesture, and four pragmatic task designs for drivingunderstanding. To thoroughly explore AIDE, we provide experimental benchmarkson three kinds of baseline frameworks via extensive methods. Moreover, twofusion strategies are introduced to give new insights into learning effectivemulti-stream/modal representations. We also systematically investigate theimportance and rationality of the key components in AIDE and benchmarks. Theproject link is https://github.com/ydk122024/AIDE.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.13933v2 |
| 378 | Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception                                                                            | Kun Yang                       | 2023-07-26     | cs.CV                                   | Multi-agent collaborative perception as a potential application forvehicle-to-everything communication could significantly improve the perceptionperformance of autonomous vehicles over single-agent perception. However,several challenges remain in achieving pragmatic information sharing in thisemerging research. In this paper, we propose SCOPE, a novel collaborativeperception framework that aggregates the spatio-temporal awarenesscharacteristics across on-road agents in an end-to-end manner. Specifically,SCOPE has three distinct strengths: i) it considers effective semantic cues ofthe temporal context to enhance current representations of the target agent;ii) it aggregates perceptually critical spatial information from heterogeneousagents and overcomes localization errors via multi-scale feature interactions;iii) it integrates multi-source representations of the target agent based ontheir complementary contributions by an adaptive fusion paradigm. To thoroughlyevaluate SCOPE, we consider both real-world and simulated scenarios ofcollaborative 3D object detection tasks on three datasets. Extensiveexperiments demonstrate the superiority of our approach and the necessity ofthe proposed components.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.13929v2 |
| 379 | PlaneRecTR: Unified Query Learning for 3D Plane Recovery from a Single View                                                                          | Jingjia Shi                    | 2023-07-25     | cs.CV                                   | 3D plane recovery from a single image can usually be divided into severalsubtasks of plane detection, segmentation, parameter estimation and possiblydepth estimation. Previous works tend to solve this task by either extendingthe RCNN-based segmentation network or the dense pixel embedding-basedclustering framework. However, none of them tried to integrate above relatedsubtasks into a unified framework but treat them separately and sequentially,which we suspect is potentially a main source of performance limitation forexisting approaches. Motivated by this finding and the success of query-basedlearning in enriching reasoning among semantic entities, in this paper, wepropose PlaneRecTR, a Transformer-based architecture, which for the first timeunifies all subtasks related to single-view plane recovery with a singlecompact model. Extensive quantitative and qualitative experiments demonstratethat our proposed unified learning achieves mutual benefits across subtasks,obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Planedatasets. Codes are available at https://github.com/SJingjia/PlaneRecTR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.13756v2 |
| 380 | RecursiveDet: End-to-End Region-based Recursive Object Detection                                                                                     | Jing Zhao                      | 2023-07-25     | cs.CV                                   | End-to-end region-based object detectors like Sparse R-CNN usually havemultiple cascade bounding box decoding stages, which refine the currentpredictions according to their previous results. Model parameters within eachstage are independent, evolving a huge cost. In this paper, we find the generalsetting of decoding stages is actually redundant. By simply sharing parametersand making a recursive decoder, the detector already obtains a significantimprovement. The recursive decoder can be further enhanced by positionalencoding (PE) of the proposal box, which makes it aware of the exact locationsand sizes of input bounding boxes, thus becoming adaptive to proposals fromdifferent stages during the recursion. Moreover, we also designcenterness-based PE to distinguish the RoI feature element and dynamicconvolution kernels at different positions within the bounding box. To validatethe effectiveness of the proposed method, we conduct intensive ablations andbuild the full model on three recent mainstream region-based detectors. TheRecusiveDet is able to achieve obvious performance boosts with even fewer modelparameters and slightly increased computation cost. Codes are available athttps://github.com/bravezzzzzz/RecursiveDet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2307.13619v1 |
| 381 | Model Calibration in Dense Classification with Adaptive Label Perturbation                                                                           | Jiawei Liu                     | 2023-07-25     | cs.CV, cs.LG                            | For safety-related applications, it is crucial to produce trustworthy deepneural networks whose prediction is associated with confidence that canrepresent the likelihood of correctness for subsequent decision-making.Existing dense binary classification models are prone to being over-confident.To improve model calibration, we propose Adaptive Stochastic Label Perturbation(ASLP) which learns a unique label perturbation level for each training image.ASLP employs our proposed Self-Calibrating Binary Cross Entropy (SC-BCE) loss,which unifies label perturbation processes including stochastic approaches(like DisturbLabel), and label smoothing, to correct calibration whilemaintaining classification rates. ASLP follows Maximum Entropy Inference ofclassic statistical mechanics to maximise prediction entropy with respect tomissing information. It performs this while: (1) preserving classificationaccuracy on known data as a conservative solution, or (2) specifically improvesmodel calibration degree by minimising the gap between the prediction accuracyand expected confidence of the target training label. Extensive resultsdemonstrate that ASLP can significantly improve calibration degrees of densebinary classification models on both in-distribution and out-of-distributiondata. The code is available on https://github.com/Carlisle-Liu/ASLP.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.13539v2 |
| 382 | Spectrum-guided Multi-granularity Referring Video Object Segmentation                                                                                | Bo Miao                        | 2023-07-25     | cs.CV, cs.AI, cs.MM                     | Current referring video object segmentation (R-VOS) techniques extractconditional kernels from encoded (low-resolution) vision-language features tosegment the decoded high-resolution features. We discovered that this causessignificant feature drift, which the segmentation kernels struggle to perceiveduring the forward computation. This negatively affects the ability ofsegmentation kernels. To address the drift problem, we propose aSpectrum-guided Multi-granularity (SgMg) approach, which performs directsegmentation on the encoded features and employs visual details to furtheroptimize the masks. In addition, we propose Spectrum-guided Cross-modal Fusion(SCF) to perform intra-frame global interactions in the spectral domain foreffective multimodal representation. Finally, we extend SgMg to performmulti-object R-VOS, a new paradigm that enables simultaneous segmentation ofmultiple referred objects in a video. This not only makes R-VOS faster, butalso more practical. Extensive experiments show that SgMg achievesstate-of-the-art performance on four video benchmark datasets, outperformingthe nearest competitor by 2.8% points on Ref-YouTube-VOS. Our extended SgMgenables multi-object R-VOS, runs about 3 times faster while maintainingsatisfactory performance. Code is available at https://github.com/bo-miao/SgMg.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.13537v1 |
| 383 | Weakly-supervised 3D Pose Transfer with Keypoints                                                                                                    | Jinnan Chen                    | 2023-07-25     | cs.CV                                   | The main challenges of 3D pose transfer are: 1) Lack of paired training datawith different characters performing the same pose; 2) Disentangling pose andshape information from the target mesh; 3) Difficulty in applying to mesheswith different topologies. We thus propose a novel weakly-supervisedkeypoint-based framework to overcome these difficulties. Specifically, we use atopology-agnostic keypoint detector with inverse kinematics to computetransformations between the source and target meshes. Our method only requiressupervision on the keypoints, can be applied to meshes with differenttopologies and is shape-invariant for the target which allows extraction ofpose-only information from the target meshes without transferring shapeinformation. We further design a cycle reconstruction to performself-supervised pose transfer without the need for ground truth deformed meshwith the same pose and shape as the target and source, respectively. Weevaluate our approach on benchmark human and animal datasets, where we achievesuperior performance compared to the state-of-the-art unsupervised approachesand even comparable performance with the fully supervised approaches. We teston the more challenging Mixamo dataset to verify our approach's ability inhandling meshes with different topologies and complex clothes. Cross-datasetevaluation further shows the strong generalization ability of our approach.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.13459v2 |
| 384 | Unmasking Anomalies in Road-Scene Segmentation                                                                                                       | Shyam Nandan Rai               | 2023-07-25     | cs.CV, I.4.6                            | Anomaly segmentation is a critical task for driving applications, and it isapproached traditionally as a per-pixel classification problem. However,reasoning individually about each pixel without considering their contextualsemantics results in high uncertainty around the objects' boundaries andnumerous false positives. We propose a paradigm change by shifting from aper-pixel classification to a mask classification. Our mask-based method,Mask2Anomaly, demonstrates the feasibility of integrating an anomaly detectionmethod in a mask-classification architecture. Mask2Anomaly includes severaltechnical novelties that are designed to improve the detection of anomalies inmasks: i) a global masked attention module to focus individually on theforeground and background regions; ii) a mask contrastive learning thatmaximizes the margin between an anomaly and known classes; and iii) a maskrefinement solution to reduce false positives. Mask2Anomaly achieves newstate-of-the-art results across a range of benchmarks, both in the per-pixeland component-level evaluations. In particular, Mask2Anomaly reduces theaverage false positives rate by 60% wrt the previous state-of-the-art. Githubpage:https://github.com/shyam671/Mask2Anomaly-Unmasking-Anomalies-in-Road-Scene-Segmentation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.13316v1 |
| 385 | Conditional Cross Attention Network for Multi-Space Embedding without Entanglement in Only a SINGLE Network                                          | Chull Hwan Song                | 2023-07-25     | cs.CV                                   | Many studies in vision tasks have aimed to create effective embedding spacesfor single-label object prediction within an image. However, in reality, mostobjects possess multiple specific attributes, such as shape, color, and length,with each attribute composed of various classes. To apply models in real-worldscenarios, it is essential to be able to distinguish between the granularcomponents of an object. Conventional approaches to embedding multiple specificattributes into a single network often result in entanglement, wherefine-grained features of each attribute cannot be identified separately. Toaddress this problem, we propose a Conditional Cross-Attention Network thatinduces disentangled multi-space embeddings for various specific attributeswith only a single backbone. Firstly, we employ a cross-attention mechanism tofuse and switch the information of conditions (specific attributes), and wedemonstrate its effectiveness through a diverse visualization example.Secondly, we leverage the vision transformer for the first time to afine-grained image retrieval task and present a simple yet effective frameworkcompared to existing methods. Unlike previous studies where performance varieddepending on the benchmark dataset, our proposed method achieved consistentstate-of-the-art performance on the FashionAI, DARN, DeepFashion, and Zappos50Kbenchmark datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.13254v1 |
| 386 | GaPro: Box-Supervised 3D Point Cloud Instance Segmentation Using Gaussian Processes as Pseudo Labelers                                               | Tuan Duc Ngo                   | 2023-07-25     | cs.CV, cs.AI                            | Instance segmentation on 3D point clouds (3DIS) is a longstanding challengein computer vision, where state-of-the-art methods are mainly based on fullsupervision. As annotating ground truth dense instance masks is tedious andexpensive, solving 3DIS with weak supervision has become more practical. Inthis paper, we propose GaPro, a new instance segmentation for 3D point cloudsusing axis-aligned 3D bounding box supervision. Our two-step approach involvesgenerating pseudo labels from box annotations and training a 3DIS network withthe resulting labels. Additionally, we employ the self-training strategy toimprove the performance of our method further. We devise an effective GaussianProcess to generate pseudo instance masks from the bounding boxes and resolveambiguities when they overlap, resulting in pseudo instance masks with theiruncertainty values. Our experiments show that GaPro outperforms previous weaklysupervised 3D instance segmentation methods and has competitive performancecompared to state-of-the-art fully supervised ones. Furthermore, we demonstratethe robustness of our approach, where we can adapt various state-of-the-artfully supervised methods to the weak supervision task by using our pseudolabels for training. The source code and trained models are available athttps://github.com/VinAIResearch/GaPro.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2307.13251v1 |
| 387 | GridMM: Grid Memory Map for Vision-and-Language Navigation                                                                                           | Zihan Wang                     | 2023-07-24     | cs.CV, cs.AI                            | Vision-and-language navigation (VLN) enables the agent to navigate to aremote location following the natural language instruction in 3D environments.To represent the previously visited environment, most approaches for VLNimplement memory using recurrent states, topological maps, or top-down semanticmaps. In contrast to these approaches, we build the top-down egocentric anddynamically growing Grid Memory Map (i.e., GridMM) to structure the visitedenvironment. From a global perspective, historical observations are projectedinto a unified grid map in a top-down view, which can better represent thespatial relations of the environment. From a local perspective, we furtherpropose an instruction relevance aggregation method to capture fine-grainedvisual clues in each grid region. Extensive experiments are conducted on boththe REVERIE, R2R, SOON datasets in the discrete environments, and the R2R-CEdataset in the continuous environments, showing the superiority of our proposedmethod.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.12907v4 |
| 388 | Persistent-Transient Duality: A Multi-mechanism Approach for Modeling Human-Object Interaction                                                       | Hung Tran                      | 2023-07-24     | cs.CV                                   | Humans are highly adaptable, swiftly switching between different modes toprogressively handle different tasks, situations and contexts. In Human-objectinteraction (HOI) activities, these modes can be attributed to two mechanisms:(1) the large-scale consistent plan for the whole activity and (2) thesmall-scale children interactive actions that start and end along the timeline.While neuroscience and cognitive science have confirmed this multi-mechanismnature of human behavior, machine modeling approaches for human motion aretrailing behind. While attempted to use gradually morphing structures (e.g.,graph attention networks) to model the dynamic HOI patterns, they miss theexpeditious and discrete mode-switching nature of the human motion. To bridgethat gap, this work proposes to model two concurrent mechanisms that jointlycontrol human motion: the Persistent process that runs continually on theglobal scale, and the Transient sub-processes that operate intermittently onthe local context of the human while interacting with objects. These twomechanisms form an interactive Persistent-Transient Duality thatsynergistically governs the activity sequences. We model this conceptualduality by a parent-child neural network of Persistent and Transient channelswith a dedicated neural module for dynamic mechanism switching. The frameworkis trialed on HOI motion forecasting. On two rich datasets and a wide varietyof settings, the model consistently delivers superior performances, proving itssuitability for the challenge.                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.12729v1 |
| 389 | PG-RCNN: Semantic Surface Point Generation for 3D Object Detection                                                                                   | Inyong Koo                     | 2023-07-24     | cs.CV                                   | One of the main challenges in LiDAR-based 3D object detection is that thesensors often fail to capture the complete spatial information about theobjects due to long distance and occlusion. Two-stage detectors with pointcloud completion approaches tackle this problem by adding more points to theregions of interest (RoIs) with a pre-trained network. However, these methodsgenerate dense point clouds of objects for all region proposals, assuming thatobjects always exist in the RoIs. This leads to the indiscriminate pointgeneration for incorrect proposals as well. Motivated by this, we propose PointGeneration R-CNN (PG-RCNN), a novel end-to-end detector that generates semanticsurface points of foreground objects for accurate detection. Our method uses ajointly trained RoI point generation module to process the contextualinformation of RoIs and estimate the complete shape and displacement offoreground objects. For every generated point, PG-RCNN assigns a semanticfeature that indicates the estimated foreground probability. Extensiveexperiments show that the point clouds generated by our method providegeometrically and semantically rich information for refining false positive andmisaligned proposals. PG-RCNN achieves competitive performance on the KITTIbenchmark, with significantly fewer parameters than state-of-the-art models.The code is available at https://github.com/quotation2520/PG-RCNN.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.12637v1 |
| 390 | CTVIS: Consistent Training for Online Video Instance Segmentation                                                                                    | Kaining Ying                   | 2023-07-24     | cs.CV, cs.AI                            | The discrimination of instance embeddings plays a vital role in associatinginstances across time for online video instance segmentation (VIS). Instanceembedding learning is directly supervised by the contrastive loss computed uponthe contrastive items (CIs), which are sets of anchor/positive/negativeembeddings. Recent online VIS methods leverage CIs sourced from one referenceframe only, which we argue is insufficient for learning highly discriminativeembeddings. Intuitively, a possible strategy to enhance CIs is replicating theinference phase during training. To this end, we propose a simple yet effectivetraining strategy, called Consistent Training for Online VIS (CTVIS), whichdevotes to aligning the training and inference pipelines in terms of buildingCIs. Specifically, CTVIS constructs CIs by referring inference themomentum-averaged embedding and the memory bank storage mechanisms, and addingnoise to the relevant embeddings. Such an extension allows a reliablecomparison between embeddings of current instances and the stablerepresentations of historical instances, thereby conferring an advantage inmodeling VIS challenges such as occlusion, re-identification, and deformation.Empirically, CTVIS outstrips the SOTA VIS models by up to +5.0 points on threeVIS benchmarks, including YTVIS19 (55.1% AP), YTVIS21 (50.1% AP) and OVIS(35.5% AP). Furthermore, we find that pseudo-videos transformed from images cantrain robust models surpassing fully-supervised ones.                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.12616v1 |
| 391 | PRIOR: Prototype Representation Joint Learning from Medical Images and Reports                                                                       | Pujin Cheng                    | 2023-07-24     | cs.CV                                   | Contrastive learning based vision-language joint pre-training has emerged asa successful representation learning strategy. In this paper, we present aprototype representation learning framework incorporating both global and localalignment between medical images and reports. In contrast to standard globalmulti-modality alignment methods, we employ a local alignment module forfine-grained representation. Furthermore, a cross-modality conditionalreconstruction module is designed to interchange information across modalitiesin the training phase by reconstructing masked images and reports. Forreconstructing long reports, a sentence-wise prototype memory bank isconstructed, enabling the network to focus on low-level localized visual andhigh-level clinical linguistic features. Additionally, a non-auto-regressivegeneration paradigm is proposed for reconstructing non-sequential reports.Experimental results on five downstream tasks, including supervisedclassification, zero-shot classification, image-to-text retrieval, semanticsegmentation, and object detection, show the proposed method outperforms otherstate-of-the-art methods across multiple datasets and under different datasetsize settings. The code is available at https://github.com/QtacierP/PRIOR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.12577v1 |
| 392 | A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation                                         | Jinjing Zhu                    | 2023-07-24     | cs.CV                                   | In this paper, we strive to answer the question "how to collaboratively learnconvolutional neural network (CNN)-based and vision transformer (ViT)-basedmodels by selecting and exchanging the reliable knowledge between them forsemantic segmentation?" Accordingly, we propose an online knowledgedistillation (KD) framework that can simultaneously learn compact yet effectiveCNN-based and ViT-based models with two key technical breakthroughs to takefull advantage of CNNs and ViT while compensating their limitations. Firstly,we propose heterogeneous feature distillation (HFD) to improve students'consistency in low-layer feature space by mimicking heterogeneous featuresbetween CNNs and ViT. Secondly, to facilitate the two students to learnreliable knowledge from each other, we propose bidirectional selectivedistillation (BSD) that can dynamically transfer selective knowledge. This isachieved by 1) region-wise BSD determining the directions of knowledgetransferred between the corresponding regions in the feature space and 2)pixel-wise BSD discerning which of the prediction knowledge to be transferredin the logit space. Extensive experiments on three benchmark datasetsdemonstrate that our proposed framework outperforms the state-of-the-art onlinedistillation methods by a large margin, and shows its efficacy in learningcollaboratively between ViT-based and CNN-based models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.12574v1 |
| 393 | Rethinking Data Distillation: Do Not Overlook Calibration                                                                                            | Dongyao Zhu                    | 2023-07-24     | cs.CV, cs.LG                            | Neural networks trained on distilled data often produce over-confident outputand require correction by calibration methods. Existing calibration methodssuch as temperature scaling and mixup work well for networks trained onoriginal large-scale data. However, we find that these methods fail tocalibrate networks trained on data distilled from large source datasets. Inthis paper, we show that distilled data lead to networks that are notcalibratable due to (i) a more concentrated distribution of the maximum logitsand (ii) the loss of information that is semantically meaningful but unrelatedto classification tasks. To address this problem, we propose Masked TemperatureScaling (MTS) and Masked Distillation Training (MDT) which mitigate thelimitations of distilled data and achieve better calibration results whilemaintaining the efficiency of dataset distillation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2307.12463v2 |
| 394 | ProtoFL: Unsupervised Federated Learning via Prototypical Distillation                                                                               | Hansol Kim                     | 2023-07-23     | cs.CV, cs.AI, cs.LG                     | Federated learning (FL) is a promising approach for enhancing data privacypreservation, particularly for authentication systems. However, limited roundcommunications, scarce representation, and scalability pose significantchallenges to its deployment, hindering its full potential. In this paper, wepropose 'ProtoFL', Prototypical Representation Distillation based unsupervisedFederated Learning to enhance the representation power of a global model andreduce round communication costs. Additionally, we introduce a local one-classclassifier based on normalizing flows to improve performance with limited data.Our study represents the first investigation of using FL to improve one-classclassification performance. We conduct extensive experiments on five widelyused benchmarks, namely MNIST, CIFAR-10, CIFAR-100, ImageNet-30, andKeystroke-Dynamics, to demonstrate the superior performance of our proposedframework over previous methods in the literature.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.12450v2 |
| 395 | Augmented Box Replay: Overcoming Foreground Shift for Incremental Object Detection                                                                   | Liu Yuyang                     | 2023-07-23     | cs.CV, cs.LG                            | In incremental learning, replaying stored samples from previous taskstogether with current task samples is one of the most efficient approaches toaddress catastrophic forgetting. However, unlike incremental classification,image replay has not been successfully applied to incremental object detection(IOD). In this paper, we identify the overlooked problem of foreground shift asthe main reason for this. Foreground shift only occurs when replaying images ofprevious tasks and refers to the fact that their background might containforeground objects of the current task. To overcome this problem, a novel andefficient Augmented Box Replay (ABR) method is developed that only stores andreplays foreground objects and thereby circumvents the foreground shiftproblem. In addition, we propose an innovative Attentive RoI Distillation lossthat uses spatial attention from region-of-interest (RoI) features to constraincurrent model to focus on the most important information from old model. ABRsignificantly reduces forgetting of previous classes while maintaining highplasticity in current classes. Moreover, it considerably reduces the storagerequirements when compared to standard image replay. Comprehensive experimentson Pascal-VOC and COCO datasets support the state-of-the-art performance of ourmodel.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.12427v1 |
| 396 | TransHuman: A Transformer-based Human Representation for Generalizable Neural Human Rendering                                                        | Xiao Pan                       | 2023-07-23     | cs.CV                                   | In this paper, we focus on the task of generalizable neural human renderingwhich trains conditional Neural Radiance Fields (NeRF) from multi-view videosof different characters. To handle the dynamic human motion, previous methodshave primarily used a SparseConvNet (SPC)-based human representation to processthe painted SMPL. However, such SPC-based representation i) optimizes under thevolatile observation space which leads to the pose-misalignment betweentraining and inference stages, and ii) lacks the global relationships amonghuman parts that is critical for handling the incomplete painted SMPL. Tacklingthese issues, we present a brand-new framework named TransHuman, which learnsthe painted SMPL under the canonical space and captures the globalrelationships between human parts with transformers. Specifically, TransHumanis mainly composed of Transformer-based Human Encoding (TransHE), DeformablePartial Radiance Fields (DPaRF), and Fine-grained Detail Integration (FDI).TransHE first processes the painted SMPL under the canonical space viatransformers for capturing the global relationships between human parts. Then,DPaRF binds each output token with a deformable radiance field for encoding thequery point under the observation space. Finally, the FDI is employed tofurther integrate fine-grained information from reference images. Extensiveexperiments on ZJU-MoCap and H36M show that our TransHuman achieves asignificantly new state-of-the-art performance with high efficiency. Projectpage: https://pansanity666.github.io/TransHuman/                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.12291v1 |
| 397 | Downstream-agnostic Adversarial Examples                                                                                                             | Ziqi Zhou                      | 2023-07-23     | cs.CV                                   | Self-supervised learning usually uses a large amount of unlabeled data topre-train an encoder which can be used as a general-purpose feature extractor,such that downstream users only need to perform fine-tuning operations to enjoythe benefit of "large model". Despite this promising prospect, the security ofpre-trained encoder has not been thoroughly investigated yet, especially whenthe pre-trained encoder is publicly available for commercial use.  In this paper, we propose AdvEncoder, the first framework for generatingdownstream-agnostic universal adversarial examples based on the pre-trainedencoder. AdvEncoder aims to construct a universal adversarial perturbation orpatch for a set of natural images that can fool all the downstream tasksinheriting the victim pre-trained encoder. Unlike traditional adversarialexample works, the pre-trained encoder only outputs feature vectors rather thanclassification labels. Therefore, we first exploit the high frequency componentinformation of the image to guide the generation of adversarial examples. Thenwe design a generative attack framework to construct adversarialperturbations/patches by learning the distribution of the attack surrogatedataset to improve their attack success rates and transferability. Our resultsshow that an attacker can successfully attack downstream tasks without knowingeither the pre-training dataset or the downstream dataset. We also tailor fourdefenses for pre-trained encoders, the results of which further prove theattack ability of AdvEncoder.                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.12280v2 |
| 398 | LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference                                                 | Cong Wang                      | 2023-07-23     | cs.CV                                   | We propose a novel method, LoLep, which regresses Locally-Learned planes froma single RGB image to represent scenes accurately, thus generating better novelviews. Without the depth information, regressing appropriate plane locations isa challenging problem. To solve this issue, we pre-partition the disparityspace into bins and design a disparity sampler to regress local offsets formultiple planes in each bin. However, only using such a sampler makes thenetwork not convergent; we further propose two optimizing strategies thatcombine with different disparity distributions of datasets and propose anocclusion-aware reprojection loss as a simple yet effective geometricsupervision technique. We also introduce a self-attention mechanism to improveocclusion inference and present a Block-Sampling Self-Attention (BS-SA) moduleto address the problem of applying self-attention to large feature maps. Wedemonstrate the effectiveness of our approach and generate state-of-the-artresults on different datasets. Compared to MINE, our approach has an LPIPSreduction of 4.8%-9.0% and an RV reduction of 73.9%-83.5%. We also evaluate theperformance on real-world images and demonstrate the benefits.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.12217v2 |
| 399 | LIST: Learning Implicitly from Spatial Transformers for Single-View 3D Reconstruction                                                                | Mohammad Samiul Arshad         | 2023-07-23     | cs.CV, cs.GR                            | Accurate reconstruction of both the geometric and topological details of a 3Dobject from a single 2D image embodies a fundamental challenge in computervision. Existing explicit/implicit solutions to this problem struggle torecover self-occluded geometry and/or faithfully reconstruct topological shapestructures. To resolve this dilemma, we introduce LIST, a novel neuralarchitecture that leverages local and global image features to accuratelyreconstruct the geometric and topological structure of a 3D object from asingle image. We utilize global 2D features to predict a coarse shape of thetarget object and then use it as a base for higher-resolution reconstruction.By leveraging both local 2D features from the image and 3D features from thecoarse prediction, we can predict the signed distance between an arbitrarypoint and the target surface via an implicit predictor with great accuracy.Furthermore, our model does not require camera estimation or pixel alignment.It provides an uninfluenced reconstruction from the input-view direction.Through qualitative and quantitative analysis, we show the superiority of ourmodel in reconstructing 3D objects from both synthetic and real-world imagesagainst the state of the art.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.12194v1 |
| 400 | Hallucination Improves the Performance of Unsupervised Visual Representation Learning                                                                | Jing Wu                        | 2023-07-22     | cs.CV, cs.AI                            | Contrastive learning models based on Siamese structure have demonstratedremarkable performance in self-supervised learning. Such a success ofcontrastive learning relies on two conditions, a sufficient number of positivepairs and adequate variations between them. If the conditions are not met,these frameworks will lack semantic contrast and be fragile on overfitting. Toaddress these two issues, we propose Hallucinator that could efficientlygenerate additional positive samples for further contrast. The Hallucinator isdifferentiable and creates new data in the feature space. Thus, it is optimizeddirectly with the pre-training task and introduces nearly negligiblecomputation. Moreover, we reduce the mutual information of hallucinated pairsand smooth them through non-linear operations. This process helps avoidover-confident contrastive learning models during the training and achievesmore transformation-invariant feature embeddings. Remarkably, we empiricallyprove that the proposed Hallucinator generalizes well to various contrastivelearning models, including MoCoV1&V2, SimCLR and SimSiam. Under the linearclassification protocol, a stable accuracy gain is achieved, ranging from 0.3%to 3.0% on CIFAR10&100, Tiny ImageNet, STL-10 and ImageNet. The improvement isalso observed in transferring pre-train encoders to the downstream tasks,including object detection and segmentation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.12168v1 |
| 401 | InFusion: Inject and Attention Fusion for Multi Concept Zero-Shot Text-based Video Editing                                                           | Anant Khandelwal               | 2023-07-22     | cs.CV                                   | Large text-to-image diffusion models have achieved remarkable success ingenerating diverse, high-quality images. Additionally, these models have beensuccessfully leveraged to edit input images by just changing the text prompt.But when these models are applied to videos, the main challenge is to ensuretemporal consistency and coherence across frames. In this paper, we proposeInFusion, a framework for zero-shot text-based video editing leveraging largepre-trained image diffusion models. Our framework specifically supports editingof multiple concepts with pixel-level control over diverse concepts mentionedin the editing prompt. Specifically, we inject the difference in featuresobtained with source and edit prompts from U-Net residual blocks of decoderlayers. When these are combined with injected attention features, it becomesfeasible to query the source contents and scale edited concepts along with theinjection of unedited parts. The editing is further controlled in afine-grained manner with mask extraction and attention fusion, which cut theedited part from the source and paste it into the denoising pipeline for theediting prompt. Our framework is a low-cost alternative to one-shot tunedmodels for editing since it does not require training. We demonstrated complexconcept editing with a generalised image model (Stable Diffusion v1.5) usingLoRA. Adaptation is compatible with all the existing image diffusiontechniques. Extensive experimental results demonstrate the effectiveness ofexisting methods in rendering high-quality and temporally consistent videos.                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2308.00135v3 |
| 402 | Spatial Self-Distillation for Object Detection with Inaccurate Bounding Boxes                                                                        | Di Wu                          | 2023-07-22     | cs.CV                                   | Object detection via inaccurate bounding boxes supervision has boosted abroad interest due to the expensive high-quality annotation data or theoccasional inevitability of low annotation quality (\eg tiny objects). Theprevious works usually utilize multiple instance learning (MIL), which highlydepends on category information, to select and refine a low-quality box. Thosemethods suffer from object drift, group prediction and part domination problemswithout exploring spatial information. In this paper, we heuristically proposea \textbf{Spatial Self-Distillation based Object Detector (SSD-Det)} to minespatial information to refine the inaccurate box in a self-distillationfashion. SSD-Det utilizes a Spatial Position Self-Distillation \textbf{(SPSD)}module to exploit spatial information and an interactive structure to combinespatial information and category information, thus constructing a high-qualityproposal bag. To further improve the selection procedure, a Spatial IdentitySelf-Distillation \textbf{(SISD)} module is introduced in SSD-Det to obtainspatial confidence to help select the best proposals. Experiments on MS-COCOand VOC datasets with noisy box annotation verify our method's effectivenessand achieve state-of-the-art performance. The code is available athttps://github.com/ucas-vg/PointTinyBenchmark/tree/SSD-Det.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.12101v2 |
| 403 | Replay: Multi-modal Multi-view Acted Videos for Casual Holography                                                                                    | Roman Shapovalov               | 2023-07-22     | cs.CV                                   | We introduce Replay, a collection of multi-view, multi-modal videos of humansinteracting socially. Each scene is filmed in high production quality, fromdifferent viewpoints with several static cameras, as well as wearable actioncameras, and recorded with a large array of microphones at different positionsin the room. Overall, the dataset contains over 4000 minutes of footage andover 7 million timestamped high-resolution frames annotated with camera posesand partially with foreground masks. The Replay dataset has many potentialapplications, such as novel-view synthesis, 3D reconstruction, novel-viewacoustic synthesis, human body and face analysis, and training generativemodels. We provide a benchmark for training and evaluating novel-viewsynthesis, with two scenarios of different difficulty. Finally, we evaluateseveral baseline state-of-the-art methods on the new benchmark.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2307.12067v1 |
| 404 | On the Effectiveness of Spectral Discriminators for Perceptual Quality Improvement                                                                   | Xin Luo                        | 2023-07-22     | cs.CV, eess.IV                          | Several recent studies advocate the use of spectral discriminators, whichevaluate the Fourier spectra of images for generative modeling. However, theeffectiveness of the spectral discriminators is not well interpreted yet. Wetackle this issue by examining the spectral discriminators in the context ofperceptual image super-resolution (i.e., GAN-based SR), as SR image quality issusceptible to spectral changes. Our analyses reveal that the spectraldiscriminator indeed performs better than the ordinary (a.k.a. spatial)discriminator in identifying the differences in the high-frequency range;however, the spatial discriminator holds an advantage in the low-frequencyrange. Thus, we suggest that the spectral and spatial discriminators shall beused simultaneously. Moreover, we improve the spectral discriminators by firstcalculating the patch-wise Fourier spectrum and then aggregating the spectra byTransformer. We verify the effectiveness of the proposed method twofold. On theone hand, thanks to the additional spectral discriminator, our obtained SRimages have their spectra better aligned to those of the real images, whichleads to a better PD tradeoff. On the other hand, our ensembled discriminatorpredicts the perceptual quality more accurately, as evidenced in theno-reference image quality assessment task.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2307.12027v2 |
| 405 | Learning Vision-and-Language Navigation from YouTube Videos                                                                                          | Kunyang Lin                    | 2023-07-22     | cs.CV, cs.CL                            | Vision-and-language navigation (VLN) requires an embodied agent to navigatein realistic 3D environments using natural language instructions. Existing VLNmethods suffer from training on small-scale environments or unreasonablepath-instruction datasets, limiting the generalization to unseen environments.There are massive house tour videos on YouTube, providing abundant realnavigation experiences and layout information. However, these videos have notbeen explored for VLN before. In this paper, we propose to learn an agent fromthese videos by creating a large-scale dataset which comprises reasonablepath-instruction pairs from house tour videos and pre-training the agent on it.To achieve this, we have to tackle the challenges of automatically constructingpath-instruction pairs and exploiting real layout knowledge from raw andunlabeled videos. To address these, we first leverage an entropy-based methodto construct the nodes of a path trajectory. Then, we propose an action-awaregenerator for generating instructions from unlabeled trajectories. Last, wedevise a trajectory judgment pretext task to encourage the agent to mine thelayout knowledge. Experimental results show that our method achievesstate-of-the-art performance on two popular benchmarks (R2R and REVERIE). Codeis available at https://github.com/JeremyLinky/YouTube-VLN                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.11984v1 |
| 406 | HybridAugment++: Unified Frequency Spectra Perturbations for Model Robustness                                                                        | Mehmet Kerim Yucel             | 2023-07-21     | cs.CV, cs.AI                            | Convolutional Neural Networks (CNN) are known to exhibit poor generalizationperformance under distribution shifts. Their generalization have been studiedextensively, and one line of work approaches the problem from afrequency-centric perspective. These studies highlight the fact that humans andCNNs might focus on different frequency components of an image. First, inspiredby these observations, we propose a simple yet effective data augmentationmethod HybridAugment that reduces the reliance of CNNs on high-frequencycomponents, and thus improves their robustness while keeping their cleanaccuracy high. Second, we propose HybridAugment++, which is a hierarchicalaugmentation method that attempts to unify various frequency-spectrumaugmentations. HybridAugment++ builds on HybridAugment, and also reduces thereliance of CNNs on the amplitude component of images, and promotes phaseinformation instead. This unification results in competitive to or better thanstate-of-the-art results on clean accuracy (CIFAR-10/100 and ImageNet),corruption benchmarks (ImageNet-C, CIFAR-10-C and CIFAR-100-C), adversarialrobustness on CIFAR-10 and out-of-distribution detection on various datasets.HybridAugment and HybridAugment++ are implemented in a few lines of code, doesnot require extra data, ensemble models or additional networks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.11823v1 |
| 407 | OxfordTVG-HIC: Can Machine Make Humorous Captions from Images?                                                                                       | Runjia Li                      | 2023-07-21     | cs.CV, cs.CL                            | This paper presents OxfordTVG-HIC (Humorous Image Captions), a large-scaledataset for humour generation and understanding. Humour is an abstract,subjective, and context-dependent cognitive construct involving severalcognitive factors, making it a challenging task to generate and interpret.Hence, humour generation and understanding can serve as a new task forevaluating the ability of deep-learning methods to process abstract andsubjective information. Due to the scarcity of data, humour-related generationtasks such as captioning remain under-explored. To address this gap,OxfordTVG-HIC offers approximately 2.9M image-text pairs with humour scores totrain a generalizable humour captioning model. Contrary to existing captioningdatasets, OxfordTVG-HIC features a wide range of emotional and semanticdiversity resulting in out-of-context examples that are particularly conduciveto generating humour. Moreover, OxfordTVG-HIC is curated devoid of offensivecontent. We also show how OxfordTVG-HIC can be leveraged for evaluating thehumour of a generated text. Through explainability analysis of the trainedmodels, we identify the visual and linguistic cues influential for evokinghumour prediction (and generation). We observe qualitatively that these cuesare aligned with the benign violation theory of humour in cognitive psychology.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.11636v1 |
| 408 | Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation                                                   | Zunnan Xu                      | 2023-07-21     | cs.CV, cs.CL                            | Parameter Efficient Tuning (PET) has gained attention for reducing the numberof parameters while maintaining performance and providing better hardwareresource savings, but few studies investigate dense prediction tasks andinteraction between modalities. In this paper, we do an investigation ofefficient tuning problems on referring image segmentation. We propose a noveladapter called Bridger to facilitate cross-modal information exchange andinject task-specific information into the pre-trained model. We also design alightweight decoder for image segmentation. Our approach achieves comparable orsuperior performance with only 1.61\% to 3.38\% backbone parameter updates,evaluated on challenging benchmarks. The code is available at\url{https://github.com/kkakkkka/ETRIS}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.11545v1 |
| 409 | CopyRNeRF: Protecting the CopyRight of Neural Radiance Fields                                                                                        | Ziyuan Luo                     | 2023-07-21     | cs.CV                                   | Neural Radiance Fields (NeRF) have the potential to be a major representationof media. Since training a NeRF has never been an easy task, the protection ofits model copyright should be a priority. In this paper, by analyzing the prosand cons of possible copyright protection solutions, we propose to protect thecopyright of NeRF models by replacing the original color representation in NeRFwith a watermarked color representation. Then, a distortion-resistant renderingscheme is designed to guarantee robust message extraction in 2D renderings ofNeRF. Our proposed method can directly protect the copyright of NeRF modelswhile maintaining high rendering quality and bit accuracy when compared amongoptional solutions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.11526v2 |
| 410 | CORE: Cooperative Reconstruction for Multi-Agent Perception                                                                                          | Binglu Wang                    | 2023-07-21     | cs.CV                                   | This paper presents CORE, a conceptually simple, effective andcommunication-efficient model for multi-agent cooperative perception. Itaddresses the task from a novel perspective of cooperative reconstruction,based on two key insights: 1) cooperating agents together provide a moreholistic observation of the environment, and 2) the holistic observation canserve as valuable supervision to explicitly guide the model learning how toreconstruct the ideal observation based on collaboration. CORE instantiates theidea with three major components: a compressor for each agent to create morecompact feature representation for efficient broadcasting, a lightweightattentive collaboration component for cross-agent message aggregation, and areconstruction module to reconstruct the observation based on aggregatedfeature representations. This learning-to-reconstruct idea is task-agnostic,and offers clear and reasonable supervision to inspire more effectivecollaboration, eventually promoting perception tasks. We validate CORE onOPV2V, a large-scale multi-agent percetion dataset, in two tasks, i.e., 3Dobject detection and semantic segmentation. Results demonstrate that the modelachieves state-of-the-art performance on both tasks, and is morecommunication-efficient.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2307.11514v2 |
| 411 | FaceCLIPNeRF: Text-driven 3D Face Manipulation using Deformable Neural Radiance Fields                                                               | Sungwon Hwang                  | 2023-07-21     | cs.CV                                   | As recent advances in Neural Radiance Fields (NeRF) have enabledhigh-fidelity 3D face reconstruction and novel view synthesis, its manipulationalso became an essential task in 3D vision. However, existing manipulationmethods require extensive human labor, such as a user-provided semantic maskand manual attribute search unsuitable for non-expert users. Instead, ourapproach is designed to require a single text to manipulate a facereconstructed with NeRF. To do so, we first train a scene manipulator, a latentcode-conditional deformable NeRF, over a dynamic scene to control a facedeformation using the latent code. However, representing a scene deformationwith a single latent code is unfavorable for compositing local deformationsobserved in different instances. As so, our proposed Position-conditionalAnchor Compositor (PAC) learns to represent a manipulated scene with spatiallyvarying latent codes. Their renderings with the scene manipulator are thenoptimized to yield high cosine similarity to a target text in CLIP embeddingspace for text-driven manipulation. To the best of our knowledge, our approachis the first to address the text-driven manipulation of a face reconstructedwith NeRF. Extensive results, comparisons, and ablation studies demonstrate theeffectiveness of our approach.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.11418v3 |
| 412 | CLR: Channel-wise Lightweight Reprogramming for Continual Learning                                                                                   | Yunhao Ge                      | 2023-07-21     | cs.CV                                   | Continual learning aims to emulate the human ability to continuallyaccumulate knowledge over sequential tasks. The main challenge is to maintainperformance on previously learned tasks after learning new tasks, i.e., toavoid catastrophic forgetting. We propose a Channel-wise LightweightReprogramming (CLR) approach that helps convolutional neural networks (CNNs)overcome catastrophic forgetting during continual learning. We show that a CNNmodel trained on an old task (or self-supervised proxy task) could be``reprogrammed" to solve a new task by using our proposed lightweight (verycheap) reprogramming parameter. With the help of CLR, we have a betterstability-plasticity trade-off to solve continual learning problems: Tomaintain stability and retain previous task ability, we use a commontask-agnostic immutable part as the shared ``anchor" parameter set. We then addtask-specific lightweight reprogramming parameters to reinterpret the outputsof the immutable parts, to enable plasticity and integrate new knowledge. Tolearn sequential tasks, we only train the lightweight reprogramming parametersto learn each new task. Reprogramming parameters are task-specific andexclusive to each task, which makes our method immune to catastrophicforgetting. To minimize the parameter requirement of reprogramming to learn newtasks, we make reprogramming lightweight by only adjusting essential kernelsand learning channel-wise linear mappings from anchor parameters totask-specific domain knowledge. We show that, for general CNNs, the CLRparameter increase is less than 0.6\% for any new task. Our method outperforms13 state-of-the-art continual learning baselines on a new challenging sequenceof 53 image classification datasets. Code and data are available athttps://github.com/gyhandy/Channel-wise-Lightweight-Reprogramming                                                               | http://arxiv.org/abs/2307.11386v1 |
| 413 | Tuning Pre-trained Model via Moment Probing                                                                                                          | Mingze Gao                     | 2023-07-21     | cs.CV                                   | Recently, efficient fine-tuning of large-scale pre-trained models hasattracted increasing research interests, where linear probing (LP) as afundamental module is involved in exploiting the final representations fortask-dependent classification. However, most of the existing methods focus onhow to effectively introduce a few of learnable parameters, and little workpays attention to the commonly used LP module. In this paper, we propose anovel Moment Probing (MP) method to further explore the potential of LP.Distinguished from LP which builds a linear classification head based on themean of final features (e.g., word tokens for ViT) or classification tokens,our MP performs a linear classifier on feature distribution, which provides thestronger representation ability by exploiting richer statistical informationinherent in features. Specifically, we represent feature distribution by itscharacteristic function, which is efficiently approximated by using first- andsecond-order moments of features. Furthermore, we propose a multi-headconvolutional cross-covariance (MHC$^3$) to compute second-order moments in anefficient and effective manner. By considering that MP could affect featurelearning, we introduce a partially shared module to learn two recalibratingparameters (PSRP) for backbones based on MP, namely MP$_{+}$. Extensiveexperiments on ten benchmarks using various models show that our MPsignificantly outperforms LP and is competitive with counterparts at lesstraining cost, while our MP$_{+}$ achieves state-of-the-art performance.                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.11342v1 |
| 414 | Tri-MipRF: Tri-Mip Representation for Efficient Anti-Aliasing Neural Radiance Fields                                                                 | Wenbo Hu                       | 2023-07-21     | cs.CV, cs.AI, cs.GR                     | Despite the tremendous progress in neural radiance fields (NeRF), we stillface a dilemma of the trade-off between quality and efficiency, e.g., MipNeRFpresents fine-detailed and anti-aliased renderings but takes days for training,while Instant-ngp can accomplish the reconstruction in a few minutes butsuffers from blurring or aliasing when rendering at various distances orresolutions due to ignoring the sampling area. To this end, we propose a novelTri-Mip encoding that enables both instant reconstruction and anti-aliasedhigh-fidelity rendering for neural radiance fields. The key is to factorize thepre-filtered 3D feature spaces in three orthogonal mipmaps. In this way, we canefficiently perform 3D area sampling by taking advantage of 2D pre-filteredfeature maps, which significantly elevates the rendering quality withoutsacrificing efficiency. To cope with the novel Tri-Mip representation, wepropose a cone-casting rendering technique to efficiently sample anti-aliased3D features with the Tri-Mip encoding considering both pixel imaging andobserving distance. Extensive experiments on both synthetic and real-worlddatasets demonstrate our method achieves state-of-the-art rendering quality andreconstruction speed while maintaining a compact representation that reduces25% model size compared against Instant-ngp.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.11335v1 |
| 415 | MAS: Towards Resource-Efficient Federated Multiple-Task Learning                                                                                     | Weiming Zhuang                 | 2023-07-21     | cs.LG, cs.CV, cs.DC                     | Federated learning (FL) is an emerging distributed machine learning methodthat empowers in-situ model training on decentralized edge devices. However,multiple simultaneous FL tasks could overload resource-constrained devices. Inthis work, we propose the first FL system to effectively coordinate and trainmultiple simultaneous FL tasks. We first formalize the problem of trainingsimultaneous FL tasks. Then, we present our new approach, MAS (Merge andSplit), to optimize the performance of training multiple simultaneous FL tasks.MAS starts by merging FL tasks into an all-in-one FL task with a multi-taskarchitecture. After training for a few rounds, MAS splits the all-in-one FLtask into two or more FL tasks by using the affinities among tasks measuredduring the all-in-one training. It then continues training each split of FLtasks based on model parameters from the all-in-one training. Extensiveexperiments demonstrate that MAS outperforms other methods while reducingtraining time by 2x and reducing energy consumption by 40%. We hope this workwill inspire the community to further study and optimize training simultaneousFL tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.11285v1 |
| 416 | AlignDet: Aligning Pre-training and Fine-tuning in Object Detection                                                                                  | Ming Li                        | 2023-07-20     | cs.CV, cs.AI, cs.LG                     | The paradigm of large-scale pre-training followed by downstream fine-tuninghas been widely employed in various object detection algorithms. In this paper,we reveal discrepancies in data, model, and task between the pre-training andfine-tuning procedure in existing practices, which implicitly limit thedetector's performance, generalization ability, and convergence speed. To thisend, we propose AlignDet, a unified pre-training framework that can be adaptedto various existing detectors to alleviate the discrepancies. AlignDetdecouples the pre-training process into two stages, i.e., image-domain andbox-domain pre-training. The image-domain pre-training optimizes the detectionbackbone to capture holistic visual abstraction, and box-domain pre-traininglearns instance-level semantics and task-aware concepts to initialize the partsout of the backbone. By incorporating the self-supervised pre-trainedbackbones, we can pre-train all modules for various detectors in anunsupervised paradigm. As depicted in Figure 1, extensive experimentsdemonstrate that AlignDet can achieve significant improvements across diverseprotocols, such as detection algorithm, model backbone, data setting, andtraining schedule. For example, AlignDet improves FCOS by 5.3 mAP, RetinaNet by2.1 mAP, Faster R-CNN by 3.3 mAP, and DETR by 2.3 mAP under fewer epochs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.11077v2 |
| 417 | CNOS: A Strong Baseline for CAD-based Novel Object Segmentation                                                                                      | Van Nguyen Nguyen              | 2023-07-20     | cs.CV                                   | We propose a simple three-stage approach to segment unseen objects in RGBimages using their CAD models. Leveraging recent powerful foundation models,DINOv2 and Segment Anything, we create descriptors and generate proposals,including binary masks for a given input RGB image. By matching proposals withreference descriptors created from CAD models, we achieve precise object IDassignment along with modal masks. We experimentally demonstrate that ourmethod achieves state-of-the-art results in CAD-based novel objectsegmentation, surpassing existing approaches on the seven core datasets of theBOP challenge by 19.8% AP using the same BOP evaluation protocol. Our sourcecode is available at https://github.com/nv-nguyen/cnos.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2307.11067v4 |
| 418 | Cascade-DETR: Delving into High-Quality Universal Object Detection                                                                                   | Mingqiao Ye                    | 2023-07-20     | cs.CV, cs.AI                            | Object localization in general environments is a fundamental part of visionsystems. While dominating on the COCO benchmark, recent Transformer-baseddetection methods are not competitive in diverse domains. Moreover, thesemethods still struggle to very accurately estimate the object bounding boxes incomplex environments.  We introduce Cascade-DETR for high-quality universal object detection. Wejointly tackle the generalization to diverse domains and localization accuracyby proposing the Cascade Attention layer, which explicitly integratesobject-centric information into the detection decoder by limiting the attentionto the previous box prediction. To further enhance accuracy, we also revisitthe scoring of queries. Instead of relying on classification scores, we predictthe expected IoU of the query, leading to substantially more well-calibratedconfidences. Lastly, we introduce a universal object detection benchmark,UDB10, that contains 10 datasets from diverse domains. While also advancing thestate-of-the-art on COCO, Cascade-DETR substantially improves DETR-baseddetectors on all datasets in UDB10, even by over 10 mAP in some cases. Theimprovements under stringent quality requirements are even more pronounced. Ourcode and models will be released at https://github.com/SysCV/cascade-detr.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.11035v1 |
| 419 | General Image-to-Image Translation with One-Shot Image Guidance                                                                                      | Bin Cheng                      | 2023-07-20     | cs.CV                                   | Large-scale text-to-image models pre-trained on massive text-image pairs showexcellent performance in image synthesis recently. However, image can providemore intuitive visual concepts than plain text. People may ask: how can weintegrate the desired visual concept into an existing image, such as ourportrait? Current methods are inadequate in meeting this demand as they lackthe ability to preserve content or translate visual concepts effectively.Inspired by this, we propose a novel framework named visual concept translator(VCT) with the ability to preserve content in the source image and translatethe visual concepts guided by a single reference image. The proposed VCTcontains a content-concept inversion (CCI) process to extract contents andconcepts, and a content-concept fusion (CCF) process to gather the extractedinformation to obtain the target image. Given only one reference image, theproposed VCT can complete a wide range of general image-to-image translationtasks with excellent results. Extensive experiments are conducted to prove thesuperiority and effectiveness of the proposed methods. Codes are available athttps://github.com/CrystalNeuro/visual-concept-translator.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2307.14352v2 |
| 420 | Metric3D: Towards Zero-shot Metric 3D Prediction from A Single Image                                                                                 | Wei Yin                        | 2023-07-20     | cs.CV, cs.AI                            | Reconstructing accurate 3D scenes from images is a long-standing vision task.Due to the ill-posedness of the single-image reconstruction problem, mostwell-established methods are built upon multi-view geometry. State-of-the-art(SOTA) monocular metric depth estimation methods can only handle a singlecamera model and are unable to perform mixed-data training due to the metricambiguity. Meanwhile, SOTA monocular methods trained on large mixed datasetsachieve zero-shot generalization by learning affine-invariant depths, whichcannot recover real-world metrics. In this work, we show that the key to azero-shot single-view metric depth model lies in the combination of large-scaledata training and resolving the metric ambiguity from various camera models. Wepropose a canonical camera space transformation module, which explicitlyaddresses the ambiguity problems and can be effortlessly plugged into existingmonocular models. Equipped with our module, monocular models can be stablytrained with over 8 million images with thousands of camera models, resultingin zero-shot generalization to in-the-wild images with unseen camera settings.Experiments demonstrate SOTA performance of our method on 7 zero-shotbenchmarks. Notably, our method won the championship in the 2nd Monocular DepthEstimation Challenge. Our method enables the accurate recovery of metric 3Dstructures on randomly collected internet images, paving the way for plausiblesingle-image metrology. The potential benefits extend to downstream tasks,which can be significantly improved by simply plugging in our model. Forexample, our model relieves the scale drift issues of monocular-SLAM (Fig. 1),leading to high-quality metric scale dense mapping. The code is available athttps://github.com/YvanYin/Metric3D.                                                                                                               | http://arxiv.org/abs/2307.10984v1 |
| 421 | Improving Online Lane Graph Extraction by Object-Lane Clustering                                                                                     | Yigit Baran Can                | 2023-07-20     | cs.CV                                   | Autonomous driving requires accurate local scene understanding information.To this end, autonomous agents deploy object detection and online BEV lanegraph extraction methods as a part of their perception stack. In this work, wepropose an architecture and loss formulation to improve the accuracy of locallane graph estimates by using 3D object detection outputs. The proposed methodlearns to assign the objects to centerlines by considering the centerlines ascluster centers and the objects as data points to be assigned a probabilitydistribution over the cluster centers. This training scheme ensures directsupervision on the relationship between lanes and objects, thus leading tobetter performance. The proposed method improves lane graph estimationsubstantially over state-of-the-art methods. The extensive ablations show thatour method can achieve significant performance improvements by using theoutputs of existing 3D object detection methods. Since our method uses thedetection outputs rather than detection method intermediate representations, asingle model of our method can use any detection method at test time.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2307.10947v1 |
| 422 | Proxy Anchor-based Unsupervised Learning for Continuous Generalized Category Discovery                                                               | Hyungmin Kim                   | 2023-07-20     | cs.CV, cs.AI                            | Recent advances in deep learning have significantly improved the performanceof various computer vision applications. However, discovering novel categoriesin an incremental learning scenario remains a challenging problem due to thelack of prior knowledge about the number and nature of new categories. Existingmethods for novel category discovery are limited by their reliance on labeleddatasets and prior knowledge about the number of novel categories and theproportion of novel samples in the batch. To address the limitations and moreaccurately reflect real-world scenarios, in this paper, we propose a novelunsupervised class incremental learning approach for discovering novelcategories on unlabeled sets without prior knowledge. The proposed methodfine-tunes the feature extractor and proxy anchors on labeled sets, then splitssamples into old and novel categories and clusters on the unlabeled dataset.Furthermore, the proxy anchors-based exemplar generates representative categoryvectors to mitigate catastrophic forgetting. Experimental results demonstratethat our proposed approach outperforms the state-of-the-art methods onfine-grained datasets under real-world scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.10943v1 |
| 423 | BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion                                                                        | Jinheng Xie                    | 2023-07-20     | cs.CV                                   | Recent text-to-image diffusion models have demonstrated an astonishingcapacity to generate high-quality images. However, researchers mainly studiedthe way of synthesizing images with only text prompts. While some works haveexplored using other modalities as conditions, considerable paired data, e.g.,box/mask-image pairs, and fine-tuning time are required for nurturing models.As such paired data is time-consuming and labor-intensive to acquire andrestricted to a closed set, this potentially becomes the bottleneck forapplications in an open world. This paper focuses on the simplest form ofuser-provided conditions, e.g., box or scribble. To mitigate the aforementionedproblem, we propose a training-free method to control objects and contexts inthe synthesized images adhering to the given spatial conditions. Specifically,three spatial constraints, i.e., Inner-Box, Outer-Box, and Corner Constraints,are designed and seamlessly integrated into the denoising step of diffusionmodels, requiring no additional training and massive annotated layout data.Extensive experimental results demonstrate that the proposed constraints cancontrol what and where to present in the images while retaining the ability ofDiffusion models to synthesize with high fidelity and diverse concept coverage.The code is publicly available at https://github.com/showlab/BoxDiff.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.10816v4 |
| 424 | HyperReenact: One-Shot Reenactment via Jointly Learning to Refine and Retarget Faces                                                                 | Stella Bounareli               | 2023-07-20     | cs.CV                                   | In this paper, we present our method for neural face reenactment, calledHyperReenact, that aims to generate realistic talking head images of a sourceidentity, driven by a target facial pose. Existing state-of-the-art facereenactment methods train controllable generative models that learn tosynthesize realistic facial images, yet producing reenacted faces that areprone to significant visual artifacts, especially under the challengingcondition of extreme head pose changes, or requiring expensive few-shotfine-tuning to better preserve the source identity characteristics. We proposeto address these limitations by leveraging the photorealistic generationability and the disentangled properties of a pretrained StyleGAN2 generator, byfirst inverting the real images into its latent space and then using ahypernetwork to perform: (i) refinement of the source identity characteristicsand (ii) facial pose re-targeting, eliminating this way the dependence onexternal editing methods that typically produce artifacts. Our method operatesunder the one-shot setting (i.e., using a single source frame) and allows forcross-subject reenactment, without requiring any subject-specific fine-tuning.We compare our method both quantitatively and qualitatively against severalstate-of-the-art techniques on the standard benchmarks of VoxCeleb1 andVoxCeleb2, demonstrating the superiority of our approach in producingartifact-free images, exhibiting remarkable robustness even under extreme headpose changes. We make the code and the pretrained models publicly available at:https://github.com/StelaBou/HyperReenact .                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2307.10797v1 |
| 425 | See More and Know More: Zero-shot Point Cloud Segmentation via Multi-modal Visual Data                                                               | Yuhang Lu                      | 2023-07-20     | cs.CV                                   | Zero-shot point cloud segmentation aims to make deep models capable ofrecognizing novel objects in point cloud that are unseen in the training phase.Recent trends favor the pipeline which transfers knowledge from seen classeswith labels to unseen classes without labels. They typically align visualfeatures with semantic features obtained from word embedding by the supervisionof seen classes' annotations. However, point cloud contains limited informationto fully match with semantic features. In fact, the rich appearance informationof images is a natural complement to the textureless point cloud, which is notwell explored in previous literature. Motivated by this, we propose a novelmulti-modal zero-shot learning method to better utilize the complementaryinformation of point clouds and images for more accurate visual-semanticalignment. Extensive experiments are performed in two popular benchmarks, i.e.,SemanticKITTI and nuScenes, and our method outperforms current SOTA methodswith 52% and 49% improvement on average for unseen class mIoU, respectively.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.10782v1 |
| 426 | Lighting up NeRF via Unsupervised Decomposition and Enhancement                                                                                      | Haoyuan Wang                   | 2023-07-20     | cs.CV, cs.GR                            | Neural Radiance Field (NeRF) is a promising approach for synthesizing novelviews, given a set of images and the corresponding camera poses of a scene.However, images photographed from a low-light scene can hardly be used to traina NeRF model to produce high-quality results, due to their low pixelintensities, heavy noise, and color distortion. Combining existing low-lightimage enhancement methods with NeRF methods also does not work well due to theview inconsistency caused by the individual 2D enhancement process. In thispaper, we propose a novel approach, called Low-Light NeRF (or LLNeRF), toenhance the scene representation and synthesize normal-light novel viewsdirectly from sRGB low-light images in an unsupervised manner. The core of ourapproach is a decomposition of radiance field learning, which allows us toenhance the illumination, reduce noise and correct the distorted colors jointlywith the NeRF optimization process. Our method is able to produce novel viewimages with proper lighting and vivid colors and details, given a collection ofcamera-finished low dynamic range (8-bits/channel) images from a low-lightscene. Experiments demonstrate that our method outperforms existing low-lightenhancement methods and NeRF methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.10664v1 |
| 427 | Physics-Driven Turbulence Image Restoration with Stochastic Refinement                                                                               | Ajay Jaiswal                   | 2023-07-20     | eess.IV, cs.CV                          | Image distortion by atmospheric turbulence is a stochastic degradation, whichis a critical problem in long-range optical imaging systems. A number ofresearch has been conducted during the past decades, including model-based andemerging deep-learning solutions with the help of synthetic data. Although fastand physics-grounded simulation tools have been introduced to help thedeep-learning models adapt to real-world turbulence conditions recently, thetraining of such models only relies on the synthetic data and ground truthpairs. This paper proposes the Physics-integrated Restoration Network (PiRN) tobring the physics-based simulator directly into the training process to helpthe network to disentangle the stochasticity from the degradation and theunderlying image. Furthermore, to overcome the ``average effect" introduced bydeterministic models and the domain gap between the synthetic and real-worlddegradation, we further introduce PiRN with Stochastic Refinement (PiRN-SR) toboost its perceptual quality. Overall, our PiRN and PiRN-SR improve thegeneralization to real-world unknown turbulence conditions and provide astate-of-the-art restoration in both pixel-wise accuracy and perceptualquality. Our codes are available at \url{https://github.com/VITA-Group/PiRN}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.10603v1 |
| 428 | Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples                         | JoonHo Lee                     | 2023-07-19     | cs.CV, cs.LG                            | Deploying deep visual models can lead to performance drops due to thediscrepancies between source and target distributions. Several approachesleverage labeled source data to estimate target domain accuracy, but accessinglabeled source data is often prohibitively difficult due to dataconfidentiality or resource limitations on serving devices. Our work proposes anew framework to estimate model accuracy on unlabeled target data withoutaccess to source data. We investigate the feasibility of using pseudo-labelsfor accuracy estimation and evolve this idea into adopting recent advances insource-free domain adaptation algorithms. Our approach measures thedisagreement rate between the source hypothesis and the target pseudo-labelingfunction, adapted from the source hypothesis. We mitigate the impact oferroneous pseudo-labels that may arise due to a high ideal joint hypothesisrisk by employing adaptive adversarial perturbation on the input of the targetmodel. Our proposed source-free framework effectively addresses the challengingdistribution shift scenarios and outperforms existing methods requiring sourcedata and labels for training.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.10062v1 |
| 429 | MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions                                                                              | Yunfei Liu                     | 2023-07-19     | cs.CV                                   | Audio-driven portrait animation aims to synthesize portrait videos that areconditioned by given audio. Animating high-fidelity and multimodal videoportraits has a variety of applications. Previous methods have attempted tocapture different motion modes and generate high-fidelity portrait videos bytraining different models or sampling signals from given videos. However,lacking correlation learning between lip-sync and other movements (e.g., headpose/eye blinking) usually leads to unnatural results. In this paper, wepropose a unified system for multi-person, diverse, and high-fidelity talkingportrait generation. Our method contains three stages, i.e., 1) Mapping-Oncenetwork with Dual Attentions (MODA) generates talking representation from givenaudio. In MODA, we design a dual-attention module to encode accurate mouthmovements and diverse modalities. 2) Facial composer network generates denseand detailed face landmarks, and 3) temporal-guided renderer syntheses stablevideos. Extensive evaluations demonstrate that the proposed system producesmore natural and realistic video portraits compared to previous methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.10008v1 |
| 430 | Density-invariant Features for Distant Point Cloud Registration                                                                                      | Quan Liu                       | 2023-07-19     | cs.CV                                   | Registration of distant outdoor LiDAR point clouds is crucial to extendingthe 3D vision of collaborative autonomous vehicles, and yet is challenging dueto small overlapping area and a huge disparity between observed pointdensities. In this paper, we propose Group-wise Contrastive Learning (GCL)scheme to extract density-invariant geometric features to register distantoutdoor LiDAR point clouds. We mark through theoretical analysis andexperiments that, contrastive positives should be independent and identicallydistributed (i.i.d.), in order to train densityinvariant feature extractors. Wepropose upon the conclusion a simple yet effective training scheme to force thefeature of multiple point clouds in the same spatial location (referred to aspositive groups) to be similar, which naturally avoids the sampling biasintroduced by a pair of point clouds to conform with the i.i.d. principle. Theresulting fully-convolutional feature extractor is more powerful anddensity-invariant than state-of-the-art methods, improving the registrationrecall of distant scenarios on KITTI and nuScenes benchmarks by 40.9% and26.9%, respectively. Code is available at https://github.com/liuQuan98/GCL.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.09788v2 |
| 431 | Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation                                            | Changqi Wang                   | 2023-07-19     | cs.CV, cs.AI                            | Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation modelwith limited labeled images and a substantial volume of unlabeled images. Toimprove the robustness of representations, powerful methods introduce apixel-wise contrastive learning approach in latent space (i.e., representationspace) that aggregates the representations to their prototypes in a fullysupervised manner. However, previous contrastive-based S4 methods merely relyon the supervision from the model's output (logits) in logit space duringunlabeled training. In contrast, we utilize the outputs in both logit space andrepresentation space to obtain supervision in a collaborative way. Thesupervision from two spaces plays two roles: 1) reduces the risk ofover-fitting to incorrect semantic information in logits with the help ofrepresentations; 2) enhances the knowledge exchange between the two spaces.Furthermore, unlike previous approaches, we use the similarity betweenrepresentations and prototypes as a new indicator to tilt training thoseunder-performing representations and achieve a more efficient contrastivelearning process. Results on two public benchmarks demonstrate the competitiveperformance of our method compared with state-of-the-art methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.09755v1 |
| 432 | CPCM: Contextual Point Cloud Modeling for Weakly-supervised Point Cloud Semantic Segmentation                                                        | Lizhao Liu                     | 2023-07-19     | cs.CV, eess.IV                          | We study the task of weakly-supervised point cloud semantic segmentation withsparse annotations (e.g., less than 0.1% points are labeled), aiming to reducethe expensive cost of dense annotations. Unfortunately, with extremely sparseannotated points, it is very difficult to extract both contextual and objectinformation for scene understanding such as semantic segmentation. Motivated bymasked modeling (e.g., MAE) in image and video representation learning, we seekto endow the power of masked modeling to learn contextual information fromsparsely-annotated points. However, directly applying MAE to 3D point cloudswith sparse annotations may fail to work. First, it is nontrivial toeffectively mask out the informative visual context from 3D point clouds.Second, how to fully exploit the sparse annotations for context modelingremains an open question. In this paper, we propose a simple yet effectiveContextual Point Cloud Modeling (CPCM) method that consists of two parts: aregion-wise masking (RegionMask) strategy and a contextual masked training(CMT) method. Specifically, RegionMask masks the point cloud continuously ingeometric space to construct a meaningful masked prediction task for subsequentcontext learning. CMT disentangles the learning of supervised segmentation andunsupervised masked context prediction for effectively learning the verylimited labeled points and mass unlabeled points, respectively. Extensiveexperiments on the widely-tested ScanNet V2 and S3DIS benchmarks demonstratethe superiority of CPCM over the state-of-the-art.                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2307.10316v1 |
| 433 | AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks                                                                                           | Kibeom Hong                    | 2023-07-19     | cs.CV                                   | To deliver the artistic expression of the target style, recent studiesexploit the attention mechanism owing to its ability to map the local patchesof the style image to the corresponding patches of the content image. However,because of the low semantic correspondence between arbitrary content andartworks, the attention module repeatedly abuses specific local patches fromthe style image, resulting in disharmonious and evident repetitive artifacts.To overcome this limitation and accomplish impeccable artistic style transfer,we focus on enhancing the attention mechanism and capturing the rhythm ofpatterns that organize the style. In this paper, we introduce a novel metric,namely pattern repeatability, that quantifies the repetition of patterns in thestyle image. Based on the pattern repeatability, we propose AestheticPattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spotof local and global style expressions. In addition, we propose a novelself-supervisory task to encourage the attention mechanism to learn precise andmeaningful semantic correspondence. Lastly, we introduce the patch-wise styleloss to transfer the elaborate rhythm of local patterns. Through qualitativeand quantitative evaluations, we verify the reliability of the proposed patternrepeatability that aligns with human perception, and demonstrate thesuperiority of the proposed framework.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.09724v3 |
| 434 | Towards Saner Deep Image Registration                                                                                                                | Bin Duan                       | 2023-07-19     | cs.CV                                   | With recent advances in computing hardware and surges of deep-learningarchitectures, learning-based deep image registration methods have surpassedtheir traditional counterparts, in terms of metric performance and inferencetime. However, these methods focus on improving performance measurements suchas Dice, resulting in less attention given to model behaviors that are equallydesirable for registrations, especially for medical imaging. This paperinvestigates these behaviors for popular learning-based deep registrationsunder a sanity-checking microscope. We find that most existing registrationssuffer from low inverse consistency and nondiscrimination of identical pairsdue to overly optimized image similarities. To rectify these behaviors, wepropose a novel regularization-based sanity-enforcer method that imposes twosanity checks on the deep model to reduce its inverse consistency errors andincrease its discriminative power simultaneously. Moreover, we derive a set oftheoretical guarantees for our sanity-checked image registration method, withexperimental results supporting our theoretical findings and theireffectiveness in increasing the sanity of models without sacrificing anyperformance. Our code and models are available athttps://github.com/tuffr5/Saner-deep-registration.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.09696v2 |
| 435 | GlobalMapper: Arbitrary-Shaped Urban Layout Generation                                                                                               | Liu He                         | 2023-07-19     | cs.CV                                   | Modeling and designing urban building layouts is of significant interest incomputer vision, computer graphics, and urban applications. A building layoutconsists of a set of buildings in city blocks defined by a network of roads. Weobserve that building layouts are discrete structures, consisting of multiplerows of buildings of various shapes, and are amenable to skeletonization formapping arbitrary city block shapes to a canonical form. Hence, we propose afully automatic approach to building layout generation using graph attentionnetworks. Our method generates realistic urban layouts given arbitrary roadnetworks, and enables conditional generation based on learned priors. Ourresults, including user study, demonstrate superior performance as compared toprior layout generation networks, support arbitrary city block and varyingbuilding shapes as demonstrated by generating layouts for 28 large cities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.09693v1 |
| 436 | Object-aware Gaze Target Detection                                                                                                                   | Francesco Tonini               | 2023-07-18     | cs.CV                                   | Gaze target detection aims to predict the image location where the person islooking and the probability that a gaze is out of the scene. Several works havetackled this task by regressing a gaze heatmap centered on the gaze location,however, they overlooked decoding the relationship between the people and thegazed objects. This paper proposes a Transformer-based architecture thatautomatically detects objects (including heads) in the scene to buildassociations between every head and the gazed-head/object, resulting in acomprehensive, explainable gaze analysis composed of: gaze target area, gazepixel point, the class and the image location of the gazed-object. Uponevaluation of the in-the-wild benchmarks, our method achieves state-of-the-artresults on all metrics (up to 2.91% gain in AUC, 50% reduction in gazedistance, and 9% gain in out-of-frame average precision) for gaze targetdetection and 11-13% improvement in average precision for the classificationand the localization of the gazed-objects. The code of the proposed method isavailable https://github.com/francescotonini/object-aware-gaze-target-detection                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.09662v1 |
| 437 | Plug the Leaks: Advancing Audio-driven Talking Face Generation by Preventing Unintended Information Flow                                             | Dogucan Yaman                  | 2023-07-18     | cs.CV                                   | Audio-driven talking face generation is the task of creating alip-synchronized, realistic face video from given audio and reference frames.This involves two major challenges: overall visual quality of generated imageson the one hand, and audio-visual synchronization of the mouth part on theother hand. In this paper, we start by identifying several problematic aspectsof synchronization methods in recent audio-driven talking face generationapproaches. Specifically, this involves unintended flow of lip and poseinformation from the reference to the generated image, as well as instabilitiesduring model training. Subsequently, we propose various techniques forobviating these issues: First, a silent-lip reference image generator preventsleaking of lips from the reference to the generated image. Second, an adaptivetriplet loss handles the pose leaking problem. Finally, we propose a stabilizedformulation of synchronization loss, circumventing aforementioned traininginstabilities while additionally further alleviating the lip leaking issue.Combining the individual improvements, we present state-of-the art performanceon LRS2 and LRW in both synchronization and visual quality. We further validateour design in various ablation experiments, confirming the individualcontributions as well as their complementary effects.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.09368v1 |
| 438 | Efficient Region-Aware Neural Radiance Fields for High-Fidelity Talking Portrait Synthesis                                                           | Jiahe Li                       | 2023-07-18     | cs.CV                                   | This paper presents ER-NeRF, a novel conditional Neural Radiance Fields(NeRF) based architecture for talking portrait synthesis that can concurrentlyachieve fast convergence, real-time rendering, and state-of-the-art performancewith small model size. Our idea is to explicitly exploit the unequalcontribution of spatial regions to guide talking portrait modeling.Specifically, to improve the accuracy of dynamic head reconstruction, a compactand expressive NeRF-based Tri-Plane Hash Representation is introduced bypruning empty spatial regions with three planar hash encoders. For speechaudio, we propose a Region Attention Module to generate region-aware conditionfeature via an attention mechanism. Different from existing methods thatutilize an MLP-based encoder to learn the cross-modal relation implicitly, theattention mechanism builds an explicit connection between audio features andspatial regions to capture the priors of local motions. Moreover, a direct andfast Adaptive Pose Encoding is introduced to optimize the head-torso separationproblem by mapping the complex transformation of the head pose into spatialcoordinates. Extensive experiments demonstrate that our method renders betterhigh-fidelity and audio-lips synchronized talking portrait videos, withrealistic details and high efficiency compared to previous methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2307.09323v2 |
| 439 | EigenTrajectory: Low-Rank Descriptors for Multi-Modal Trajectory Forecasting                                                                         | Inhwan Bae                     | 2023-07-18     | cs.CV, cs.LG, cs.RO                     | Capturing high-dimensional social interactions and feasible futures isessential for predicting trajectories. To address this complex nature, severalattempts have been devoted to reducing the dimensionality of the outputvariables via parametric curve fitting such as the B\'ezier curve and B-splinefunction. However, these functions, which originate in computer graphicsfields, are not suitable to account for socially acceptable human dynamics. Inthis paper, we present EigenTrajectory ($\mathbb{ET}$), a trajectory predictionapproach that uses a novel trajectory descriptor to form a compact space, knownhere as $\mathbb{ET}$ space, in place of Euclidean space, for representingpedestrian movements. We first reduce the complexity of the trajectorydescriptor via a low-rank approximation. We transform the pedestrians' historypaths into our $\mathbb{ET}$ space represented by spatio-temporal principlecomponents, and feed them into off-the-shelf trajectory forecasting models. Theinputs and outputs of the models as well as social interactions are allgathered and aggregated in the corresponding $\mathbb{ET}$ space. Lastly, wepropose a trajectory anchor-based refinement method to cover all possiblefutures in the proposed $\mathbb{ET}$ space. Extensive experiments demonstratethat our EigenTrajectory predictor can significantly improve both theprediction accuracy and reliability of existing trajectory forecasting modelson public benchmarks, indicating that the proposed descriptor is suited torepresent pedestrian behaviors. Code is publicly available athttps://github.com/inhwanbae/EigenTrajectory .                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.09306v1 |
| 440 | Constraining Depth Map Geometry for Multi-View Stereo: A Dual-Depth Approach with Saddle-shaped Depth Cells                                          | Xinyi Ye                       | 2023-07-18     | cs.CV                                   | Learning-based multi-view stereo (MVS) methods deal with predicting accuratedepth maps to achieve an accurate and complete 3D representation. Despite theexcellent performance, existing methods ignore the fact that a suitable depthgeometry is also critical in MVS. In this paper, we demonstrate that differentdepth geometries have significant performance gaps, even using the same depthprediction error. Therefore, we introduce an ideal depth geometry composed ofSaddle-Shaped Cells, whose predicted depth map oscillates upward and downwardaround the ground-truth surface, rather than maintaining a continuous andsmooth depth plane. To achieve it, we develop a coarse-to-fine framework calledDual-MVSNet (DMVSNet), which can produce an oscillating depth plane.Technically, we predict two depth values for each pixel (Dual-Depth), andpropose a novel loss function and a checkerboard-shaped selecting strategy toconstrain the predicted depth geometry. Compared to existing methods,DMVSNetachieves a high rank on the DTU benchmark and obtains the top performance onchallenging scenes of Tanks and Temples, demonstrating its strong performanceand generalization ability. Our method also points to a new research directionfor considering depth geometry in MVS.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.09160v1 |
| 441 | Learning Adaptive Neighborhoods for Graph Neural Networks                                                                                            | Avishkar Saha                  | 2023-07-18     | cs.CV, cs.LG                            | Graph convolutional networks (GCNs) enable end-to-end learning on graphstructured data. However, many works assume a given graph structure. When theinput graph is noisy or unavailable, one approach is to construct or learn alatent graph structure. These methods typically fix the choice of node degreefor the entire graph, which is suboptimal. Instead, we propose a novelend-to-end differentiable graph generator which builds graph topologies whereeach node selects both its neighborhood and its size. Our module can be readilyintegrated into existing pipelines involving graph convolution operations,replacing the predetermined or existing adjacency matrix with one that islearned, and optimized, as part of the general objective. As such it isapplicable to any GCN. We integrate our module into trajectory prediction,point cloud classification and node classification pipelines resulting inimproved accuracy over other structure-learning methods across a wide range ofdatasets and GCN backbones.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2307.09065v1 |
| 442 | LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise                                                         | Zhiyu Wu                       | 2023-07-18     | cs.CV                                   | Facial expression recognition (FER) remains a challenging task due to theambiguity of expressions. The derived noisy labels significantly harm theperformance in real-world scenarios. To address this issue, we present a newFER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarksto mitigate the impact of label noise from two perspectives. Firstly, LA-Netuses landmark information to suppress the uncertainty in expression space andconstructs the label distribution of each sample by neighborhood aggregation,which in turn improves the quality of training supervision. Secondly, the modelincorporates landmark information into expression representations using thedevised expression-landmark contrastive loss. The enhanced expression featureextractor can be less susceptible to label noise. Our method can be integratedwith any deep neural network for better training supervision withoutintroducing extra inference costs. We conduct extensive experiments on bothin-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Netachieves state-of-the-art performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.09023v3 |
| 443 | Towards Authentic Face Restoration with Iterative Diffusion Models and Beyond                                                                        | Yang Zhao                      | 2023-07-18     | cs.CV                                   | An authentic face restoration system is becoming increasingly demanding inmany computer vision applications, e.g., image enhancement, videocommunication, and taking portrait. Most of the advanced face restorationmodels can recover high-quality faces from low-quality ones but usually fail tofaithfully generate realistic and high-frequency details that are favored byusers. To achieve authentic restoration, we propose $\textbf{IDM}$, an$\textbf{I}$teratively learned face restoration system based on denoising$\textbf{D}$iffusion $\textbf{M}$odels (DDMs). We define the criterion of anauthentic face restoration system, and argue that denoising diffusion modelsare naturally endowed with this property from two aspects: intrinsic iterativerefinement and extrinsic iterative enhancement. Intrinsic learning can preservethe content well and gradually refine the high-quality details, while extrinsicenhancement helps clean the data and improve the restoration task one stepfurther. We demonstrate superior performance on blind face restoration tasks.Beyond restoration, we find the authentically cleaned data by the proposedrestoration system is also helpful to image generation tasks in terms oftraining stabilization and sample quality. Without modifying the models, weachieve better quality than state-of-the-art on FFHQ and ImageNet generationusing either GANs or diffusion models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2307.08996v1 |
| 444 | What Can Simple Arithmetic Operations Do for Temporal Modeling?                                                                                      | Wenhao Wu                      | 2023-07-18     | cs.CV                                   | Temporal modeling plays a crucial role in understanding video content. Totackle this problem, previous studies built complicated temporal relationsthrough time sequence thanks to the development of computationally powerfuldevices. In this work, we explore the potential of four simple arithmeticoperations for temporal modeling. Specifically, we first capture auxiliarytemporal cues by computing addition, subtraction, multiplication, and divisionbetween pairs of extracted frame features. Then, we extract correspondingfeatures from these cues to benefit the original temporal-irrespective domain.We term such a simple pipeline as an Arithmetic Temporal Module (ATM), whichoperates on the stem of a visual backbone with a plug-and-play style. Weconduct comprehensive ablation studies on the instantiation of ATMs anddemonstrate that this module provides powerful temporal modeling capability ata low computational cost. Moreover, the ATM is compatible with both CNNs- andViTs-based architectures. Our results show that ATM achieves superiorperformance over several popular video benchmarks. Specifically, onSomething-Something V1, V2 and Kinetics-400, we reach top-1 accuracy of 65.6%,74.6%, and 89.4% respectively. The code is available athttps://github.com/whwu95/ATM.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.08908v2 |
| 445 | Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels                                                         | Yae Jee Cho                    | 2023-07-17     | cs.LG, cs.AI, cs.CV                     | Many existing FL methods assume clients with fully-labeled data, while inrealistic settings, clients have limited labels due to the expensive andlaborious process of labeling. Limited labeled local data of the clients oftenleads to their local model having poor generalization abilities to their largerunlabeled local data, such as having class-distribution mismatch with theunlabeled data. As a result, clients may instead look to benefit from theglobal model trained across clients to leverage their unlabeled data, but thisalso becomes difficult due to data heterogeneity across clients. In our work,we propose FedLabel where clients selectively choose the local or global modelto pseudo-label their unlabeled data depending on which is more of an expert ofthe data. We further utilize both the local and global models' knowledge viaglobal-local consistency regularization which minimizes the divergence betweenthe two models' outputs when they have identical pseudo-labels for theunlabeled data. Unlike other semi-supervised FL baselines, our method does notrequire additional experts other than the local or global model, nor requireadditional parameters to be communicated. We also do not assume anyserver-labeled data or fully labeled clients. For both cross-device andcross-silo settings, we show that FedLabel outperforms other semi-supervised FLbaselines by $8$-$24\%$, and even outperforms standard fully supervised FLbaselines ($100\%$ labeled data) with only $5$-$20\%$ of labeled data.                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.08809v1 |
| 446 | Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation                                                                                            | Rundong Luo                    | 2023-07-17     | cs.CV                                   | Low-light conditions not only hamper human visual experience but also degradethe model's performance on downstream vision tasks. While existing works makeremarkable progress on day-night domain adaptation, they rely heavily on domainknowledge derived from the task-specific nighttime dataset. This paperchallenges a more complicated scenario with border applicability, i.e.,zero-shot day-night domain adaptation, which eliminates reliance on anynighttime data. Unlike prior zero-shot adaptation approaches emphasizing eitherimage-level translation or model-level adaptation, we propose a similaritymin-max paradigm that considers them under a unified framework. On the imagelevel, we darken images towards minimum feature similarity to enlarge thedomain gap. Then on the model level, we maximize the feature similarity betweenthe darkened images and their normal-light counterparts for better modeladaptation. To the best of our knowledge, this work represents the pioneeringeffort in jointly optimizing both aspects, resulting in a significantimprovement of model generalizability. Extensive experiments demonstrate ourmethod's effectiveness and broad applicability on various nighttime visiontasks, including classification, semantic segmentation, visual placerecognition, and video action recognition. Code and pre-trained models areavailable at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.08779v2 |
| 447 | Scale-Aware Modulation Meet Transformer                                                                                                              | Weifeng Lin                    | 2023-07-17     | cs.CV                                   | This paper presents a new vision Transformer, Scale-Aware ModulationTransformer (SMT), that can handle various downstream tasks efficiently bycombining the convolutional network and vision Transformer. The proposedScale-Aware Modulation (SAM) in the SMT includes two primary novel designs.Firstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which cancapture multi-scale features and expand the receptive field. Secondly, wepropose the Scale-Aware Aggregation (SAA) module, which is lightweight buteffective, enabling information fusion across different heads. By leveragingthese two modules, convolutional modulation is further enhanced. Furthermore,in contrast to prior works that utilized modulations throughout all stages tobuild an attention-free network, we propose an Evolutionary Hybrid Network(EHN), which can effectively simulate the shift from capturing local to globaldependencies as the network becomes deeper, resulting in superior performance.Extensive experiments demonstrate that SMT significantly outperforms existingstate-of-the-art models across a wide range of visual tasks. Specifically, SMTwith 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1accuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned withresolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN,the SMT base trained with 1x and 3x schedule outperforms the Swin Transformercounterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentationwith UPerNet, the SMT base test at single- and multi-scale surpasses Swin by2.0 and 1.1 mIoU respectively on the ADE20K.                                                                                                                                                                            | http://arxiv.org/abs/2307.08579v2 |
| 448 | Cumulative Spatial Knowledge Distillation for Vision Transformers                                                                                    | Borui Zhao                     | 2023-07-17     | cs.CV                                   | Distilling knowledge from convolutional neural networks (CNNs) is adouble-edged sword for vision transformers (ViTs). It boosts the performancesince the image-friendly local-inductive bias of CNN helps ViT learn faster andbetter, but leading to two problems: (1) Network designs of CNN and ViT arecompletely different, which leads to different semantic levels of intermediatefeatures, making spatial-wise knowledge transfer methods (e.g., featuremimicking) inefficient. (2) Distilling knowledge from CNN limits the networkconvergence in the later training period since ViT's capability of integratingglobal information is suppressed by CNN's local-inductive-bias supervision. Tothis end, we present Cumulative Spatial Knowledge Distillation (CSKD). CSKDdistills spatial-wise knowledge to all patch tokens of ViT from thecorresponding spatial responses of CNN, without introducing intermediatefeatures. Furthermore, CSKD exploits a Cumulative Knowledge Fusion (CKF)module, which introduces the global response of CNN and increasingly emphasizesits importance during the training. Applying CKF leverages CNN's localinductive bias in the early training period and gives full play to ViT's globalcapability in the later one. Extensive experiments and analysis on ImageNet-1kand downstream datasets demonstrate the superiority of our CSKD. Code will bepublicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.08500v1 |
| 449 | SVDFormer: Complementing Point Cloud via Self-view Augmentation and Self-structure Dual-generator                                                    | Zhe Zhu                        | 2023-07-17     | cs.CV                                   | In this paper, we propose a novel network, SVDFormer, to tackle two specificchallenges in point cloud completion: understanding faithful global shapes fromincomplete point clouds and generating high-accuracy local structures. Currentmethods either perceive shape patterns using only 3D coordinates or importextra images with well-calibrated intrinsic parameters to guide the geometryestimation of the missing parts. However, these approaches do not always fullyleverage the cross-modal self-structures available for accurate andhigh-quality point cloud completion. To this end, we first design a Self-viewFusion Network that leverages multiple-view depth image information to observeincomplete self-shape and generate a compact global shape. To reveal highlydetailed structures, we then introduce a refinement module, calledSelf-structure Dual-generator, in which we incorporate learned shape priors andgeometric self-similarities for producing new points. By perceiving theincompleteness of each point, the dual-path design disentangles refinementstrategies conditioned on the structural type of each point. SVDFormer absorbsthe wisdom of self-structures, avoiding any additional paired information suchas color images with precisely calibrated camera intrinsic parameters.Comprehensive experiments indicate that our method achieves state-of-the-artperformance on widely-used benchmarks. Code will be available athttps://github.com/czvvd/SVDFormer.                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.08492v2 |
| 450 | Differentiable Transportation Pruning                                                                                                                | Yunqiang Li                    | 2023-07-17     | cs.CV                                   | Deep learning algorithms are increasingly employed at the edge. However, edgedevices are resource constrained and thus require efficient deployment of deepneural networks. Pruning methods are a key tool for edge deployment as they canimprove storage, compute, memory bandwidth, and energy usage. In this paper wepropose a novel accurate pruning technique that allows precise control over theoutput network size. Our method uses an efficient optimal transportation schemewhich we make end-to-end differentiable and which automatically tunes theexploration-exploitation behavior of the algorithm to find accurate sparsesub-networks. We show that our method achieves state-of-the-art performancecompared to previous pruning methods on 3 different datasets, using 5 differentmodels, across a wide range of pruning ratios, and with two types of sparsitybudgets and pruning granularities.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.08483v2 |
| 451 | SkeletonMAE: Graph-based Masked Autoencoder for Skeleton Sequence Pre-training                                                                       | Hong Yan                       | 2023-07-17     | cs.CV                                   | Skeleton sequence representation learning has shown great advantages foraction recognition due to its promising ability to model human joints andtopology. However, the current methods usually require sufficient labeled datafor training computationally expensive models, which is labor-intensive andtime-consuming. Moreover, these methods ignore how to utilize the fine-graineddependencies among different skeleton joints to pre-train an efficient skeletonsequence learning model that can generalize well across different datasets. Inthis paper, we propose an efficient skeleton sequence learning framework, namedSkeleton Sequence Learning (SSL). To comprehensively capture the human pose andobtain discriminative skeleton sequence representation, we build an asymmetricgraph-based encoder-decoder pre-training architecture named SkeletonMAE, whichembeds skeleton joint sequence into Graph Convolutional Network (GCN) andreconstructs the masked skeleton joints and edges based on the prior humantopology knowledge. Then, the pre-trained SkeletonMAE encoder is integratedwith the Spatial-Temporal Representation Learning (STRL) module to build theSSL framework. Extensive experimental results show that our SSL generalizeswell across different datasets and outperforms the state-of-the-artself-supervised skeleton-based action recognition methods on FineGym, Diving48,NTU 60 and NTU 120 datasets. Additionally, we obtain comparable performance tosome fully supervised methods. The code is avaliable athttps://github.com/HongYan1123/SkeletonMAE.                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.08476v1 |
| 452 | DOT: A Distillation-Oriented Trainer                                                                                                                 | Borui Zhao                     | 2023-07-17     | cs.CV                                   | Knowledge distillation transfers knowledge from a large model to a small onevia task and distillation losses. In this paper, we observe a trade-off betweentask and distillation losses, i.e., introducing distillation loss limits theconvergence of task loss. We believe that the trade-off results from theinsufficient optimization of distillation loss. The reason is: The teacher hasa lower task loss than the student, and a lower distillation loss drives thestudent more similar to the teacher, then a better-converged task loss could beobtained. To break the trade-off, we propose the Distillation-Oriented Trainer(DOT). DOT separately considers gradients of task and distillation losses, thenapplies a larger momentum to distillation loss to accelerate its optimization.We empirically prove that DOT breaks the trade-off, i.e., both losses aresufficiently optimized. Extensive experiments validate the superiority of DOT.Notably, DOT achieves a +2.59% accuracy improvement on ImageNet-1k for theResNet50-MobileNetV1 pair. Conclusively, DOT greatly benefits the student'soptimization properties in terms of loss convergence and model generalization.Code will be made publicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.08436v1 |
| 453 | Dynamic Snake Convolution based on Topological Geometric Constraints for Tubular Structure Segmentation                                              | Yaolei Qi                      | 2023-07-17     | cs.CV, eess.IV                          | Accurate segmentation of topological tubular structures, such as bloodvessels and roads, is crucial in various fields, ensuring accuracy andefficiency in downstream tasks. However, many factors complicate the task,including thin local structures and variable global morphologies. In this work,we note the specificity of tubular structures and use this knowledge to guideour DSCNet to simultaneously enhance perception in three stages: featureextraction, feature fusion, and loss constraint. First, we propose a dynamicsnake convolution to accurately capture the features of tubular structures byadaptively focusing on slender and tortuous local structures. Subsequently, wepropose a multi-view feature fusion strategy to complement the attention tofeatures from multiple perspectives during feature fusion, ensuring theretention of important information from different global morphologies. Finally,a continuity constraint loss function, based on persistent homology, isproposed to constrain the topological continuity of the segmentation better.Experiments on 2D and 3D datasets show that our DSCNet provides better accuracyand continuity on the tubular structure segmentation task compared with severalmethods. Our codes will be publicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2307.08388v2 |
| 454 | Self-supervised Monocular Depth Estimation: Let's Talk About The Weather                                                                             | Kieran Saunders                | 2023-07-17     | cs.CV                                   | Current, self-supervised depth estimation architectures rely on clear andsunny weather scenes to train deep neural networks. However, in many locations,this assumption is too strong. For example in the UK (2021), 149 days consistedof rain. For these architectures to be effective in real-world applications, wemust create models that can generalise to all weather conditions, times of theday and image qualities. Using a combination of computer graphics andgenerative models, one can augment existing sunny-weather data in a variety ofways that simulate adverse weather effects. While it is tempting to use suchdata augmentations for self-supervised depth, in the past this was shown todegrade performance instead of improving it. In this paper, we put forward amethod that uses augmentations to remedy this problem. By exploiting thecorrespondence between unaugmented and augmented data we introduce apseudo-supervised loss for both depth and pose estimation. This brings backsome of the benefits of supervised learning while still not requiring anylabels. We also make a series of practical recommendations which collectivelyoffer a reliable, efficient framework for weather-related augmentation ofself-supervised depth from monocular video. We present extensive testing toshow that our method, Robust-Depth, achieves SotA performance on the KITTIdataset while significantly surpassing SotA on challenging, adverse conditiondata such as DrivingStereo, Foggy CityScape and NuScenes-Night. The projectwebsite can be found here https://kieran514.github.io/Robust-Depth-Project/.                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.08357v1 |
| 455 | ShiftNAS: Improving One-shot NAS via Probability Shift                                                                                               | Mingyang Zhang                 | 2023-07-17     | cs.CV, cs.AI                            | One-shot Neural architecture search (One-shot NAS) has been proposed as atime-efficient approach to obtain optimal subnet architectures and weightsunder different complexity cases by training only once. However, the subnetperformance obtained by weight sharing is often inferior to the performanceachieved by retraining. In this paper, we investigate the performance gap andattribute it to the use of uniform sampling, which is a common approach insupernet training. Uniform sampling concentrates training resources on subnetswith intermediate computational resources, which are sampled with highprobability. However, subnets with different complexity regions requiredifferent optimal training strategies for optimal performance. To address theproblem of uniform sampling, we propose ShiftNAS, a method that can adjust thesampling probability based on the complexity of subnets. We achieve this byevaluating the performance variation of subnets with different complexity anddesigning an architecture generator that can accurately and efficiently providesubnets with the desired complexity. Both the sampling probability and thearchitecture generator can be trained end-to-end in a gradient-based manner.With ShiftNAS, we can directly obtain the optimal model architecture andparameters for a given computational complexity. We evaluate our approach onmultiple visual network models, including convolutional neural networks (CNNs)and vision transformers (ViTs), and demonstrate that ShiftNAS ismodel-agnostic. Experimental results on ImageNet show that ShiftNAS can improvethe performance of one-shot NAS without additional consumption. Source codesare available at https://github.com/bestfleer/ShiftNAS.                                                                                                                                                                                       | http://arxiv.org/abs/2307.08300v1 |
| 456 | Large-Scale Person Detection and Localization using Overhead Fisheye Cameras                                                                         | Lu Yang                        | 2023-07-17     | cs.CV                                   | Location determination finds wide applications in daily life. Instead ofexisting efforts devoted to localizing tourist photos captured by perspectivecameras, in this article, we focus on devising person positioning solutionsusing overhead fisheye cameras. Such solutions are advantageous in large fieldof view (FOV), low cost, anti-occlusion, and unaggressive work mode (withoutthe necessity of cameras carried by persons). However, related studies arequite scarce, due to the paucity of data. To stimulate research in thisexciting area, we present LOAF, the first large-scale overhead fisheye datasetfor person detection and localization. LOAF is built with many essentialfeatures, e.g., i) the data cover abundant diversities in scenes, human pose,density, and location; ii) it contains currently the largest number ofannotated pedestrian, i.e., 457K bounding boxes with groundtruth locationinformation; iii) the body-boxes are labeled as radius-aligned so as to fullyaddress the positioning challenge. To approach localization, we build a fisheyeperson detection network, which exploits the fisheye distortions by arotation-equivariant training strategy and predict radius-aligned human boxesend-to-end. Then, the actual locations of the detected persons are calculatedby a numerical solution on the fisheye model and camera altitude data.Extensive experiments on LOAF validate the superiority of our fisheye detectorw.r.t. previous methods, and show that our whole fisheye positioning solutionis able to locate all persons in FOV with an accuracy of 0.5 m, within 0.1 s.                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.08252v1 |
| 457 | Random Boxes Are Open-world Object Detectors                                                                                                         | Yanghao Wang                   | 2023-07-17     | cs.CV                                   | We show that classifiers trained with random region proposals achievestate-of-the-art Open-world Object Detection (OWOD): they can not only maintainthe accuracy of the known objects (w/ training labels), but also considerablyimprove the recall of unknown ones (w/o training labels). Specifically, wepropose RandBox, a Fast R-CNN based architecture trained on random proposals ateach training iteration, surpassing existing Faster R-CNN and Transformer basedOWOD. Its effectiveness stems from the following two benefits introduced byrandomness. First, as the randomization is independent of the distribution ofthe limited known objects, the random proposals become the instrumentalvariable that prevents the training from being confounded by the known objects.Second, the unbiased training encourages more proposal explorations by usingour proposed matching score that does not penalize the random proposals whoseprediction scores do not match the known objects. On two benchmarks:Pascal-VOC/MS-COCO and LVIS, RandBox significantly outperforms the previousstate-of-the-art in all metrics. We also detail the ablations on randomizationand loss designs. Codes are available at https://github.com/scuwyh2000/RandBox.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.08249v1 |
| 458 | Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting                                                              | Wentao Bao                     | 2023-07-17     | cs.CV                                   | Hand trajectory forecasting from egocentric views is crucial for enabling aprompt understanding of human intentions when interacting with AR/VR systems.However, existing methods handle this problem in a 2D image space which isinadequate for 3D real-world applications. In this paper, we set up anegocentric 3D hand trajectory forecasting task that aims to predict handtrajectories in a 3D space from early observed RGB videos in a first-personview. To fulfill this goal, we propose an uncertainty-aware state spaceTransformer (USST) that takes the merits of the attention mechanism andaleatoric uncertainty within the framework of the classical state-space model.The model can be further enhanced by the velocity constraint and visual prompttuning (VPT) on large vision transformers. Moreover, we develop an annotationworkflow to collect 3D hand trajectories with high quality. Experimentalresults on H2O and EgoPAT3D datasets demonstrate the superiority of USST forboth 2D and 3D trajectory forecasting. The code and datasets are publiclyreleased: https://github.com/Cogito2012/USST.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.08243v1 |
| 459 | Cross-Ray Neural Radiance Fields for Novel-view Synthesis from Unconstrained Image Collections                                                       | Yifan Yang                     | 2023-07-16     | cs.CV                                   | Neural Radiance Fields (NeRF) is a revolutionary approach for renderingscenes by sampling a single ray per pixel and it has demonstrated impressivecapabilities in novel-view synthesis from static scene images. However, inpractice, we usually need to recover NeRF from unconstrained image collections,which poses two challenges: 1) the images often have dynamic changes inappearance because of different capturing time and camera settings; 2) theimages may contain transient objects such as humans and cars, leading toocclusion and ghosting artifacts. Conventional approaches seek to address thesechallenges by locally utilizing a single ray to synthesize a color of a pixel.In contrast, humans typically perceive appearance and objects by globallyutilizing information across multiple pixels. To mimic the perception processof humans, in this paper, we propose Cross-Ray NeRF (CR-NeRF) that leveragesinteractive information across multiple rays to synthesize occlusion-free novelviews with the same appearances as the images. Specifically, to model varyingappearances, we first propose to represent multiple rays with a novel cross-rayfeature and then recover the appearance by fusing global statistics, i.e.,feature covariance of the rays and the image appearance. Moreover, to avoidocclusion introduced by transient objects, we propose a transient objectshandler and introduce a grid sampling strategy for masking out the transientobjects. We theoretically find that leveraging correlation across multiple rayspromotes capturing more global information. Moreover, extensive experimentalresults on large real-world datasets verify the effectiveness of CR-NeRF.                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.08093v2 |
| 460 | Multi-Object Discovery by Low-Dimensional Object Motion                                                                                              | Sadra Safadoust                | 2023-07-16     | cs.CV                                   | Recent work in unsupervised multi-object segmentation shows impressiveresults by predicting motion from a single image despite the inherent ambiguityin predicting motion without the next image. On the other hand, the set ofpossible motions for an image can be constrained to a low-dimensional space byconsidering the scene structure and moving objects in it. We propose to modelpixel-wise geometry and object motion to remove ambiguity in reconstructingflow from a single image. Specifically, we divide the image into coherentlymoving regions and use depth to construct flow bases that best explain theobserved flow in each region. We achieve state-of-the-art results inunsupervised multi-object segmentation on synthetic and real-world datasets bymodeling the scene structure and object motion. Our evaluation of the predicteddepth maps shows reliable performance in monocular depth estimation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2307.08027v1 |
| 461 | Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer                                           | Yujiao Shi                     | 2023-07-16     | cs.CV                                   | Image retrieval-based cross-view localization methods often lead to verycoarse camera pose estimation, due to the limited sampling density of thedatabase satellite images. In this paper, we propose a method to increase theaccuracy of a ground camera's location and orientation by estimating therelative rotation and translation between the ground-level image and itsmatched/retrieved satellite image. Our approach designs a geometry-guidedcross-view transformer that combines the benefits of conventional geometry andlearnable cross-view transformers to map the ground-view observations to anoverhead view. Given the synthesized overhead view and observed satellitefeature maps, we construct a neural pose optimizer with strong globalinformation embedding ability to estimate the relative rotation between them.After aligning their rotations, we develop an uncertainty-guided spatialcorrelation to generate a probability map of the vehicle locations, from whichthe relative translation can be determined. Experimental results demonstratethat our method significantly outperforms the state-of-the-art. Notably, thelikelihood of restricting the vehicle lateral pose to be within 1m of itsGround Truth (GT) value on the cross-view KITTI dataset has been improved from$35.54\%$ to $76.44\%$, and the likelihood of restricting the vehicleorientation to be within $1^{\circ}$ of its GT value has been improved from$19.64\%$ to $99.10\%$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.08015v3 |
| 462 | MoTIF: Learning Motion Trajectories with Local Implicit Neural Functions for Continuous Space-Time Video Super-Resolution                            | Yi-Hsin Chen                   | 2023-07-16     | eess.IV                                 | This work addresses continuous space-time video super-resolution (C-STVSR)that aims to up-scale an input video both spatially and temporally by anyscaling factors. One key challenge of C-STVSR is to propagate informationtemporally among the input video frames. To this end, we introduce a space-timelocal implicit neural function. It has the striking feature of learning forwardmotion for a continuum of pixels. We motivate the use of forward motion fromthe perspective of learning individual motion trajectories, as opposed tolearning a mixture of motion trajectories with backward motion. To ease motioninterpolation, we encode sparsely sampled forward motion extracted from theinput video as the contextual input. Along with a reliability-aware splattingand decoding scheme, our framework, termed MoTIF, achieves the state-of-the-artperformance on C-STVSR. The source code of MoTIF is available athttps://github.com/sichun233746/MoTIF.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2307.07988v1 |
| 463 | Towards Viewpoint-Invariant Visual Recognition via Adversarial Training                                                                              | Shouwei Ruan                   | 2023-07-16     | cs.CV                                   | Visual recognition models are not invariant to viewpoint changes in the 3Dworld, as different viewing directions can dramatically affect the predictionsgiven the same object. Although many efforts have been devoted to making neuralnetworks invariant to 2D image translations and rotations, viewpoint invarianceis rarely investigated. As most models process images in the perspective view,it is challenging to impose invariance to 3D viewpoint changes based only on 2Dinputs. Motivated by the success of adversarial training in promoting modelrobustness, we propose Viewpoint-Invariant Adversarial Training (VIAT) toimprove viewpoint robustness of common image classifiers. By regardingviewpoint transformation as an attack, VIAT is formulated as a minimaxoptimization problem, where the inner maximization characterizes diverseadversarial viewpoints by learning a Gaussian mixture distribution based on anew attack GMVFool, while the outer minimization trains a viewpoint-invariantclassifier by minimizing the expected loss over the worst-case adversarialviewpoint distributions. To further improve the generalization performance, adistribution sharing strategy is introduced leveraging the transferability ofadversarial viewpoints across objects. Experiments validate the effectivenessof VIAT in improving the viewpoint robustness of various image classifiersbased on the diversity of adversarial viewpoints generated by GMVFool.                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2307.10235v1 |
| 464 | Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling                                               | Zhuoxiao Chen                  | 2023-07-16     | cs.CV, cs.AI, cs.LG                     | Unsupervised domain adaptation (DA) with the aid of pseudo labelingtechniques has emerged as a crucial approach for domain-adaptive 3D objectdetection. While effective, existing DA methods suffer from a substantial dropin performance when applied to a multi-class training setting, due to theco-existence of low-quality pseudo labels and class imbalance issues. In thispaper, we address this challenge by proposing a novel ReDB framework tailoredfor learning to detect all classes at once. Our approach produces Reliable,Diverse, and class-Balanced pseudo 3D boxes to iteratively guide theself-training on a distributionally different target domain. To alleviatedisruptions caused by the environmental discrepancy (e.g., beam numbers), theproposed cross-domain examination (CDE) assesses the correctness of pseudolabels by copy-pasting target instances into a source environment and measuringthe prediction consistency. To reduce computational overhead and mitigate theobject shift (e.g., scales and point densities), we design an overlapped boxescounting (OBC) metric that allows to uniformly downsample pseudo-labeledobjects across different geometric characteristics. To confront the issue ofinter-class imbalance, we progressively augment the target point clouds with aclass-balanced set of pseudo-labeled target instances and source objects, whichboosts recognition accuracies on both frequently appearing and rare classes.Experimental results on three benchmark datasets using both voxel-based (i.e.,SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that ourproposed ReDB approach outperforms existing 3D domain adaptation methods by alarge margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task.The code is available at https://github.com/zhuoxiao-chen/ReDB-DA-3Ddet.                                                                                | http://arxiv.org/abs/2307.07944v3 |
| 465 | KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection                                                                                | Yadan Luo                      | 2023-07-16     | cs.CV, cs.AI, cs.LG                     | Achieving a reliable LiDAR-based object detector in autonomous driving isparamount, but its success hinges on obtaining large amounts of precise 3Dannotations. Active learning (AL) seeks to mitigate the annotation burdenthrough algorithms that use fewer labels and can attain performance comparableto fully supervised learning. Although AL has shown promise, current approachesprioritize the selection of unlabeled point clouds with high uncertainty and/ordiversity, leading to the selection of more instances for labeling and reducedcomputational efficiency. In this paper, we resort to a novel kernel codingrate maximization (KECOR) strategy which aims to identify the most informativepoint clouds to acquire labels through the lens of information theory. Greedysearch is applied to seek desired point clouds that can maximize the minimalnumber of bits required to encode the latent features. To determine theuniqueness and informativeness of the selected samples from the modelperspective, we construct a proxy network of the 3D detector head and computethe outer product of Jacobians from all proxy layers to form the empiricalneural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e.,SECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate theclassification entropy maximization and well trade-off between detectionperformance and the total number of bounding boxes selected for annotation.Extensive experiments conducted on two 3D benchmarks and a 2D detection datasetevidence the superiority and versatility of the proposed approach. Our resultsshow that approximately 44% box-level annotation costs and 26% computationaltime are reduced compared to the state-of-the-art AL method, withoutcompromising detection performance.                                                                                                                              | http://arxiv.org/abs/2307.07942v1 |
| 466 | Reinforced Disentanglement for Face Swapping without Skip Connection                                                                                 | Xiaohang Ren                   | 2023-07-16     | cs.CV                                   | The SOTA face swap models still suffer the problem of either target identity(i.e., shape) being leaked or the target non-identity attributes (i.e.,background, hair) failing to be fully preserved in the final results. We showthat this insufficient disentanglement is caused by two flawed designs thatwere commonly adopted in prior models: (1) counting on only one compressedencoder to represent both the semantic-level non-identity facialattributes(i.e., pose) and the pixel-level non-facial region details, which iscontradictory to satisfy at the same time; (2) highly relying on longskip-connections between the encoder and the final generator, leaking a certainamount of target face identity into the result. To fix them, we introduce a newface swap framework called 'WSC-swap' that gets rid of skip connections anduses two target encoders to respectively capture the pixel-level non-facialregion attributes and the semantic non-identity attributes in the face region.To further reinforce the disentanglement learning for the target encoder, weemploy both identity removal loss via adversarial training (i.e., GAN) and thenon-identity preservation loss via prior 3DMM models like [11]. Extensiveexperiments on both FaceForensics++ and CelebA-HQ show that our resultssignificantly outperform previous works on a rich set of metrics, including onenovel metric for measuring identity consistency that was completely neglectedbefore.                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.07928v4 |
| 467 | Handwritten and Printed Text Segmentation: A Signature Case Study                                                                                    | Sina Gholamian                 | 2023-07-15     | cs.CV, cs.AI, cs.LG                     | While analyzing scanned documents, handwritten text can overlap with printedtext. This overlap causes difficulties during the optical character recognition(OCR) and digitization process of documents, and subsequently, hurts downstreamNLP tasks. Prior research either focuses solely on the binary classification ofhandwritten text or performs a three-class segmentation of the document, i.e.,recognition of handwritten, printed, and background pixels. This approachresults in the assignment of overlapping handwritten and printed pixels to onlyone of the classes, and thus, they are not accounted for in the other class.Thus, in this research, we develop novel approaches to address the challengesof handwritten and printed text segmentation. Our objective is to recover textfrom different classes in their entirety, especially enhancing the segmentationperformance on overlapping sections. To support this task, we introduce a newdataset, SignaTR6K, collected from real legal documents, as well as a new modelarchitecture for the handwritten and printed text segmentation task. Our bestconfiguration outperforms prior work on two different datasets by 17.9% and7.3% on IoU scores. The SignaTR6K dataset is accessible for download via thefollowing link: https://forms.office.com/r/2a5RDg7cAY.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2307.07887v3 |
| 468 | Adaptive Nonlinear Latent Transformation for Conditional Face Editing                                                                                | Zhizhong Huang                 | 2023-07-15     | cs.CV                                   | Recent works for face editing usually manipulate the latent space of StyleGANvia the linear semantic directions. However, they usually suffer from theentanglement of facial attributes, need to tune the optimal editing strength,and are limited to binary attributes with strong supervision signals. Thispaper proposes a novel adaptive nonlinear latent transformation fordisentangled and conditional face editing, termed AdaTrans. Specifically, ourAdaTrans divides the manipulation process into several finer steps; i.e., thedirection and size at each step are conditioned on both the facial attributesand the latent codes. In this way, AdaTrans describes an adaptive nonlineartransformation trajectory to manipulate the faces into target attributes whilekeeping other attributes unchanged. Then, AdaTrans leverages a predefineddensity model to constrain the learned trajectory in the distribution of latentcodes by maximizing the likelihood of transformed latent code. Moreover, wealso propose a disentangled learning strategy under a mutual informationframework to eliminate the entanglement among attributes, which can furtherrelax the need for labeled data. Consequently, AdaTrans enables a controllableface editing with the advantages of disentanglement, flexibility withnon-binary attributes, and high fidelity. Extensive experimental results onvarious facial attributes demonstrate the qualitative and quantitativeeffectiveness of the proposed AdaTrans over existing state-of-the-art methods,especially in the most challenging scenarios with a large age gap and fewlabeled examples. The source code is available athttps://github.com/Hzzone/AdaTrans.                                                                                                                                                                                                                                         | http://arxiv.org/abs/2307.07790v1 |
| 469 | Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer                                                                     | Wing-Yin Yu                    | 2023-07-15     | cs.CV, cs.AI                            | Video-based human pose transfer is a video-to-video generation task thatanimates a plain source human image based on a series of target human poses.Considering the difficulties in transferring highly structural patterns on thegarments and discontinuous poses, existing methods often generateunsatisfactory results such as distorted textures and flickering artifacts. Toaddress these issues, we propose a novel Deformable Motion Modulation (DMM)that utilizes geometric kernel offset with adaptive weight modulation tosimultaneously perform feature alignment and style transfer. Different fromnormal style modulation used in style transfer, the proposed modulationmechanism adaptively reconstructs smoothed frames from style codes according tothe object shape through an irregular receptive field of view. To enhance thespatio-temporal consistency, we leverage bidirectional propagation to extractthe hidden motion information from a warped image sequence generated by noisyposes. The proposed feature propagation significantly enhances the motionprediction ability by forward and backward propagation. Both quantitative andqualitative experimental results demonstrate superiority over thestate-of-the-arts in terms of image fidelity and visual continuity. The sourcecode is publicly available at github.com/rocketappslab/bdmm.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2307.07754v2 |
| 470 | SINC: Self-Supervised In-Context Learning for Vision-Language Tasks                                                                                  | Yi-Syuan Chen                  | 2023-07-15     | cs.CV, cs.AI                            | Large Pre-trained Transformers exhibit an intriguing capacity for in-contextlearning. Without gradient updates, these models can rapidly construct newpredictors from demonstrations presented in the inputs. Recent works promotethis ability in the vision-language domain by incorporating visual informationinto large language models that can already make in-context predictions.However, these methods could inherit issues in the language domain, such astemplate sensitivity and hallucination. Also, the scale of these languagemodels raises a significant demand for computations, making learning andoperating these models resource-intensive. To this end, we raise a question:``How can we enable in-context learning without relying on the intrinsicin-context ability of large language models?". To answer it, we propose asuccinct and general framework, Self-supervised IN-Context learning (SINC),that introduces a meta-model to learn on self-supervised prompts consisting oftailored demonstrations. The learned models can be transferred to downstreamtasks for making in-context predictions on-the-fly. Extensive experiments showthat SINC outperforms gradient-based methods in various vision-language tasksunder few-shot settings. Furthermore, the designs of SINC help us investigatethe benefits of in-context learning across different tasks, and the analysisfurther reveals the essential components for the emergence of in-contextlearning in the vision-language domain.                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2307.07742v2 |
| 471 | Neural Deformable Models for 3D Bi-Ventricular Heart Shape Reconstruction and Modeling from 2D Sparse Cardiac Magnetic Resonance Imaging             | Meng Ye                        | 2023-07-15     | cs.CV                                   | We propose a novel neural deformable model (NDM) targeting at thereconstruction and modeling of 3D bi-ventricular shape of the heart from 2Dsparse cardiac magnetic resonance (CMR) imaging data. We model thebi-ventricular shape using blended deformable superquadrics, which areparameterized by a set of geometric parameter functions and are capable ofdeforming globally and locally. While global geometric parameter functions anddeformations capture gross shape features from visual data, local deformations,parameterized as neural diffeomorphic point flows, can be learned to recoverthe detailed heart shape.Different from iterative optimization methods used inconventional deformable model formulations, NDMs can be trained to learn suchgeometric parameter functions, global and local deformations from a shapedistribution manifold. Our NDM can learn to densify a sparse cardiac pointcloud with arbitrary scales and generate high-quality triangular meshesautomatically. It also enables the implicit learning of dense correspondencesamong different heart shape instances for accurate cardiac shape registration.Furthermore, the parameters of NDM are intuitive, and can be used by aphysician without sophisticated post-processing. Experimental results on alarge CMR dataset demonstrate the improved performance of NDM over conventionalmethods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2307.07693v2 |
| 472 | TALL: Thumbnail Layout for Deepfake Video Detection                                                                                                  | Yuting Xu                      | 2023-07-14     | cs.CV                                   | The growing threats of deepfakes to society and cybersecurity have raisedenormous public concerns, and increasing efforts have been devoted to thiscritical topic of deepfake video detection. Existing video methods achieve goodperformance but are computationally intensive. This paper introduces a simpleyet effective strategy named Thumbnail Layout (TALL), which transforms a videoclip into a pre-defined layout to realize the preservation of spatial andtemporal dependencies. Specifically, consecutive frames are masked in a fixedposition in each frame to improve generalization, then resized to sub-imagesand rearranged into a pre-defined layout as the thumbnail. TALL ismodel-agnostic and extremely simple by only modifying a few lines of code.Inspired by the success of vision transformers, we incorporate TALL into SwinTransformer, forming an efficient and effective method TALL-Swin. Extensiveexperiments on intra-dataset and cross-dataset validate the validity andsuperiority of TALL and SOTA TALL-Swin. TALL-Swin achieves 90.79$\%$ AUC on thechallenging cross-dataset task, FaceForensics++ $\to$ Celeb-DF. The code isavailable at https://github.com/rainy-xu/TALL4Deepfake.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.07494v2 |
| 473 | Multimodal Distillation for Egocentric Action Recognition                                                                                            | Gorjan Radevski                | 2023-07-14     | cs.CV                                   | The focal point of egocentric video understanding is modelling hand-objectinteractions. Standard models, e.g. CNNs or Vision Transformers, which receiveRGB frames as input perform well. However, their performance improves furtherby employing additional input modalities that provide complementary cues, suchas object detections, optical flow, audio, etc. The added complexity of themodality-specific modules, on the other hand, makes these models impracticalfor deployment. The goal of this work is to retain the performance of such amultimodal approach, while using only the RGB frames as input at inferencetime. We demonstrate that for egocentric action recognition on theEpic-Kitchens and the Something-Something datasets, students which are taughtby multimodal teachers tend to be more accurate and better calibrated thanarchitecturally equivalent models trained on ground truth labels in a unimodalor multimodal fashion. We further adopt a principled multimodal knowledgedistillation framework, allowing us to deal with issues which occur whenapplying multimodal knowledge distillation in a naive manner. Lastly, wedemonstrate the achieved reduction in computational complexity, and show thatour approach maintains higher performance with the reduction of the number ofinput views. We release our code athttps://github.com/gorjanradevski/multimodal-distillation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2307.07483v2 |
| 474 | Improving Zero-Shot Generalization for CLIP with Synthesized Prompts                                                                                 | Zhengbo Wang                   | 2023-07-14     | cs.CV, cs.LG                            | With the growing interest in pretrained vision-language models like CLIP,recent research has focused on adapting these models to downstream tasks.Despite achieving promising results, most existing methods require labeled datafor all classes, which may not hold in real-world applications due to the longtail and Zipf's law. For example, some classes may lack labeled data entirely,such as emerging concepts. To address this problem, we propose a plug-and-playgenerative approach called \textbf{S}ynt\textbf{H}es\textbf{I}zed\textbf{P}rompts~(\textbf{SHIP}) to improve existing fine-tuning methods.Specifically, we follow variational autoencoders to introduce a generator thatreconstructs the visual features by inputting the synthesized prompts and thecorresponding class names to the textual encoder of CLIP. In this manner, weeasily obtain the synthesized features for the remaining label-only classes.Thereafter, we fine-tune CLIP with off-the-shelf methods by combining labeledand synthesized features. Extensive experiments on base-to-new generalization,cross-dataset transfer learning, and generalized zero-shot learning demonstratethe superiority of our approach. The code is available at\url{https://github.com/mrflogs/SHIP}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2307.07397v1 |
| 475 | Mitigating Adversarial Vulnerability through Causal Parameter Estimation by Adversarial Double Machine Learning                                      | Byung-Kwan Lee                 | 2023-07-14     | cs.LG, cs.AI, cs.CV                     | Adversarial examples derived from deliberately crafted perturbations onvisual inputs can easily harm decision process of deep neural networks. Toprevent potential threats, various adversarial training-based defense methodshave grown rapidly and become a de facto standard approach for robustness.Despite recent competitive achievements, we observe that adversarialvulnerability varies across targets and certain vulnerabilities remainprevalent. Intriguingly, such peculiar phenomenon cannot be relieved even withdeeper architectures and advanced defense methods. To address this issue, inthis paper, we introduce a causal approach called Adversarial Double MachineLearning (ADML), which allows us to quantify the degree of adversarialvulnerability for network predictions and capture the effect of treatments onoutcome of interests. ADML can directly estimate causal parameter ofadversarial perturbations per se and mitigate negative effects that canpotentially damage robustness, bridging a causal perspective into theadversarial vulnerability. Through extensive experiments on various CNN andTransformer architectures, we corroborate that ADML improves adversarialrobustness with large margins and relieve the empirical observation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2307.07250v2 |
| 476 | FreeCOS: Self-Supervised Learning from Fractals and Unlabeled Images for Curvilinear Object Segmentation                                             | Tianyi Shi                     | 2023-07-14     | cs.CV                                   | Curvilinear object segmentation is critical for many applications. However,manually annotating curvilinear objects is very time-consuming and error-prone,yielding insufficiently available annotated datasets for existing supervisedmethods and domain adaptation methods. This paper proposes a self-supervisedcurvilinear object segmentation method that learns robust and distinctivefeatures from fractals and unlabeled images (FreeCOS). The key contributionsinclude a novel Fractal-FDA synthesis (FFS) module and a geometric informationalignment (GIA) approach. FFS generates curvilinear structures based on theparametric Fractal L-system and integrates the generated structures intounlabeled images to obtain synthetic training images via Fourier DomainAdaptation. GIA reduces the intensity differences between the synthetic andunlabeled images by comparing the intensity order of a given pixel to thevalues of its nearby neighbors. Such image alignment can explicitly remove thedependency on absolute intensity values and enhance the inherent geometriccharacteristics which are common in both synthetic and real images. Inaddition, GIA aligns features of synthetic and real images via the predictionspace adaptation loss (PSAL) and the curvilinear mask contrastive loss (CMCL).Extensive experimental results on four public datasets, i.e., XCAD, DRIVE,STARE and CrackTree demonstrate that our method outperforms thestate-of-the-art unsupervised methods, self-supervised methods and traditionalmethods by a large margin. The source code of this work is available athttps://github.com/TY-Shi/FreeCOS.                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.07245v1 |
| 477 | Self-regulating Prompts: Foundational Model Adaptation without Forgetting                                                                            | Muhammad Uzair Khattak         | 2023-07-13     | cs.CV                                   | Prompt learning has emerged as an efficient alternative for fine-tuningfoundational models, such as CLIP, for various downstream tasks. Conventionallytrained using the task-specific objective, i.e., cross-entropy loss, promptstend to overfit downstream data distributions and find it challenging tocapture task-agnostic general features from the frozen CLIP. This leads to theloss of the model's original generalization capability. To address this issue,our work introduces a self-regularization framework for prompting calledPromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides theprompts to optimize for both task-specific and task-agnostic generalrepresentations using a three-pronged approach by: (a) regulating promptedrepresentations via mutual agreement maximization with the frozen model, (b)regulating with self-ensemble of prompts over the training trajectory to encodetheir complementary strengths, and (c) regulating with textual diversity tomitigate sample diversity imbalance with the visual branch. To the best of ourknowledge, this is the first regularization framework for prompt learning thatavoids overfitting by jointly attending to pre-trained model features, thetraining trajectory during prompting, and the textual diversity. PromptSRCexplicitly steers the prompts to learn a representation space that maximizesperformance on downstream tasks without compromising CLIP generalization. Weperform extensive experiments on 4 benchmarks where PromptSRC overall performsfavorably well compared to the existing methods. Our code and pre-trainedmodels are publicly available at: https://github.com/muzairkhattak/PromptSRC.                                                                                                                                                                                                                                    | http://arxiv.org/abs/2307.06948v2 |
| 478 | Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition                                                                       | Syed Talal Wasim               | 2023-07-13     | cs.CV, cs.AI                            | Recent video recognition models utilize Transformer models for long-rangespatio-temporal context modeling. Video transformer designs are based onself-attention that can model global context at a high computational cost. Incomparison, convolutional designs for videos offer an efficient alternative butlack long-range dependency modeling. Towards achieving the best of bothdesigns, this work proposes Video-FocalNet, an effective and efficientarchitecture for video recognition that models both local and global contexts.Video-FocalNet is based on a spatio-temporal focal modulation architecture thatreverses the interaction and aggregation steps of self-attention for betterefficiency. Further, the aggregation step and the interaction step are bothimplemented using efficient convolution and element-wise multiplicationoperations that are computationally less expensive than their self-attentioncounterparts on video representations. We extensively explore the design spaceof focal modulation-based spatio-temporal context modeling and demonstrate ourparallel spatial and temporal encoding design to be the optimal choice.Video-FocalNets perform favorably well against the state-of-the-arttransformer-based models for video recognition on five large-scale datasets(Kinetics-400, Kinetics-600, SS-v2, Diving-48, and ActivityNet-1.3) at a lowercomputational cost. Our code/models are released athttps://github.com/TalalWasim/Video-FocalNets.                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2307.06947v3 |
| 479 | GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video                                         | Bruce X. B. Yu                 | 2023-07-12     | cs.CV                                   | 3D human pose estimation has been researched for decades with promisingfruits. 3D human pose lifting is one of the promising research directionstoward the task where both estimated pose and ground truth pose data are usedfor training. Existing pose lifting works mainly focus on improving theperformance of estimated pose, but they usually underperform when testing onthe ground truth pose data. We observe that the performance of the estimatedpose can be easily improved by preparing good quality 2D pose, such asfine-tuning the 2D pose or using advanced 2D pose detectors. As such, weconcentrate on improving the 3D human pose lifting via ground truth data forthe future improvement of more quality estimated pose data. Towards this goal,a simple yet effective model called Global-local Adaptive Graph ConvolutionalNetwork (GLA-GCN) is proposed in this work. Our GLA-GCN globally models thespatiotemporal structure via a graph representation and backtraces local jointfeatures for 3D human pose estimation via individually connected layers. Tovalidate our model design, we conduct extensive experiments on three benchmarkdatasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results showthat our GLA-GCN implemented with ground truth 2D poses significantlyoutperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 14% errorreductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively). GitHub:https://github.com/bruceyo/GLA-GCN.                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2307.05853v2 |
| 480 | EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone                                                                         | Shraman Pramanick              | 2023-07-11     | cs.CV                                   | Video-language pre-training (VLP) has become increasingly important due toits ability to generalize to various vision and language tasks. However,existing egocentric VLP frameworks utilize separate video and language encodersand learn task-specific cross-modal information only during fine-tuning,limiting the development of a unified system. In this work, we introduce thesecond generation of egocentric video-language pre-training (EgoVLPv2), asignificant improvement from the previous generation, by incorporatingcross-modal fusion directly into the video and language backbones. EgoVLPv2learns strong video-text representation during pre-training and reuses thecross-modal attention modules to support different downstream tasks in aflexible and efficient manner, reducing fine-tuning costs. Moreover, ourproposed fusion in the backbone strategy is more lightweight andcompute-efficient than stacking additional fusion-specific layers. Extensiveexperiments on a wide range of VL tasks demonstrate the effectiveness ofEgoVLPv2 by achieving consistent state-of-the-art performance over strongbaselines across all downstream. Our project page can be found athttps://shramanpramanick.github.io/EgoVLPv2/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2307.05463v2 |
| 481 | Distilling Large Vision-Language Model with Out-of-Distribution Generalizability                                                                     | Xuanlin Li                     | 2023-07-06     | cs.CV, cs.AI, cs.CL, cs.LG              | Large vision-language models have achieved outstanding performance, but theirsize and computational requirements make their deployment onresource-constrained devices and time-sensitive tasks impractical. Modeldistillation, the process of creating smaller, faster models that maintain theperformance of larger models, is a promising direction towards the solution.This paper investigates the distillation of visual representations in largeteacher vision-language models into lightweight student models using a small-or mid-scale dataset. Notably, this study focuses on open-vocabularyout-of-distribution (OOD) generalization, a challenging problem that has beenoverlooked in previous model distillation literature. We propose two principlesfrom vision and language modality perspectives to enhance student's OODgeneralization: (1) by better imitating teacher's visual representation space,and carefully promoting better coherence in vision-language alignment with theteacher; (2) by enriching the teacher's language representations withinformative and finegrained semantic attributes to effectively distinguishbetween different labels. We propose several metrics and conduct extensiveexperiments to investigate their techniques. The results demonstratesignificant improvements in zero-shot and few-shot student performance onopen-vocabulary out-of-distribution classification, highlighting theeffectiveness of our proposed approaches. Code released athttps://github.com/xuanlinli17/large_vlm_distillation_ood                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2307.03135v2 |
| 482 | Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality                                                       | Peter Lorenz                   | 2023-07-05     | cs.CV, cs.CR                            | Diffusion models recently have been successfully applied for the visualsynthesis of strikingly realistic appearing images. This raises strong concernsabout their potential for malicious purposes. In this paper, we propose usingthe lightweight multi Local Intrinsic Dimensionality (multiLID), which has beenoriginally developed in context of the detection of adversarial examples, forthe automatic detection of synthetic images and the identification of theaccording generator networks. In contrast to many existing detectionapproaches, which often only work for GAN-generated images, the proposed methodprovides close to perfect detection results in many realistic use cases.Extensive experiments on known and newly created datasets demonstrate that theproposed multiLID approach exhibits superiority in diffusion detection andmodel identification. Since the empirical evaluations of recent publications onthe detection of generated images are often mainly focused on the"LSUN-Bedroom" dataset, we further establish a comprehensive benchmark for thedetection of diffusion-generated images, including samples from severaldiffusion models with different image sizes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2307.02347v5 |
| 483 | FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis                                                                                  | Seunghyeon Seo                 | 2023-06-30     | cs.CV                                   | Neural Radiance Field (NeRF) has been a mainstream in novel view synthesiswith its remarkable quality of rendered images and simple architecture.Although NeRF has been developed in various directions improving continuouslyits performance, the necessity of a dense set of multi-view images still existsas a stumbling block to progress for practical application. In this work, wepropose FlipNeRF, a novel regularization method for few-shot novel viewsynthesis by utilizing our proposed flipped reflection rays. The flippedreflection rays are explicitly derived from the input ray directions andestimated normal vectors, and play a role of effective additional training rayswhile enabling to estimate more accurate surface normals and learn the 3Dgeometry effectively. Since the surface normal and the scene depth are bothderived from the estimated densities along a ray, the accurate surface normalleads to more exact depth estimation, which is a key factor for few-shot novelview synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Lossand Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate morereliable outputs with reducing floating artifacts effectively across thedifferent scene structures, and enhance the feature-level consistency betweenthe pair of the rays cast toward the photo-consistent pixels without anyadditional feature extractor, respectively. Our FlipNeRF achieves the SOTAperformance on the multiple benchmarks across all the scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2306.17723v4 |
| 484 | PoseDiffusion: Solving Pose Estimation via Diffusion-aided Bundle Adjustment                                                                         | Jianyuan Wang                  | 2023-06-27     | cs.CV                                   | Camera pose estimation is a long-standing computer vision problem that todate often relies on classical methods, such as handcrafted keypoint matching,RANSAC and bundle adjustment. In this paper, we propose to formulate theStructure from Motion (SfM) problem inside a probabilistic diffusion framework,modelling the conditional distribution of camera poses given input images. Thisnovel view of an old problem has several advantages. (i) The nature of thediffusion framework mirrors the iterative procedure of bundle adjustment. (ii)The formulation allows a seamless integration of geometric constraints fromepipolar geometry. (iii) It excels in typically difficult scenarios such assparse views with wide baselines. (iv) The method can predict intrinsics andextrinsics for an arbitrary amount of images. We demonstrate that our methodPoseDiffusion significantly improves over the classic SfM pipelines and thelearned approaches on two real-world datasets. Finally, it is observed that ourmethod can generalize across datasets without further training. Project page:https://posediffusion.github.io/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2306.15667v3 |
| 485 | Can Differentiable Decision Trees Learn Interpretable Reward Functions?                                                                              | Akansha Kalra                  | 2023-06-22     | cs.LG, cs.AI                            | There is an increasing interest in learning reward functions that model humanintent and human preferences. However, many frameworks use blackbox learningmethods that, while expressive, are difficult to interpret. We propose andevaluate a novel approach for learning expressive and interpretable rewardfunctions from preferences using Differentiable Decision Trees (DDTs). Ourexperiments across several domains, including Cartpole, Visual Gridworldenvironments and Atari games, provide evidence that that the tree structure ofour learned reward function is useful in determining the extent to which thereward function is aligned with human preferences. We experimentallydemonstrate that using reward DDTs results in competitive performance whencompared with larger capacity deep neural network reward functions. We alsoobserve that the choice between soft and hard (argmax) output of reward DDTreveals a tension between wanting highly shaped rewards to ensure good RLperformance, while also wanting simple, non-shaped rewards to affordinterpretability.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2306.13004v2 |
| 486 | Rapid building damage assessment workflow: An implementation for the 2023 Rolling Fork, Mississippi tornado event                                    | Caleb Robinson                 | 2023-06-21     | cs.CV, cs.LG                            | Rapid and accurate building damage assessments from high-resolution satelliteimagery following a natural disaster is essential to inform and optimize firstresponder efforts. However, performing such building damage assessments in anautomated manner is non-trivial due to the challenges posed by variations indisaster-specific damage, diversity in satellite imagery, and the dearth ofextensive, labeled datasets. To circumvent these issues, this paper introducesa human-in-the-loop workflow for rapidly training building damage assessmentmodels after a natural disaster. This article details a case study using thisworkflow, executed in partnership with the American Red Cross during a tornadoevent in Rolling Fork, Mississippi in March, 2023. The output from ourhuman-in-the-loop modeling process achieved a precision of 0.86 and recall of0.80 for damaged buildings when compared to ground truth data collectedpost-disaster. This workflow was implemented end-to-end in under 2 hours persatellite imagery scene, highlighting its potential for real-time deployment.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2306.12589v2 |
| 487 | Dynamic Perceiver for Efficient Visual Recognition                                                                                                   | Yizeng Han                     | 2023-06-20     | cs.CV                                   | Early exiting has become a promising approach to improving the inferenceefficiency of deep networks. By structuring models with multiple classifiers(exits), predictions for ``easy'' samples can be generated at earlier exits,negating the need for executing deeper layers. Current multi-exit networkstypically implement linear classifiers at intermediate layers, compellinglow-level features to encapsulate high-level semantics. This sub-optimal designinvariably undermines the performance of later exits. In this paper, we proposeDynamic Perceiver (Dyn-Perceiver) to decouple the feature extraction procedureand the early classification task with a novel dual-branch architecture. Afeature branch serves to extract image features, while a classification branchprocesses a latent code assigned for classification tasks. Bi-directionalcross-attention layers are established to progressively fuse the information ofboth branches. Early exits are placed exclusively within the classificationbranch, thus eliminating the need for linear separability in low-levelfeatures. Dyn-Perceiver constitutes a versatile and adaptable framework thatcan be built upon various architectures. Experiments on image classification,action recognition, and object detection demonstrate that our methodsignificantly improves the inference efficiency of different backbones,outperforming numerous competitive approaches across a broad range ofcomputational budgets. Evaluation on both CPU and GPU platforms substantiatethe superior practical efficiency of Dyn-Perceiver. Code is available athttps://www.github.com/LeapLabTHU/Dynamic_Perceiver.                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2306.11248v2 |
| 488 | TeleViT: Teleconnection-driven Transformers Improve Subseasonal to Seasonal Wildfire Forecasting                                                     | Ioannis Prapas                 | 2023-06-19     | cs.CV, cs.AI, cs.LG                     | Wildfires are increasingly exacerbated as a result of climate change,necessitating advanced proactive measures for effective mitigation. It isimportant to forecast wildfires weeks and months in advance to plan forest fuelmanagement, resource procurement and allocation. To achieve such accuratelong-term forecasts at a global scale, it is crucial to employ models thataccount for the Earth system's inherent spatio-temporal interactions, such asmemory effects and teleconnections. We propose a teleconnection-driven visiontransformer (TeleViT), capable of treating the Earth as one interconnectedsystem, integrating fine-grained local-scale inputs with global-scale inputs,such as climate indices and coarse-grained global variables. Throughcomprehensive experimentation, we demonstrate the superiority of TeleViT inaccurately predicting global burned area patterns for various forecastingwindows, up to four months in advance. The gain is especially pronounced inlarger forecasting windows, demonstrating the improved ability of deep learningmodels that exploit teleconnections to capture Earth system dynamics. Codeavailable at https://github.com/Orion-Ai-Lab/TeleViT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2306.10940v2 |
| 489 | 3D VR Sketch Guided 3D Shape Prototyping and Exploration                                                                                             | Ling Luo                       | 2023-06-19     | cs.CV                                   | 3D shape modeling is labor-intensive, time-consuming, and requires years ofexpertise. To facilitate 3D shape modeling, we propose a 3D shape generationnetwork that takes a 3D VR sketch as a condition. We assume that sketches arecreated by novices without art training and aim to reconstruct geometricallyrealistic 3D shapes of a given category. To handle potential sketch ambiguity,our method creates multiple 3D shapes that align with the original sketch'sstructure. We carefully design our method, training the model step-by-step andleveraging multi-modal 3D shape representation to support training with limitedtraining data. To guarantee the realism of generated 3D shapes we leverage thenormalizing flow that models the distribution of the latent space of 3D shapes.To encourage the fidelity of the generated 3D shapes to an input sketch, wepropose a dedicated loss that we deploy at different stages of the trainingprocess. The code is available at https://github.com/Rowl1ng/3Dsketch2shape.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2306.10830v4 |
| 490 | Evaluating Data Attribution for Text-to-Image Models                                                                                                 | Sheng-Yu Wang                  | 2023-06-15     | cs.CV, cs.LG                            | While large text-to-image models are able to synthesize "novel" images, theseimages are necessarily a reflection of the training data. The problem of dataattribution in such models -- which of the images in the training set are mostresponsible for the appearance of a given generated image -- is a difficult yetimportant one. As an initial step toward this problem, we evaluate attributionthrough "customization" methods, which tune an existing large-scale modeltoward a given exemplar object or style. Our key insight is that this allows usto efficiently create synthetic images that are computationally influenced bythe exemplar by construction. With our new dataset of such exemplar-influencedimages, we are able to evaluate various data attribution algorithms anddifferent possible feature spaces. Furthermore, by training on our dataset, wecan tune standard models, such as DINO, CLIP, and ViT, toward the attributionproblem. Even though the procedure is tuned towards small exemplar sets, weshow generalization to larger sets. Finally, by taking into account theinherent uncertainty of the problem, we can assign soft attribution scores overa set of training images.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2306.09345v2 |
| 491 | Encyclopedic VQA: Visual questions about detailed properties of fine-grained categories                                                              | Thomas Mensink                 | 2023-06-15     | cs.CV                                   | We propose Encyclopedic-VQA, a large scale visual question answering (VQA)dataset featuring visual questions about detailed properties of fine-grainedcategories and instances. It contains 221k unique question+answer pairs eachmatched with (up to) 5 images, resulting in a total of 1M VQA samples.Moreover, our dataset comes with a controlled knowledge base derived fromWikipedia, marking the evidence to support each answer. Empirically, we showthat our dataset poses a hard challenge for large vision+language models asthey perform poorly on our dataset: PaLI [14] is state-of-the-art on OK-VQA[37], yet it only achieves 13.0% accuracy on our dataset. Moreover, weexperimentally show that progress on answering our encyclopedic questions canbe achieved by augmenting large models with a mechanism that retrieves relevantinformation from the knowledge base. An oracle experiment with perfectretrieval achieves 87.0% accuracy on the single-hop portion of our dataset, andan automatic retrieval-augmented prototype yields 48.8%. We believe that ourdataset enables future research on retrieval-augmented vision+language models.It is available athttps://github.com/google-research/google-research/tree/master/encyclopedic_vqa .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2306.09224v2 |
| 492 | What can a cook in Italy teach a mechanic in India? Action Recognition Generalisation Over Scenarios and Locations                                   | Chiara Plizzari                | 2023-06-14     | cs.CV                                   | We propose and address a new generalisation problem: can a model trained foraction recognition successfully classify actions when they are performed withina previously unseen scenario and in a previously unseen location? To answerthis question, we introduce the Action Recognition Generalisation Overscenarios and locations dataset (ARGO1M), which contains 1.1M video clips fromthe large-scale Ego4D dataset, across 10 scenarios and 13 locations. Wedemonstrate recognition models struggle to generalise over 10 proposed testsplits, each of an unseen scenario in an unseen location. We thus propose CIR,a method to represent each video as a Cross-Instance Reconstruction of videosfrom other domains. Reconstructions are paired with text narrations to guidethe learning of a domain generalisable representation. We provide extensiveanalysis and ablations on ARGO1M that show CIR outperforms prior domaingeneralisation works on all test splits. Code and data:https://chiaraplizz.github.io/what-can-a-cook/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2306.08713v2 |
| 493 | TAPIR: Tracking Any Point with per-frame Initialization and temporal Refinement                                                                      | Carl Doersch                   | 2023-06-14     | cs.CV                                   | We present a novel model for Tracking Any Point (TAP) that effectively tracksany queried point on any physical surface throughout a video sequence. Ourapproach employs two stages: (1) a matching stage, which independently locatesa suitable candidate point match for the query point on every other frame, and(2) a refinement stage, which updates both the trajectory and query featuresbased on local correlations. The resulting model surpasses all baseline methodsby a significant margin on the TAP-Vid benchmark, as demonstrated by anapproximate 20% absolute average Jaccard (AJ) improvement on DAVIS. Our modelfacilitates fast inference on long and high-resolution video sequences. On amodern GPU, our implementation has the capacity to track points faster thanreal-time, and can be flexibly extended to higher-resolution videos. Given thehigh-quality trajectories extracted from a large dataset, we demonstrate aproof-of-concept diffusion model which generates trajectories from staticimages, enabling plausible animations. Visualizations, source code, andpretrained models can be found on our project webpage.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2306.08637v2 |
| 494 | Predict to Detect: Prediction-guided 3D Object Detection using Sequential Images                                                                     | Sanmin Kim                     | 2023-06-14     | cs.CV                                   | Recent camera-based 3D object detection methods have introduced sequentialframes to improve the detection performance hoping that multiple frames wouldmitigate the large depth estimation error. Despite improved detectionperformance, prior works rely on naive fusion methods (e.g., concatenation) orare limited to static scenes (e.g., temporal stereo), neglecting the importanceof the motion cue of objects. These approaches do not fully exploit thepotential of sequential images and show limited performance improvements. Toaddress this limitation, we propose a novel 3D object detection model, P2D(Predict to Detect), that integrates a prediction scheme into a detectionframework to explicitly extract and leverage motion features. P2D predictsobject information in the current frame using solely past frames to learntemporal motion features. We then introduce a novel temporal featureaggregation method that attentively exploits Bird's-Eye-View (BEV) featuresbased on predicted object information, resulting in accurate 3D objectdetection. Experimental results demonstrate that P2D improves mAP and NDS by3.0% and 3.7% compared to the sequential image-based baseline, illustratingthat incorporating a prediction scheme can significantly improve detectionaccuracy.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2306.08528v2 |
| 495 | Efficient 3D Semantic Segmentation with Superpoint Transformer                                                                                       | Damien Robert                  | 2023-06-13     | cs.CV                                   | We introduce a novel superpoint-based transformer architecture for efficientsemantic segmentation of large-scale 3D scenes. Our method incorporates a fastalgorithm to partition point clouds into a hierarchical superpoint structure,which makes our preprocessing 7 times faster than existing superpoint-basedapproaches. Additionally, we leverage a self-attention mechanism to capture therelationships between superpoints at multiple scales, leading tostate-of-the-art performance on three challenging benchmark datasets: S3DIS(76.0% mIoU 6-fold validation), KITTI-360 (63.5% on Val), and DALES (79.6%).With only 212k parameters, our approach is up to 200 times more compact thanother state-of-the-art models while maintaining similar performance.Furthermore, our model can be trained on a single GPU in 3 hours for a fold ofthe S3DIS dataset, which is 7x to 70x fewer GPU-hours than the best-performingmethods. Our code and models are accessible atgithub.com/drprojects/superpoint_transformer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2306.08045v2 |
| 496 | Hidden Biases of End-to-End Driving Models                                                                                                           | Bernhard Jaeger                | 2023-06-13     | cs.CV, cs.AI, cs.LG, cs.RO              | End-to-end driving systems have recently made rapid progress, in particularon CARLA. Independent of their major contribution, they introduce changes tominor system components. Consequently, the source of improvements is unclear.We identify two biases that recur in nearly all state-of-the-art methods andare critical for the observed progress on CARLA: (1) lateral recovery via astrong inductive bias towards target point following, and (2) longitudinalaveraging of multimodal waypoint predictions for slowing down. We investigatethe drawbacks of these biases and identify principled alternatives. Byincorporating our insights, we develop TF++, a simple end-to-end method thatranks first on the Longest6 and LAV benchmarks, gaining 11 driving score overthe best prior work on Longest6.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2306.07957v2 |
| 497 | Waffling around for Performance: Visual Classification with Random Words and Broad Concepts                                                          | Karsten Roth                   | 2023-06-12     | cs.CV, cs.LG                            | The visual classification performance of vision-language models such as CLIPhas been shown to benefit from additional semantic knowledge from largelanguage models (LLMs) such as GPT-3. In particular, averaging overLLM-generated class descriptors, e.g. "waffle, which has a round shape", cannotably improve generalization performance. In this work, we critically studythis behavior and propose WaffleCLIP, a framework for zero-shot visualclassification which simply replaces LLM-generated descriptors with randomcharacter and word descriptors. Without querying external models, we achievecomparable performance gains on a large number of visual classification tasks.This allows WaffleCLIP to both serve as a low-cost alternative, as well as asanity check for any future LLM-based vision-language model extensions. Weconduct an extensive experimental study on the impact and shortcomings ofadditional semantics introduced with LLM-generated descriptors, and showcasehow - if available - semantic context is better leveraged by querying LLMs forhigh-level concepts, which we show can be done to jointly resolve potentialclass name ambiguities. Code is available here:https://github.com/ExplainableML/WaffleCLIP.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2306.07282v2 |
| 498 | DetZero: Rethinking Offboard 3D Object Detection with Long-term Sequential Point Clouds                                                              | Tao Ma                         | 2023-06-09     | cs.CV                                   | Existing offboard 3D detectors always follow a modular pipeline design totake advantage of unlimited sequential point clouds. We have found that thefull potential of offboard 3D detectors is not explored mainly due to tworeasons: (1) the onboard multi-object tracker cannot generate sufficientcomplete object trajectories, and (2) the motion state of objects poses aninevitable challenge for the object-centric refining stage in leveraging thelong-term temporal context representation. To tackle these problems, we proposea novel paradigm of offboard 3D object detection, named DetZero. Concretely, anoffline tracker coupled with a multi-frame detector is proposed to focus on thecompleteness of generated object tracks. An attention-mechanism refining moduleis proposed to strengthen contextual information interaction across long-termsequential point clouds for object refining with decomposed regression methods.Extensive experiments on Waymo Open Dataset show our DetZero outperforms allstate-of-the-art onboard and offboard 3D detection methods. Notably, DetZeroranks 1st place on Waymo 3D object detection leaderboard with 85.15 mAPH (L2)detection performance. Further experiments validate the application of takingthe place of human labels with such high-quality results. Our empirical studyleads to rethinking conventions and interesting findings that can guide futureresearch on offboard 3D object detection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2306.06023v2 |
| 499 | TrajectoryFormer: 3D Object Tracking Transformer with Predictive Trajectory Hypotheses                                                               | Xuesong Chen                   | 2023-06-09     | cs.CV                                   | 3D multi-object tracking (MOT) is vital for many applications includingautonomous driving vehicles and service robots. With the commonly usedtracking-by-detection paradigm, 3D MOT has made important progress in recentyears. However, these methods only use the detection boxes of the current frameto obtain trajectory-box association results, which makes it impossible for thetracker to recover objects missed by the detector. In this paper, we presentTrajectoryFormer, a novel point-cloud-based 3D MOT framework. To recover themissed object by detector, we generates multiple trajectory hypotheses withhybrid candidate boxes, including temporally predicted boxes and current-framedetection boxes, for trajectory-box association. The predicted boxes canpropagate object's history trajectory information to the current frame and thusthe network can tolerate short-term miss detection of the tracked objects. Wecombine long-term object motion feature and short-term object appearancefeature to create per-hypothesis feature embedding, which reduces thecomputational overhead for spatial-temporal encoding. Additionally, weintroduce a Global-Local Interaction Module to conduct information interactionamong all hypotheses and models their spatial relations, leading to accurateestimation of hypotheses. Our TrajectoryFormer achieves state-of-the-artperformance on the Waymo 3D MOT benchmarks. Code is available athttps://github.com/poodarchu/EFG .                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2306.05888v2 |
| 500 | Unsupervised Compositional Concepts Discovery with Text-to-Image Generative Models                                                                   | Nan Liu                        | 2023-06-08     | cs.CV, cs.AI, cs.LG                     | Text-to-image generative models have enabled high-resolution image synthesisacross different domains, but require users to specify the content they wish togenerate. In this paper, we consider the inverse problem -- given a collectionof different images, can we discover the generative concepts that representeach image? We present an unsupervised approach to discover generative conceptsfrom a collection of images, disentangling different art styles in paintings,objects, and lighting from kitchen scenes, and discovering image classes givenImageNet images. We show how such generative concepts can accurately representthe content of images, be recombined and composed to generate new artistic andhybrid images, and be further used as a representation for downstreamclassification tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2306.05357v2 |
| 501 | TransTIC: Transferring Transformer-based Image Compression from Human Perception to Machine Perception                                               | Yi-Hsin Chen                   | 2023-06-08     | eess.IV                                 | This work aims for transferring a Transformer-based image compression codecfrom human perception to machine perception without fine-tuning the codec. Wepropose a transferable Transformer-based image compression framework, termedTransTIC. Inspired by visual prompt tuning, TransTIC adopts aninstance-specific prompt generator to inject instance-specific prompts to theencoder and task-specific prompts to the decoder. Extensive experiments showthat our proposed method is capable of transferring the base codec to variousmachine tasks and outperforms the competing methods significantly. To our bestknowledge, this work is the first attempt to utilize prompting on the low-levelimage compression task.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2306.05085v2 |
| 502 | DVIS: Decoupled Video Instance Segmentation Framework                                                                                                | Tao Zhang                      | 2023-06-06     | cs.CV                                   | Video instance segmentation (VIS) is a critical task with diverseapplications, including autonomous driving and video editing. Existing methodsoften underperform on complex and long videos in real world, primarily due totwo factors. Firstly, offline methods are limited by the tightly-coupledmodeling paradigm, which treats all frames equally and disregards theinterdependencies between adjacent frames. Consequently, this leads to theintroduction of excessive noise during long-term temporal alignment. Secondly,online methods suffer from inadequate utilization of temporal information. Totackle these challenges, we propose a decoupling strategy for VIS by dividingit into three independent sub-tasks: segmentation, tracking, and refinement.The efficacy of the decoupling strategy relies on two crucial elements: 1)attaining precise long-term alignment outcomes via frame-by-frame associationduring tracking, and 2) the effective utilization of temporal informationpredicated on the aforementioned accurate alignment outcomes during refinement.We introduce a novel referring tracker and temporal refiner to construct the\textbf{D}ecoupled \textbf{VIS} framework (\textbf{DVIS}). DVIS achieves newSOTA performance in both VIS and VPS, surpassing the current SOTA methods by7.3 AP and 9.6 VPQ on the OVIS and VIPSeg datasets, which are the mostchallenging and realistic benchmarks. Moreover, thanks to the decouplingstrategy, the referring tracker and temporal refiner are super light-weight(only 1.69\% of the segmenter FLOPs), allowing for efficient training andinference on a single GPU with 11G memory. The code is available at\href{https://github.com/zhang-tao-whu/DVIS}{https://github.com/zhang-tao-whu/DVIS}.                                                                                                                                                                           | http://arxiv.org/abs/2306.03413v3 |
| 503 | DeepVAT: A Self-Supervised Technique for Cluster Assessment in Image Datasets                                                                        | Alokendu Mazumder              | 2023-05-29     | cs.LG, cs.CV                            | Estimating the number of clusters and cluster structures in unlabeled,complex, and high-dimensional datasets (like images) is challenging fortraditional clustering algorithms. In recent years, a matrix reordering-basedalgorithm called Visual Assessment of Tendency (VAT), and its variants haveattracted many researchers from various domains to estimate the number ofclusters and inherent cluster structure present in the data. However, thesealgorithms face significant challenges when dealing with image data as theyfail to effectively capture the crucial features inherent in images. Toovercome these limitations, we propose a deep-learning-based framework thatenables the assessment of cluster structure in complex image datasets. Ourapproach utilizes a self-supervised deep neural network to generaterepresentative embeddings for the data. These embeddings are then reduced to2-dimension using t-distributed Stochastic Neighbour Embedding (t-SNE) andinputted into VAT based algorithms to estimate the underlying clusterstructure. Importantly, our framework does not rely on any prior knowledge ofthe number of clusters. Our proposed approach demonstrates superior performancecompared to state-of-the-art VAT family algorithms and two other deepclustering algorithms on four benchmark image datasets, namely MNIST, FMNIST,CIFAR-10, and INTEL.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2306.00011v2 |
| 504 | EgoHumans: An Egocentric 3D Multi-Human Benchmark                                                                                                    | Rawal Khirodkar                | 2023-05-25     | cs.CV, cs.AI                            | We present EgoHumans, a new multi-view multi-human video benchmark to advancethe state-of-the-art of egocentric human 3D pose estimation and tracking.Existing egocentric benchmarks either capture single subject or indoor-onlyscenarios, which limit the generalization of computer vision algorithms forreal-world applications. We propose a novel 3D capture setup to construct acomprehensive egocentric multi-human benchmark in the wild with annotations tosupport diverse tasks such as human detection, tracking, 2D/3D pose estimation,and mesh recovery. We leverage consumer-grade wearable camera-equipped glassesfor the egocentric view, which enables us to capture dynamic activities likeplaying tennis, fencing, volleyball, etc. Furthermore, our multi-view setupgenerates accurate 3D ground truth even under severe or complete occlusion. Thedataset consists of more than 125k egocentric images, spanning diverse sceneswith a particular focus on challenging and unchoreographed multi-humanactivities and fast-moving egocentric views. We rigorously evaluate existingstate-of-the-art methods and highlight their limitations in the egocentricscenario, specifically on multi-human tracking. To address such limitations, wepropose EgoFormer, a novel approach with a multi-stream transformerarchitecture and explicit 3D spatial reasoning to estimate and track the humanpose. EgoFormer significantly outperforms prior art by 13.6% IDF1 on theEgoHumans dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2305.16487v2 |
| 505 | MRN: Multiplexed Routing Network for Incremental Multilingual Text Recognition                                                                       | Tianlun Zheng                  | 2023-05-24     | cs.CV                                   | Multilingual text recognition (MLTR) systems typically focus on a fixed setof languages, which makes it difficult to handle newly added languages or adaptto ever-changing data distribution. In this paper, we propose the IncrementalMLTR (IMLTR) task in the context of incremental learning (IL), where differentlanguages are introduced in batches. IMLTR is particularly challenging due torehearsal-imbalance, which refers to the uneven distribution of samplecharacters in the rehearsal set, used to retain a small amount of old data aspast memories. To address this issue, we propose a Multiplexed Routing Network(MRN). MRN trains a recognizer for each language that is currently seen.Subsequently, a language domain predictor is learned based on the rehearsal setto weigh the recognizers. Since the recognizers are derived from the originaldata, MRN effectively reduces the reliance on older data and better fightsagainst catastrophic forgetting, the core issue in IL. We extensively evaluateMRN on MLT17 and MLT19 datasets. It outperforms existing general-purpose ILmethods by large margins, with average accuracy improvements ranging from 10.3%to 35.8% under different settings. Code is available athttps://github.com/simplify23/MRN.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2305.14758v3 |
| 506 | BlindHarmony: "Blind" Harmonization for MR Images via Flow model                                                                                     | Hwihun Jeong                   | 2023-05-18     | eess.IV, cs.CV                          | In MRI, images of the same contrast (e.g., T$_1$) from the same subject canexhibit noticeable differences when acquired using different hardware,sequences, or scan parameters. These differences in images create a domain gapthat needs to be bridged by a step called image harmonization, to process theimages successfully using conventional or deep learning-based image analysis(e.g., segmentation). Several methods, including deep learning-basedapproaches, have been proposed to achieve image harmonization. However, theyoften require datasets from multiple domains for deep learning training and maystill be unsuccessful when applied to images from unseen domains. To addressthis limitation, we propose a novel concept called `Blind Harmonization', whichutilizes only target domain data for training but still has the capability toharmonize images from unseen domains. For the implementation of blindharmonization, we developed BlindHarmony using an unconditional flow modeltrained on target domain data. The harmonized image is optimized to have acorrelation with the input source domain image while ensuring that the latentvector of the flow model is close to the center of the Gaussian distribution.BlindHarmony was evaluated on both simulated and real datasets and compared toconventional methods. BlindHarmony demonstrated noticeable performance on bothdatasets, highlighting its potential for future use in clinical settings. Thesource code is available at: https://github.com/SNU-LIST/BlindHarmony                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2305.10732v2 |
| 507 | Understanding 3D Object Interaction from a Single Image                                                                                              | Shengyi Qian                   | 2023-05-16     | cs.CV                                   | Humans can easily understand a single image as depicting multiple potentialobjects permitting interaction. We use this skill to plan our interactions withthe world and accelerate understanding new objects without engaging ininteraction. In this paper, we would like to endow machines with the similarability, so that intelligent agents can better explore the 3D scene ormanipulate objects. Our approach is a transformer-based model that predicts the3D location, physical properties and affordance of objects. To power thismodel, we collect a dataset with Internet videos, egocentric videos and indoorimages to train and validate our approach. Our model yields strong performanceon our data, and generalizes well to robotics data. Project site:https://jasonqsy.github.io/3DOI/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2305.09664v2 |
| 508 | Learning Higher-order Object Interactions for Keypoint-based Video Understanding                                                                     | Yi Huang                       | 2023-05-16     | cs.CV                                   | Action recognition is an important problem that requires identifying actionsin video by learning complex interactions across scene actors and objects.However, modern deep-learning based networks often require significantcomputation, and may capture scene context using various modalities thatfurther increases compute costs. Efficient methods such as those used for AR/VRoften only use human-keypoint information but suffer from a loss of scenecontext that hurts accuracy. In this paper, we describe an action-localizationmethod, KeyNet, that uses only the keypoint data for tracking and actionrecognition. Specifically, KeyNet introduces the use of object based keypointinformation to capture context in the scene. Our method illustrates how tobuild a structured intermediate representation that allows modelinghigher-order interactions in the scene from object and human keypoints withoutusing any RGB information. We find that KeyNet is able to track and classifyhuman actions at just 5 FPS. More importantly, we demonstrate that objectkeypoints can be modeled to recover any loss in context from using keypointinformation over AVA action and Kinetics datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2305.09539v1 |
| 509 | Distracting Downpour: Adversarial Weather Attacks for Motion Estimation                                                                              | Jenny Schmalfuss               | 2023-05-11     | cs.CV                                   | Current adversarial attacks on motion estimation, or optical flow, optimizesmall per-pixel perturbations, which are unlikely to appear in the real world.In contrast, adverse weather conditions constitute a much more realistic threatscenario. Hence, in this work, we present a novel attack on motion estimationthat exploits adversarially optimized particles to mimic weather effects likesnowflakes, rain streaks or fog clouds. At the core of our attack framework isa differentiable particle rendering system that integrates particles (i)consistently over multiple time steps (ii) into the 3D space (iii) with aphoto-realistic appearance. Through optimization, we obtain adversarial weatherthat significantly impacts the motion estimation. Surprisingly, methods thatpreviously showed good robustness towards small per-pixel perturbations areparticularly vulnerable to adversarial weather. At the same time, augmentingthe training with non-optimized weather increases a method's robustness towardsweather effects and improves generalizability at almost no additional cost. Ourcode will be available at https://github.com/cv-stuttgart/DistractingDownpour.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2305.06716v2 |
| 510 | Undercover Deepfakes: Detecting Fake Segments in Videos                                                                                              | Sanjay Saha                    | 2023-05-11     | cs.CV                                   | The recent renaissance in generative models, driven primarily by the adventof diffusion models and iterative improvement in GAN methods, has enabled manycreative applications. However, each advancement is also accompanied by a risein the potential for misuse. In the arena of the deepfake generation, this is akey societal issue. In particular, the ability to modify segments of videosusing such generative techniques creates a new paradigm of deepfakes which aremostly real videos altered slightly to distort the truth. This paradigm hasbeen under-explored by the current deepfake detection methods in the academicliterature. In this paper, we present a deepfake detection method that canaddress this issue by performing deepfake prediction at the frame and videolevels. To facilitate testing our method, we prepared a new benchmark datasetwhere videos have both real and fake frame sequences with very subtletransitions. We provide a benchmark on the proposed dataset with our detectionmethod which utilizes the Vision Transformer based on Scaling and Shifting tolearn spatial features, and a Timeseries Transformer to learn temporal featuresof the videos to help facilitate the interpretation of possible deepfakes.Extensive experiments on a variety of deepfake generation methods showexcellent results by the proposed method on temporal segmentation and classicalvideo-level predictions as well. In particular, the paradigm we address willform a powerful tool for the moderation of deepfakes, where human oversight canbe better targeted to the parts of videos suspected of being deepfakes. Allexperiments can be reproduced at:github.com/rgb91/temporal-deepfake-segmentation.                                                                                                                                                                                                                | http://arxiv.org/abs/2305.06564v4 |
| 511 | Learning Semi-supervised Gaussian Mixture Models for Generalized Category Discovery                                                                  | Bingchen Zhao                  | 2023-05-10     | cs.CV                                   | In this paper, we address the problem of generalized category discovery(GCD), \ie, given a set of images where part of them are labelled and the restare not, the task is to automatically cluster the images in the unlabelleddata, leveraging the information from the labelled data, while the unlabelleddata contain images from the labelled classes and also new ones. GCD is similarto semi-supervised learning (SSL) but is more realistic and challenging, as SSLassumes all the unlabelled images are from the same classes as the labelledones. We also do not assume the class number in the unlabelled data is knowna-priori, making the GCD problem even harder. To tackle the problem of GCDwithout knowing the class number, we propose an EM-like framework thatalternates between representation learning and class number estimation. Wepropose a semi-supervised variant of the Gaussian Mixture Model (GMM) with astochastic splitting and merging mechanism to dynamically determine theprototypes by examining the cluster compactness and separability. With theseprototypes, we leverage prototypical contrastive learning for representationlearning on the partially labelled data subject to the constraints imposed bythe labelled data. Our framework alternates between these two steps untilconvergence. The cluster assignment for an unlabelled instance can then beretrieved by identifying its nearest prototype. We comprehensively evaluate ourframework on both generic image classification datasets and challengingfine-grained object recognition datasets, achieving state-of-the-artperformance.                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2305.06144v2 |
| 512 | Relightify: Relightable 3D Faces from a Single Image via Diffusion Models                                                                            | Foivos Paraperas Papantoniou   | 2023-05-10     | cs.CV                                   | Following the remarkable success of diffusion models on image generation,recent works have also demonstrated their impressive ability to address anumber of inverse problems in an unsupervised way, by properly constraining thesampling process based on a conditioning input. Motivated by this, in thispaper, we present the first approach to use diffusion models as a prior forhighly accurate 3D facial BRDF reconstruction from a single image. We start byleveraging a high-quality UV dataset of facial reflectance (diffuse andspecular albedo and normals), which we render under varying illuminationsettings to simulate natural RGB textures and, then, train an unconditionaldiffusion model on concatenated pairs of rendered textures and reflectancecomponents. At test time, we fit a 3D morphable model to the given image andunwrap the face in a partial UV texture. By sampling from the diffusion model,while retaining the observed texture part intact, the model inpaints not onlythe self-occluded areas but also the unknown reflectance components, in asingle sequence of denoising steps. In contrast to existing methods, wedirectly acquire the observed texture from the input image, thus, resulting inmore faithful and consistent reflectance estimation. Through a series ofqualitative and quantitative comparisons, we demonstrate superior performancein both texture completion as well as reflectance reconstruction tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2305.06077v2 |
| 513 | Neural LiDAR Fields for Novel View Synthesis                                                                                                         | Shengyu Huang                  | 2023-05-02     | cs.CV                                   | We present Neural Fields for LiDAR (NFL), a method to optimise a neural fieldscene representation from LiDAR measurements, with the goal of synthesizingrealistic LiDAR scans from novel viewpoints. NFL combines the rendering powerof neural fields with a detailed, physically motivated model of the LiDARsensing process, thus enabling it to accurately reproduce key sensor behaviorslike beam divergence, secondary returns, and ray dropping. We evaluate NFL onsynthetic and real LiDAR scans and show that it outperforms explicitreconstruct-then-simulate methods as well as other NeRF-style methods on LiDARnovel view synthesis task. Moreover, we show that the improved realism of thesynthesized views narrows the domain gap to real scans and translates to betterregistration and semantic segmentation performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2305.01643v2 |
| 514 | TMR: Text-to-Motion Retrieval Using Contrastive 3D Human Motion Synthesis                                                                            | Mathis Petrovich               | 2023-05-02     | cs.CV, cs.CL                            | In this paper, we present TMR, a simple yet effective approach for text to 3Dhuman motion retrieval. While previous work has only treated retrieval as aproxy evaluation metric, we tackle it as a standalone task. Our method extendsthe state-of-the-art text-to-motion synthesis model TEMOS, and incorporates acontrastive loss to better structure the cross-modal latent space. We show thatmaintaining the motion generation loss, along with the contrastive training, iscrucial to obtain good performance. We introduce a benchmark for evaluation andprovide an in-depth analysis by reporting results on several protocols. Ourextensive experiments on the KIT-ML and HumanML3D datasets show that TMRoutperforms the prior work by a significant margin, for example reducing themedian rank from 54 to 19. Finally, we showcase the potential of our approachon moment retrieval. Our code and models are publicly available athttps://mathis.petrovich.fr/tmr.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2305.00976v2 |
| 515 | CVRecon: Rethinking 3D Geometric Feature Learning For Neural Reconstruction                                                                          | Ziyue Feng                     | 2023-04-28     | cs.CV, cs.AI, cs.LG, cs.RO              | Recent advances in neural reconstruction using posed image sequences havemade remarkable progress. However, due to the lack of depth information,existing volumetric-based techniques simply duplicate 2D image features of theobject surface along the entire camera ray. We contend this duplicationintroduces noise in empty and occluded spaces, posing challenges for producinghigh-quality 3D geometry. Drawing inspiration from traditional multi-viewstereo methods, we propose an end-to-end 3D neural reconstruction frameworkCVRecon, designed to exploit the rich geometric embedding in the cost volumesto facilitate 3D geometric feature learning. Furthermore, we presentRay-contextual Compensated Cost Volume (RCCV), a novel 3D geometric featurerepresentation that encodes view-dependent information with improved integrityand robustness. Through comprehensive experiments, we demonstrate that ourapproach significantly improves the reconstruction quality in various metricsand recovers clear fine details of the 3D geometries. Our extensive ablationstudies provide insights into the development of effective 3D geometric featurelearning schemes. Project page: https://cvrecon.ziyue.cool/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2304.14633v2 |
| 516 | Incremental Generalized Category Discovery                                                                                                           | Bingchen Zhao                  | 2023-04-27     | cs.CV                                   | We explore the problem of Incremental Generalized Category Discovery (IGCD).This is a challenging category incremental learning setting where the goal isto develop models that can correctly categorize images from previously seencategories, in addition to discovering novel ones. Learning is performed over aseries of time steps where the model obtains new labeled and unlabeled data,and discards old data, at each iteration. The difficulty of the problem iscompounded in our generalized setting as the unlabeled data can contain imagesfrom categories that may or may not have been observed before. We present a newmethod for IGCD which combines non-parametric categorization with efficientimage sampling to mitigate catastrophic forgetting. To quantify performance, wepropose a new benchmark dataset named iNatIGCD that is motivated by areal-world fine-grained visual categorization task. In our experiments weoutperform existing related methods                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2304.14310v2 |
| 517 | Learning Human-Human Interactions in Images from Weak Textual Supervision                                                                            | Morris Alper                   | 2023-04-27     | cs.CV, cs.CL, cs.LG                     | Interactions between humans are diverse and context-dependent, but previousworks have treated them as categorical, disregarding the heavy tail of possibleinteractions. We propose a new paradigm of learning human-human interactions asfree text from a single still image, allowing for flexibility in modeling theunlimited space of situations and relationships between people. To overcome theabsence of data labelled specifically for this task, we use knowledgedistillation applied to synthetic caption data produced by a large languagemodel without explicit supervision. We show that the pseudo-labels produced bythis procedure can be used to train a captioning model to effectivelyunderstand human-human interactions in images, as measured by a variety ofmetrics that measure textual and semantic faithfulness and factual groundednessof our predictions. We further show that our approach outperforms SOTA imagecaptioning and situation recognition models on this task. We will release ourcode and pseudo-labels along with Waldo and Wenda, a manually-curated test setfor still image human-human interaction understanding.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2304.14104v3 |
| 518 | From Chaos Comes Order: Ordering Event Representations for Object Recognition and Detection                                                          | Nikola Zubić                   | 2023-04-26     | cs.CV, cs.LG                            | Today, state-of-the-art deep neural networks that process events firstconvert them into dense, grid-like input representations before using anoff-the-shelf network. However, selecting the appropriate representation forthe task traditionally requires training a neural network for eachrepresentation and selecting the best one based on the validation score, whichis very time-consuming. This work eliminates this bottleneck by selectingrepresentations based on the Gromov-Wasserstein Discrepancy (GWD) between rawevents and their representation. It is about 200 times faster to compute thantraining a neural network and preserves the task performance ranking of eventrepresentations across multiple representations, network backbones, datasets,and tasks. Thus finding representations with high task scores is equivalent tofinding representations with a low GWD. We use this insight to, for the firsttime, perform a hyperparameter search on a large family of eventrepresentations, revealing new and powerful representations that exceed thestate-of-the-art. Our optimized representations outperform existingrepresentations by 1.7 mAP on the 1 Mpx dataset and 0.3 mAP on the Gen1dataset, two established object detection benchmarks, and reach a 3.8% higherclassification score on the mini N-ImageNet benchmark. Moreover, we outperformstate-of-the-art by 2.1 mAP on Gen1 and state-of-the-art feed-forward methodsby 6.0 mAP on the 1 Mpx datasets. This work opens a new unexplored field ofexplicit representation optimization for event-based learning.                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2304.13455v3 |
| 519 | Neural-PBIR Reconstruction of Shape, Material, and Illumination                                                                                      | Cheng Sun                      | 2023-04-26     | cs.CV                                   | Reconstructing the shape and spatially varying surface appearances of aphysical-world object as well as its surrounding illumination based on 2Dimages (e.g., photographs) of the object has been a long-standing problem incomputer vision and graphics. In this paper, we introduce an accurate andhighly efficient object reconstruction pipeline combining neural based objectreconstruction and physics-based inverse rendering (PBIR). Our pipeline firstlyleverages a neural SDF based shape reconstruction to produce high-quality butpotentially imperfect object shape. Then, we introduce a neural material andlighting distillation stage to achieve high-quality predictions for materialand illumination. In the last stage, initialized by the neural predictions, weperform PBIR to refine the initial results and obtain the final high-qualityreconstruction of object shape, material, and illumination. Experimentalresults demonstrate our pipeline significantly outperforms existing methodsquality-wise and performance-wise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2304.13445v3 |
| 520 | EverLight: Indoor-Outdoor Editable HDR Lighting Estimation                                                                                           | Mohammad Reza Karimi Dastjerdi | 2023-04-26     | cs.CV                                   | Because of the diversity in lighting environments, existing illuminationestimation techniques have been designed explicitly on indoor or outdoorenvironments. Methods have focused specifically on capturing accurate energy(e.g., through parametric lighting models), which emphasizes shading and strongcast shadows; or producing plausible texture (e.g., with GANs), whichprioritizes plausible reflections. Approaches which provide editable lightingcapabilities have been proposed, but these tend to be with simplified lightingmodels, offering limited realism. In this work, we propose to bridge the gapbetween these recent trends in the literature, and propose a method whichcombines a parametric light model with 360{\deg} panoramas, ready to use asHDRI in rendering engines. We leverage recent advances in GAN-based LDRpanorama extrapolation from a regular image, which we extend to HDR usingparametric spherical gaussians. To achieve this, we introduce a novel lightingco-modulation method that injects lighting-related features throughout thegenerator, tightly coupling the original or edited scene illumination withinthe panorama generation process. In our representation, users can easily editlight direction, intensity, number, etc. to impact shading while providingrich, complex reflections while seamlessly blending with the edits.Furthermore, our method encompasses indoor and outdoor environments,demonstrating state-of-the-art results even when compared to domain-specificmethods.                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2304.13207v2 |
| 521 | SAFE: Machine Unlearning With Shard Graphs                                                                                                           | Yonatan Dukler                 | 2023-04-25     | cs.LG                                   | We present Synergy Aware Forgetting Ensemble (SAFE), a method to adapt largemodels on a diverse collection of data while minimizing the expected cost toremove the influence of training samples from the trained model. This process,also known as selective forgetting or unlearning, is often conducted bypartitioning a dataset into shards, training fully independent models on each,then ensembling the resulting models. Increasing the number of shards reducesthe expected cost to forget but at the same time it increases inference costand reduces the final accuracy of the model since synergistic informationbetween samples is lost during the independent model training. Rather thantreating each shard as independent, SAFE introduces the notion of a shardgraph, which allows incorporating limited information from other shards duringtraining, trading off a modest increase in expected forgetting cost with asignificant increase in accuracy, all while still attaining complete removal ofresidual influence after forgetting. SAFE uses a lightweight system of adapterswhich can be trained while reusing most of the computations. This allows SAFEto be trained on shards an order-of-magnitude smaller than currentstate-of-the-art methods (thus reducing the forgetting costs) while alsomaintaining high accuracy, as we demonstrate empirically on fine-grainedcomputer vision datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2304.13169v2 |
| 522 | DQS3D: Densely-matched Quantization-aware Semi-supervised 3D Detection                                                                               | Huan-ang Gao                   | 2023-04-25     | cs.CV                                   | In this paper, we study the problem of semi-supervised 3D object detection,which is of great importance considering the high annotation cost for cluttered3D indoor scenes. We resort to the robust and principled framework ofselfteaching, which has triggered notable progress for semisupervised learningrecently. While this paradigm is natural for image-level or pixel-levelprediction, adapting it to the detection problem is challenged by the issue ofproposal matching. Prior methods are based upon two-stage pipelines, matchingheuristically selected proposals generated in the first stage and resulting inspatially sparse training signals. In contrast, we propose the firstsemisupervised 3D detection algorithm that works in the singlestage manner andallows spatially dense training signals. A fundamental issue of this new designis the quantization error caused by point-to-voxel discretization, whichinevitably leads to misalignment between two transformed views in the voxeldomain. To this end, we derive and implement closed-form rules that compensatethis misalignment onthe-fly. Our results are significant, e.g., promotingScanNet mAP@0.5 from 35.2% to 48.5% using 20% annotation. Codes and data willbe publicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2304.13031v2 |
| 523 | Bayesian Optimization Meets Self-Distillation                                                                                                        | HyunJae Lee                    | 2023-04-25     | cs.CV                                   | Bayesian optimization (BO) has contributed greatly to improving modelperformance by suggesting promising hyperparameter configurations iterativelybased on observations from multiple training trials. However, only partialknowledge (i.e., the measured performances of trained models and theirhyperparameter configurations) from previous trials is transferred. On theother hand, Self-Distillation (SD) only transfers partial knowledge learned bythe task model itself. To fully leverage the various knowledge gained from alltraining trials, we propose the BOSS framework, which combines BO and SD. BOSSsuggests promising hyperparameter configurations through BO and carefullyselects pre-trained models from previous trials for SD, which are otherwiseabandoned in the conventional BO process. BOSS achieves significantly betterperformance than both BO and SD in a wide range of tasks including generalimage classification, learning with noisy labels, semi-supervised learning, andmedical image analysis tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2304.12666v2 |
| 524 | Score-Based Diffusion Models as Principled Priors for Inverse Imaging                                                                                | Berthy T. Feng                 | 2023-04-23     | cs.CV                                   | Priors are essential for reconstructing images from noisy and/or incompletemeasurements. The choice of the prior determines both the quality anduncertainty of recovered images. We propose turning score-based diffusionmodels into principled image priors ("score-based priors") for analyzing aposterior of images given measurements. Previously, probabilistic priors werelimited to handcrafted regularizers and simple distributions. In this work, weempirically validate the theoretically-proven probability function of ascore-based diffusion model. We show how to sample from resulting posteriors byusing this probability function for variational inference. Our results,including experiments on denoising, deblurring, and interferometric imaging,suggest that score-based priors enable principled inference with asophisticated, data-driven image prior.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2304.11751v2 |
| 525 | Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation                                                               | Cristiano Saltori              | 2023-04-23     | cs.CV, cs.AI, cs.LG                     | The ability to deploy robots that can operate safely in diverse environmentsis crucial for developing embodied intelligent agents. As a community, we havemade tremendous progress in within-domain LiDAR semantic segmentation. However,do these methods generalize across domains? To answer this question, we designthe first experimental setup for studying domain generalization (DG) for LiDARsemantic segmentation (DG-LSS). Our results confirm a significant gap betweenmethods, evaluated in a cross-domain setting: for example, a model trained onthe source dataset (SemanticKITTI) obtains $26.53$ mIoU on the target data,compared to $48.49$ mIoU obtained by the model trained on the target domain(nuScenes). To tackle this gap, we propose the first method specificallydesigned for DG-LSS, which obtains $34.88$ mIoU on the target domain,outperforming all baselines. Our method augments a sparse-convolutionalencoder-decoder 3D segmentation network with an additional, dense 2Dconvolutional decoder that learns to classify a birds-eye view of the pointcloud. This simple auxiliary task encourages the 3D network to learn featuresthat are robust to sensor placement shifts and resolution, and are transferableacross domains. With this work, we aim to inspire the community to develop andevaluate future models in such cross-domain conditions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2304.11705v2 |
| 526 | OmniLabel: A Challenging Benchmark for Language-Based Object Detection                                                                               | Samuel Schulter                | 2023-04-22     | cs.CV                                   | Language-based object detection is a promising direction towards building anatural interface to describe objects in images that goes far beyond plaincategory names. While recent methods show great progress in that direction,proper evaluation is lacking. With OmniLabel, we propose a novel taskdefinition, dataset, and evaluation metric. The task subsumes standard- andopen-vocabulary detection as well as referring expressions. With more than 28Kunique object descriptions on over 25K images, OmniLabel provides a challengingbenchmark with diverse and complex object descriptions in a naturallyopen-vocabulary setting. Moreover, a key differentiation to existing benchmarksis that our object descriptions can refer to one, multiple or even no object,hence, providing negative examples in free-form text. The proposed evaluationhandles the large label space and judges performance via a modified averageprecision metric, which we validate by evaluating strong language-basedbaselines. OmniLabel indeed provides a challenging test bed for future researchon language-based detection.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2304.11463v2 |
| 527 | The Devil is in the Upsampling: Architectural Decisions Made Simpler for Denoising with Deep Image Prior                                             | Yilin Liu                      | 2023-04-22     | eess.IV, cs.CV                          | Deep Image Prior (DIP) shows that some network architectures naturally biastowards smooth images and resist noises, a phenomenon known as spectral bias.Image denoising is an immediate application of this property. Although DIP hasremoved the requirement of large training sets, it still presents two practicalchallenges for denoising: architectural design and noise-fitting, which areoften intertwined. Existing methods mostly handcraft or search for thearchitecture from a large design space, due to the lack of understanding on howthe architectural choice corresponds to the image. In this study, we analyzefrom a frequency perspective to demonstrate that the unlearnt upsampling is themain driving force behind the denoising phenomenon in DIP. This finding thenleads to strategies for estimating a suitable architecture for every imagewithout a laborious search. Extensive experiments show that the estimatedarchitectures denoise and preserve the textural details better than currentmethods with up to 95% fewer parameters. The under-parameterized nature alsomakes them especially robust to a higher level of noise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2304.11409v2 |
| 528 | Implicit Temporal Modeling with Learnable Alignment for Video Recognition                                                                            | Shuyuan Tu                     | 2023-04-20     | cs.CV, cs.AI                            | Contrastive language-image pretraining (CLIP) has demonstrated remarkablesuccess in various image tasks. However, how to extend CLIP with effectivetemporal modeling is still an open and crucial problem. Existing factorized orjoint spatial-temporal modeling trades off between the efficiency andperformance. While modeling temporal information within straight through tubeis widely adopted in literature, we find that simple frame alignment alreadyprovides enough essence without temporal attention. To this end, in this paper,we proposed a novel Implicit Learnable Alignment (ILA) method, which minimizesthe temporal modeling effort while achieving incredibly high performance.Specifically, for a frame pair, an interactive point is predicted in eachframe, serving as a mutual information rich region. By enhancing the featuresaround the interactive point, two frames are implicitly aligned. The alignedfeatures are then pooled into a single token, which is leveraged in thesubsequent spatial self-attention. Our method allows eliminating the costly orinsufficient temporal self-attention in video. Extensive experiments onbenchmarks demonstrate the superiority and generality of our module.Particularly, the proposed ILA achieves a top-1 accuracy of 88.7% onKinetics-400 with much fewer FLOPs compared with Swin-L and ViViT-H. Code isreleased at https://github.com/Francis-Rings/ILA .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2304.10465v2 |
| 529 | SINC: Spatial Composition of 3D Human Motions for Simultaneous Action Generation                                                                     | Nikos Athanasiou               | 2023-04-20     | cs.CV                                   | Our goal is to synthesize 3D human motions given textual inputs describingsimultaneous actions, for example 'waving hand' while 'walking' at the sametime. We refer to generating such simultaneous movements as performing 'spatialcompositions'. In contrast to temporal compositions that seek to transitionfrom one action to another, spatial compositing requires understanding whichbody parts are involved in which action, to be able to move themsimultaneously. Motivated by the observation that the correspondence betweenactions and body parts is encoded in powerful language models, we extract thisknowledge by prompting GPT-3 with text such as "what are the body partsinvolved in the action <action name>?", while also providing the parts list andfew-shot examples. Given this action-part mapping, we combine body parts fromtwo motions together and establish the first automated method to spatiallycompose two actions. However, training data with compositional actions isalways limited by the combinatorics. Hence, we further create synthetic datawith this approach, and use it to train a new state-of-the-art text-to-motiongeneration model, called SINC ("SImultaneous actioN Compositions for 3D humanmotions"). In our experiments, that training with such GPT-guided syntheticdata improves spatial composition generation over baselines. Our code ispublicly available at https://sinc.is.tue.mpg.de/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2304.10417v2 |
| 530 | Tetra-NeRF: Representing Neural Radiance Fields Using Tetrahedra                                                                                     | Jonas Kulhanek                 | 2023-04-19     | cs.CV, cs.GR, cs.LG                     | Neural Radiance Fields (NeRFs) are a very recent and very popular approachfor the problems of novel view synthesis and 3D reconstruction. A popular scenerepresentation used by NeRFs is to combine a uniform, voxel-based subdivisionof the scene with an MLP. Based on the observation that a (sparse) point cloudof the scene is often available, this paper proposes to use an adaptiverepresentation based on tetrahedra obtained by Delaunay triangulation insteadof uniform subdivision or point-based representations. We show that such arepresentation enables efficient training and leads to state-of-the-artresults. Our approach elegantly combines concepts from 3D geometry processing,triangle-based rendering, and modern neural radiance fields. Compared tovoxel-based representations, ours provides more detail around parts of thescene likely to be close to the surface. Compared to point-basedrepresentations, our approach achieves better performance. The source code ispublicly available at: https://jkulhanek.com/tetra-nerf.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2304.09987v3 |
| 531 | Self-supervised Image Denoising with Downsampled Invariance Loss and Conditional Blind-Spot Network                                                  | Yeong Il Jang                  | 2023-04-19     | eess.IV, cs.CV                          | There have been many image denoisers using deep neural networks, whichoutperform conventional model-based methods by large margins. Recently,self-supervised methods have attracted attention because constructing a largereal noise dataset for supervised training is an enormous burden. The mostrepresentative self-supervised denoisers are based on blind-spot networks,which exclude the receptive field's center pixel. However, excluding any inputpixel is abandoning some information, especially when the input pixel at thecorresponding output position is excluded. In addition, a standard blind-spotnetwork fails to reduce real camera noise due to the pixel-wise correlation ofnoise, though it successfully removes independently distributed syntheticnoise. Hence, to realize a more practical denoiser, we propose a novelself-supervised training framework that can remove real noise. For this, wederive the theoretic upper bound of a supervised loss where the network isguided by the downsampled blinded output. Also, we design a conditionalblind-spot network (C-BSN), which selectively controls the blindness of thenetwork to use the center pixel information. Furthermore, we exploit a randomsubsampler to decorrelate noise spatially, making the C-BSN free of visualartifacts that were often seen in downsample-based methods. Extensiveexperiments show that the proposed C-BSN achieves state-of-the-art performanceon real-world datasets as a self-supervised denoiser and shows qualitativelypleasing results without any post-processing or refinement.                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2304.09507v2 |
| 532 | UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer                                                              | Soon Yau Cheong                | 2023-04-18     | cs.CV                                   | Text-to-image models (T2I) such as StableDiffusion have been used to generatehigh quality images of people. However, due to the random nature of thegeneration process, the person has a different appearance e.g. pose, face, andclothing, despite using the same text prompt. The appearance inconsistencymakes T2I unsuitable for pose transfer. We address this by proposing amultimodal diffusion model that accepts text, pose, and visual prompting. Ourmodel is the first unified method to perform all person image tasks -generation, pose transfer, and mask-less edit. We also pioneer using smalldimensional 3D body model parameters directly to demonstrate new capability -simultaneous pose and camera view interpolation while maintaining the person'sappearance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2304.08870v2 |
| 533 | Self-Supervised 3D Action Representation Learning with Skeleton Cloud Colorization                                                                   | Siyuan Yang                    | 2023-04-18     | cs.CV, cs.AI                            | 3D Skeleton-based human action recognition has attracted increasing attentionin recent years. Most of the existing work focuses on supervised learning whichrequires a large number of labeled action sequences that are often expensiveand time-consuming to annotate. In this paper, we address self-supervised 3Daction representation learning for skeleton-based action recognition. Weinvestigate self-supervised representation learning and design a novel skeletoncloud colorization technique that is capable of learning spatial and temporalskeleton representations from unlabeled skeleton sequence data. We represent askeleton action sequence as a 3D skeleton cloud and colorize each point in thecloud according to its temporal and spatial orders in the original(unannotated) skeleton sequence. Leveraging the colorized skeleton point cloud,we design an auto-encoder framework that can learn spatial-temporal featuresfrom the artificial color labels of skeleton joints effectively. Specifically,we design a two-steam pretraining network that leverages fine-grained andcoarse-grained colorization to learn multi-scale spatial-temporal features. Inaddition, we design a Masked Skeleton Cloud Repainting task that can pretrainthe designed auto-encoder framework to learn informative representations. Weevaluate our skeleton cloud colorization approach with linear classifierstrained under different configurations, including unsupervised,semi-supervised, fully-supervised, and transfer learning settings. Extensiveexperiments on NTU RGB+D, NTU RGB+D 120, PKU-MMD, NW-UCLA, and UWA3D datasetsshow that the proposed method outperforms existing unsupervised andsemi-supervised 3D action recognition methods by large margins and achievescompetitive performance in supervised 3D action recognition as well.                                                                                            | http://arxiv.org/abs/2304.08799v2 |
| 534 | Pretrained Language Models as Visual Planners for Human Assistance                                                                                   | Dhruvesh Patel                 | 2023-04-17     | cs.CV, cs.AI                            | In our pursuit of advancing multi-modal AI assistants capable of guidingusers to achieve complex multi-step goals, we propose the task of "VisualPlanning for Assistance (VPA)". Given a succinct natural language goal, e.g.,"make a shelf", and a video of the user's progress so far, the aim of VPA is todevise a plan, i.e., a sequence of actions such as "sand shelf", "paint shelf",etc. to realize the specified goal. This requires assessing the user's progressfrom the (untrimmed) video, and relating it to the requirements of naturallanguage goal, i.e., which actions to select and in what order? Consequently,this requires handling long video history and arbitrarily complex actiondependencies. To address these challenges, we decompose VPA into video actionsegmentation and forecasting. Importantly, we experiment by formulating theforecasting step as a multi-modal sequence modeling problem, allowing us toleverage the strength of pre-trained LMs (as the sequence model). This novelapproach, which we call Visual Language Model based Planner (VLaMP),outperforms baselines across a suite of metrics that gauge the quality of thegenerated plans. Furthermore, through comprehensive ablations, we also isolatethe value of each component--language pre-training, visual observations, andgoal information. We have open-sourced all the data, model checkpoints, andtraining code.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2304.09179v3 |
| 535 | Instance-aware Dynamic Prompt Tuning for Pre-trained Point Cloud Models                                                                              | Yaohua Zha                     | 2023-04-14     | cs.CV                                   | Pre-trained point cloud models have found extensive applications in 3Dunderstanding tasks like object classification and part segmentation. However,the prevailing strategy of full fine-tuning in downstream tasks leads to largeper-task storage overhead for model parameters, which limits the efficiencywhen applying large-scale pre-trained models. Inspired by the recent success ofvisual prompt tuning (VPT), this paper attempts to explore prompt tuning onpre-trained point cloud models, to pursue an elegant balance betweenperformance and parameter efficiency. We find while instance-agnostic staticprompting, e.g. VPT, shows some efficacy in downstream transfer, it isvulnerable to the distribution diversity caused by various types of noises inreal-world point cloud data. To conquer this limitation, we propose a novelInstance-aware Dynamic Prompt Tuning (IDPT) strategy for pre-trained pointcloud models. The essence of IDPT is to develop a dynamic prompt generationmodule to perceive semantic prior features of each point cloud instance andgenerate adaptive prompt tokens to enhance the model's robustness. Notably,extensive experiments demonstrate that IDPT outperforms full fine-tuning inmost tasks with a mere 7% of the trainable parameters, providing a promisingsolution to parameter-efficient learning for pre-trained point cloud models.Code is available at \url{https://github.com/zyh16143998882/ICCV23-IDPT}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2304.07221v2 |
| 536 | Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction                                                                  | Hansheng Chen                  | 2023-04-13     | cs.CV                                   | 3D-aware image synthesis encompasses a variety of tasks, such as scenegeneration and novel view synthesis from images. Despite numerous task-specificmethods, developing a comprehensive model remains challenging. In this paper,we present SSDNeRF, a unified approach that employs an expressive diffusionmodel to learn a generalizable prior of neural radiance fields (NeRF) frommulti-view images of diverse objects. Previous studies have used two-stageapproaches that rely on pretrained NeRFs as real data to train diffusionmodels. In contrast, we propose a new single-stage training paradigm with anend-to-end objective that jointly optimizes a NeRF auto-decoder and a latentdiffusion model, enabling simultaneous 3D reconstruction and prior learning,even from sparsely available views. At test time, we can directly sample thediffusion prior for unconditional generation, or combine it with arbitraryobservations of unseen objects for NeRF reconstruction. SSDNeRF demonstratesrobust results comparable to or better than leading task-specific methods inunconditional generation and single/sparse-view 3D reconstruction.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2304.06714v4 |
| 537 | What does CLIP know about a red circle? Visual prompt engineering for VLMs                                                                           | Aleksandar Shtedritski         | 2023-04-13     | cs.CV                                   | Large-scale Vision-Language Models, such as CLIP, learn powerful image-textrepresentations that have found numerous applications, from zero-shotclassification to text-to-image generation. Despite that, their capabilitiesfor solving novel discriminative tasks via prompting fall behind those of largelanguage models, such as GPT-3. Here we explore the idea of visual promptengineering for solving computer vision tasks beyond classification by editingin image space instead of text. In particular, we discover an emergent abilityof CLIP, where, by simply drawing a red circle around an object, we can directthe model's attention to that region, while also maintaining globalinformation. We show the power of this simple approach by achievingstate-of-the-art in zero-shot referring expressions comprehension and strongperformance in keypoint localization tasks. Finally, we draw attention to somepotential ethical concerns of large language-vision models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2304.06712v2 |
| 538 | DynaMITe: Dynamic Query Bootstrapping for Multi-object Interactive Segmentation Transformer                                                          | Amit Kumar Rana                | 2023-04-13     | cs.CV                                   | Most state-of-the-art instance segmentation methods rely on large amounts ofpixel-precise ground-truth annotations for training, which are expensive tocreate. Interactive segmentation networks help generate such annotations basedon an image and the corresponding user interactions such as clicks. Existingmethods for this task can only process a single instance at a time and eachuser interaction requires a full forward pass through the entire deep network.We introduce a more efficient approach, called DynaMITe, in which we representuser interactions as spatio-temporal queries to a Transformer decoder with apotential to segment multiple object instances in a single iteration. Ourarchitecture also alleviates any need to re-compute image features duringrefinement, and requires fewer interactions for segmenting multiple instancesin a single image when compared to other methods. DynaMITe achievesstate-of-the-art results on multiple existing interactive segmentationbenchmarks, and also on the new multi-instance benchmark that we propose inthis paper.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2304.06668v2 |
| 539 | Instance Neural Radiance Field                                                                                                                       | Yichen Liu                     | 2023-04-10     | cs.CV                                   | This paper presents one of the first learning-based NeRF 3D instancesegmentation pipelines, dubbed as {\bf \inerflong}, or \inerf. Taking a NeRFpretrained from multi-view RGB images as input, \inerf can learn 3D instancesegmentation of a given scene, represented as an instance field component ofthe NeRF model. To this end, we adopt a 3D proposal-based mask predictionnetwork on the sampled volumetric features from NeRF, which generates discrete3D instance masks. The coarse 3D mask prediction is then projected to imagespace to match 2D segmentation masks from different views generated by existingpanoptic segmentation models, which are used to supervise the training of theinstance field. Notably, beyond generating consistent 2D segmentation maps fromnovel views, \inerf can query instance information at any 3D point, whichgreatly enhances NeRF object segmentation and manipulation. Our method is alsoone of the first to achieve such results in pure inference. Experimented onsynthetic and real-world NeRF datasets with complex indoor scenes, \inerfsurpasses previous NeRF segmentation works and competitive 2D segmentationmethods in segmentation performance on unseen views. Watch the demo video athttps://youtu.be/wW9Bme73coI. Code and data are available athttps://github.com/lyclyc52/Instance_NeRF.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2304.04395v2 |
| 540 | Improving automatic endoscopic stone recognition using a multi-view fusion approach enhanced with two-step transfer learning                         | Francisco Lopez-Tiro           | 2023-04-06     | eess.IV, cs.CV, cs.LG                   | This contribution presents a deep-learning method for extracting and fusingimage information acquired from different viewpoints, with the aim to producemore discriminant object features for the identification of the type of kidneystones seen in endoscopic images. The model was further improved with atwo-step transfer learning approach and by attention blocks to refine thelearned feature maps. Deep feature fusion strategies improved the results ofsingle view extraction backbone models by more than 6% in terms of accuracy ofthe kidney stones classification.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2304.03193v2 |
| 541 | How to choose your best allies for a transferable attack?                                                                                            | Thibault Maho                  | 2023-04-05     | cs.CR, cs.AI                            | The transferability of adversarial examples is a key issue in the security ofdeep neural networks. The possibility of an adversarial example crafted for asource model fooling another targeted model makes the threat of adversarialattacks more realistic. Measuring transferability is a crucial problem, but theAttack Success Rate alone does not provide a sound evaluation. This paperproposes a new methodology for evaluating transferability by putting distortionin a central position. This new tool shows that transferable attacks mayperform far worse than a black box attack if the attacker randomly picks thesource model. To address this issue, we propose a new selection mechanism,called FiT, which aims at choosing the best source model with only a fewpreliminary queries to the target. Our experimental results show that FiT ishighly effective at selecting the best source model for multiple scenarios suchas single-model attacks, ensemble-model attacks and multiple attacks (Codeavailable at: https://github.com/t-maho/transferability_measure_fit).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2304.02312v2 |
| 542 | Multimodal Garment Designer: Human-Centric Latent Diffusion Models for Fashion Image Editing                                                         | Alberto Baldrati               | 2023-04-04     | cs.CV, cs.AI, cs.MM                     | Fashion illustration is used by designers to communicate their vision and tobring the design idea from conceptualization to realization, showing howclothes interact with the human body. In this context, computer vision can thusbe used to improve the fashion design process. Differently from previous worksthat mainly focused on the virtual try-on of garments, we propose the task ofmultimodal-conditioned fashion image editing, guiding the generation ofhuman-centric fashion images by following multimodal prompts, such as text,human body poses, and garment sketches. We tackle this problem by proposing anew architecture based on latent diffusion models, an approach that has notbeen used before in the fashion domain. Given the lack of existing datasetssuitable for the task, we also extend two existing fashion datasets, namelyDress Code and VITON-HD, with multimodal annotations collected in asemi-automatic manner. Experimental results on these new datasets demonstratethe effectiveness of our proposal, both in terms of realism and coherence withthe given multimodal inputs. Source code and collected multimodal annotationsare publicly available at:https://github.com/aimagelab/multimodal-garment-designer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2304.02051v2 |
| 543 | Black Box Few-Shot Adaptation for Vision-Language models                                                                                             | Yassine Ouali                  | 2023-04-04     | cs.CV, cs.CL, cs.LG                     | Vision-Language (V-L) models trained with contrastive learning to align thevisual and language modalities have been shown to be strong few-shot learners.Soft prompt learning is the method of choice for few-shot downstream adaptationaiming to bridge the modality gap caused by the distribution shift induced bythe new domain. While parameter-efficient, prompt learning still requiresaccess to the model weights and can be computationally infeasible for largemodels with billions of parameters. To address these shortcomings, in thiswork, we describe a black-box method for V-L few-shot adaptation that (a)operates on pre-computed image and text features and hence works without accessto the model's weights, (b) it is orders of magnitude faster at training time,(c) it is amenable to both supervised and unsupervised training, and (d) it canbe even used to align image and text features computed from uni-modal models.To achieve this, we propose Linear Feature Alignment (LFA), a simple linearapproach for V-L re-alignment in the target domain. LFA is initialized from aclosed-form solution to a least-squares problem and then it is iterativelyupdated by minimizing a re-ranking loss. Despite its simplicity, our approachcan even surpass soft-prompt learning methods as shown by extensive experimentson 11 image and 2 video datasets.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2304.01752v3 |
| 544 | Improved Visual Fine-tuning with Natural Language Supervision                                                                                        | Junyang Wang                   | 2023-04-04     | cs.CV                                   | Fine-tuning a visual pre-trained model can leverage the semantic informationfrom large-scale pre-training data and mitigate the over-fitting problem ondownstream vision tasks with limited training examples. While the problem ofcatastrophic forgetting in pre-trained backbone has been extensively studiedfor fine-tuning, its potential bias from the corresponding pre-training taskand data, attracts less attention. In this work, we investigate this problem bydemonstrating that the obtained classifier after fine-tuning will be close tothat induced by the pre-trained model. To reduce the bias in the classifiereffectively, we introduce a reference distribution obtained from a fixed textclassifier, which can help regularize the learned vision classifier. Theproposed method, Text Supervised fine-tuning (TeS), is evaluated with diversepre-trained vision models including ResNet and ViT, and text encoders includingBERT and CLIP, on 11 downstream tasks. The consistent improvement with a clearmargin over distinct scenarios confirms the effectiveness of our proposal. Codeis available at \url{https://github.com/idstcv/TeS}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2304.01489v2 |
| 545 | FineRecon: Depth-aware Feed-forward Network for Detailed 3D Reconstruction                                                                           | Noah Stier                     | 2023-04-04     | cs.CV                                   | Recent works on 3D reconstruction from posed images have demonstrated thatdirect inference of scene-level 3D geometry without test-time optimization isfeasible using deep neural networks, showing remarkable promise and highefficiency. However, the reconstructed geometry, typically represented as a 3Dtruncated signed distance function (TSDF), is often coarse without finegeometric details. To address this problem, we propose three effectivesolutions for improving the fidelity of inference-based 3D reconstructions. Wefirst present a resolution-agnostic TSDF supervision strategy to provide thenetwork with a more accurate learning signal during training, avoiding thepitfalls of TSDF interpolation seen in previous work. We then introduce a depthguidance strategy using multi-view depth estimates to enhance the scenerepresentation and recover more accurate surfaces. Finally, we develop a novelarchitecture for the final layers of the network, conditioning the output TSDFprediction on high-resolution image features in addition to coarse voxelfeatures, enabling sharper reconstruction of fine details. Our method,FineRecon, produces smooth and highly accurate reconstructions, showingsignificant improvements across multiple depth and 3D reconstruction metrics.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2304.01480v2 |
| 546 | CRN: Camera Radar Net for Accurate, Robust, Efficient 3D Perception                                                                                  | Youngseok Kim                  | 2023-04-03     | cs.CV, cs.AI, cs.RO                     | Autonomous driving requires an accurate and fast 3D perception system thatincludes 3D object detection, tracking, and segmentation. Although recentlow-cost camera-based approaches have shown promising results, they aresusceptible to poor illumination or bad weather conditions and have a largelocalization error. Hence, fusing camera with low-cost radar, which providesprecise long-range measurement and operates reliably in all environments, ispromising but has not yet been thoroughly investigated. In this paper, wepropose Camera Radar Net (CRN), a novel camera-radar fusion framework thatgenerates a semantically rich and spatially accurate bird's-eye-view (BEV)feature map for various tasks. To overcome the lack of spatial information inan image, we transform perspective view image features to BEV with the help ofsparse but accurate radar points. We further aggregate image and radar featuremaps in BEV using multi-modal deformable attention designed to tackle thespatial misalignment between inputs. CRN with real-time setting operates at 20FPS while achieving comparable performance to LiDAR detectors on nuScenes, andeven outperforms at a far distance on 100m setting. Moreover, CRN with offlinesetting yields 62.4% NDS, 57.5% mAP on nuScenes test set and ranks first amongall camera and camera-radar 3D object detectors.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2304.00670v2 |
| 547 | Weakly-Supervised Text-driven Contrastive Learning for Facial Behavior Understanding                                                                 | Xiang Zhang                    | 2023-03-31     | cs.CV                                   | Contrastive learning has shown promising potential for learning robustrepresentations by utilizing unlabeled data. However, constructing effectivepositive-negative pairs for contrastive learning on facial behavior datasetsremains challenging. This is because such pairs inevitably encode thesubject-ID information, and the randomly constructed pairs may push similarfacial images away due to the limited number of subjects in facial behaviordatasets. To address this issue, we propose to utilize activity descriptions,coarse-grained information provided in some datasets, which can providehigh-level semantic information about the image sequences but is oftenneglected in previous studies. More specifically, we introduce a two-stageContrastive Learning with Text-Embeded framework for Facial behaviorunderstanding (CLEF). The first stage is a weakly-supervised contrastivelearning method that learns representations from positive-negative pairsconstructed using coarse-grained activity information. The second stage aims totrain the recognition of facial expressions or facial action units bymaximizing the similarity between image and the corresponding text label names.The proposed CLEF achieves state-of-the-art performance on three in-the-labdatasets for AU recognition and three in-the-wild datasets for facialexpression recognition.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2304.00058v2 |
| 548 | LivePose: Online 3D Reconstruction from Monocular Video with Dynamic Camera Poses                                                                    | Noah Stier                     | 2023-03-31     | cs.CV                                   | Dense 3D reconstruction from RGB images traditionally assumes static camerapose estimates. This assumption has endured, even as recent works haveincreasingly focused on real-time methods for mobile devices. However, theassumption of a fixed pose for each image does not hold for online execution:poses from real-time SLAM are dynamic and may be updated following events suchas bundle adjustment and loop closure. This has been addressed in the RGB-Dsetting, by de-integrating past views and re-integrating them with updatedposes, but it remains largely untreated in the RGB-only setting. We formalizethis problem to define the new task of dense online reconstruction fromdynamically-posed images. To support further research, we introduce a datasetcalled LivePose containing the dynamic poses from a SLAM system running onScanNet. We select three recent reconstruction systems and apply a frameworkbased on de-integration to adapt each one to the dynamic-pose setting. Inaddition, we propose a novel, non-linear de-integration module that learns toremove stale scene content. We show that responding to pose updates is criticalfor high-quality reconstruction, and that our de-integration framework is aneffective solution.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2304.00054v2 |
| 549 | DIME-FM: DIstilling Multimodal and Efficient Foundation Models                                                                                       | Ximeng Sun                     | 2023-03-31     | cs.CV                                   | Large Vision-Language Foundation Models (VLFM), such as CLIP, ALIGN andFlorence, are trained on large-scale datasets of image-caption pairs andachieve superior transferability and robustness on downstream tasks, but theyare difficult to use in many practical applications due to their large size,high latency and fixed architectures. Unfortunately, recent work shows traininga small custom VLFM for resource-limited applications is currently verydifficult using public and smaller-scale data. In this paper, we introduce anew distillation mechanism (DIME-FM) that allows us to transfer the knowledgecontained in large VLFMs to smaller, customized foundation models using arelatively small amount of inexpensive, unpaired images and sentences. Wetransfer the knowledge from the pre-trained CLIP-ViTL/14 model to a ViT-B/32model, with only 40M public images and 28.4M unpaired public sentences. Theresulting model "Distill-ViT-B/32" rivals the CLIP-ViT-B/32 model pre-trainedon its private WiT dataset (400M image-text pairs): Distill-ViT-B/32 achievessimilar results in terms of zero-shot and linear-probing performance on bothImageNet and the ELEVATER (20 image classification tasks) benchmarks. It alsodisplays comparable robustness when evaluated on five datasets with naturaldistribution shifts from ImageNet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.18232v2 |
| 550 | Towards Nonlinear-Motion-Aware and Occlusion-Robust Rolling Shutter Correction                                                                       | Delin Qu                       | 2023-03-31     | cs.CV                                   | This paper addresses the problem of rolling shutter correction in complexnonlinear and dynamic scenes with extreme occlusion. Existing methods sufferfrom two main drawbacks. Firstly, they face challenges in estimating theaccurate correction field due to the uniform velocity assumption, leading tosignificant image correction errors under complex motion. Secondly, the drasticocclusion in dynamic scenes prevents current solutions from achieving betterimage quality because of the inherent difficulties in aligning and aggregatingmultiple frames. To tackle these challenges, we model the curvilineartrajectory of pixels analytically and propose a geometry-based QuadraticRolling Shutter (QRS) motion solver, which precisely estimates the high-ordercorrection field of individual pixels. Besides, to reconstruct high-qualityocclusion frames in dynamic scenes, we present a 3D video architecture thateffectively Aligns and Aggregates multi-frame context, namely, RSA2-Net. Weevaluate our method across a broad range of cameras and video sequences,demonstrating its significant superiority. Specifically, our method surpassesthe state-of-the-art by +4.98, +0.77, and +4.33 of PSNR on Carla-RS, Fastec-RS,and BS-RSC datasets, respectively. Code is available athttps://github.com/DelinQu/qrsc.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2303.18125v3 |
| 551 | Diffusion Action Segmentation                                                                                                                        | Daochang Liu                   | 2023-03-31     | cs.CV, eess.IV                          | Temporal action segmentation is crucial for understanding long-form videos.Previous works on this task commonly adopt an iterative refinement paradigm byusing multi-stage models. We propose a novel framework via denoising diffusionmodels, which nonetheless shares the same inherent spirit of such iterativerefinement. In this framework, action predictions are iteratively generatedfrom random noise with input video features as conditions. To enhance themodeling of three striking characteristics of human actions, including theposition prior, the boundary ambiguity, and the relational dependency, wedevise a unified masking strategy for the conditioning inputs in our framework.Extensive experiments on three benchmark datasets, i.e., GTEA, 50Salads, andBreakfast, are performed and the proposed method achieves superior orcomparable results to state-of-the-art methods, showing the effectiveness of agenerative approach for action segmentation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2303.17959v2 |
| 552 | AvatarCraft: Transforming Text into Neural Human Avatars with Parameterized Shape and Pose Control                                                   | Ruixiang Jiang                 | 2023-03-30     | cs.CV                                   | Neural implicit fields are powerful for representing 3D scenes and generatinghigh-quality novel views, but it remains challenging to use such implicitrepresentations for creating a 3D human avatar with a specific identity andartistic style that can be easily animated. Our proposed method, AvatarCraft,addresses this challenge by using diffusion models to guide the learning ofgeometry and texture for a neural avatar based on a single text prompt. Wecarefully design the optimization framework of neural implicit fields,including a coarse-to-fine multi-bounding box training strategy, shaperegularization, and diffusion-based constraints, to produce high-qualitygeometry and texture. Additionally, we make the human avatar animatable bydeforming the neural implicit field with an explicit warping field that mapsthe target human mesh to a template human mesh, both represented usingparametric human models. This simplifies animation and reshaping of thegenerated avatar by controlling pose and shape parameters. Extensiveexperiments on various text descriptions show that AvatarCraft is effective androbust in creating human avatars and rendering novel views, poses, and shapes.Our project page is: https://avatar-craft.github.io/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.17606v2 |
| 553 | Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts                                                                       | Dongyoon Han                   | 2023-03-30     | cs.CV, cs.LG                            | Supervised learning of image classifiers distills human knowledge into aparametric model through pairs of images and corresponding labels (X,Y). Weargue that this simple and widely used representation of human knowledgeneglects rich auxiliary information from the annotation procedure, such as thetime-series of mouse traces and clicks left after image selection. Our insightis that such annotation byproducts Z provide approximate human attention thatweakly guides the model to focus on the foreground cues, reducing spuriouscorrelations and discouraging shortcut learning. To verify this, we createImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched withsample-wise annotation byproducts, collected by replicating the respectiveoriginal annotation tasks. We refer to the new paradigm of training models withannotation byproducts as learning using annotation byproducts (LUAB). We showthat a simple multitask loss for regressing Z together with Y already improvesthe generalisability and robustness of the learned models. Compared to theoriginal supervised learning, LUAB does not require extra annotation costs.ImageNet-AB and COCO-AB are at https://github.com/naver-ai/NeglectedFreeLunch.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2303.17595v3 |
| 554 | Going Beyond Nouns With Vision & Language Models Using Synthetic Data                                                                                | Paola Cascante-Bonilla         | 2023-03-30     | cs.CV, cs.CL                            | Large-scale pre-trained Vision & Language (VL) models have shown remarkableperformance in many applications, enabling replacing a fixed set of supportedclasses with zero-shot open vocabulary reasoning over (almost arbitrary)natural language prompts. However, recent works have uncovered a fundamentalweakness of these models. For example, their difficulty to understand VisualLanguage Concepts (VLC) that go 'beyond nouns' such as the meaning ofnon-object words (e.g., attributes, actions, relations, states, etc.), ordifficulty in performing compositional reasoning such as understanding thesignificance of the order of the words in a sentence. In this work, weinvestigate to which extent purely synthetic data could be leveraged to teachthese models to overcome such shortcomings without compromising their zero-shotcapabilities. We contribute Synthetic Visual Concepts (SyViC) - a million-scalesynthetic dataset and data generation codebase allowing to generate additionalsuitable data to improve VLC understanding and compositional reasoning of VLmodels. Additionally, we propose a general VL finetuning strategy foreffectively leveraging SyViC towards achieving these improvements. Ourextensive experiments and ablations on VL-Checklist, Winoground, and ARObenchmarks demonstrate that it is possible to adapt strong pre-trained VLmodels with synthetic data significantly enhancing their VLC understanding(e.g. by 9.9% on ARO and 4.3% on VL-Checklist) with under 1% drop in theirzero-shot accuracy.                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2303.17590v2 |
| 555 | Discriminative Class Tokens for Text-to-Image Diffusion Models                                                                                       | Idan Schwartz                  | 2023-03-30     | cs.CV, cs.AI                            | Recent advances in text-to-image diffusion models have enabled the generationof diverse and high-quality images. While impressive, the images often fallshort of depicting subtle details and are susceptible to errors due toambiguity in the input text. One way of alleviating these issues is to traindiffusion models on class-labeled datasets. This approach has twodisadvantages: (i) supervised datasets are generally small compared tolarge-scale scraped text-image datasets on which text-to-image models aretrained, affecting the quality and diversity of the generated images, or (ii)the input is a hard-coded label, as opposed to free-form text, limiting thecontrol over the generated images.  In this work, we propose a non-invasive fine-tuning technique thatcapitalizes on the expressive potential of free-form text while achieving highaccuracy through discriminative signals from a pretrained classifier. This isdone by iteratively modifying the embedding of an added input token of atext-to-image diffusion model, by steering generated images toward a giventarget class according to a classifier. Our method is fast compared to priorfine-tuning methods and does not require a collection of in-class images orretraining of a noise-tolerant classifier. We evaluate our method extensively,showing that the generated images are: (i) more accurate and of higher qualitythan standard diffusion models, (ii) can be used to augment training data in alow-resource setting, and (iii) reveal information about the data used to trainthe guiding classifier. The code is available at\url{https://github.com/idansc/discriminative_class_tokens}.                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2303.17155v2 |
| 556 | ViewRefer: Grasp the Multi-view Knowledge for 3D Visual Grounding with GPT and Prototype Guidance                                                    | Zoey Guo                       | 2023-03-29     | cs.CV, cs.AI, cs.CL                     | Understanding 3D scenes from multi-view inputs has been proven to alleviatethe view discrepancy issue in 3D visual grounding. However, existing methodsnormally neglect the view cues embedded in the text modality and fail to weighthe relative importance of different views. In this paper, we proposeViewRefer, a multi-view framework for 3D visual grounding exploring how tograsp the view knowledge from both text and 3D modalities. For the text branch,ViewRefer leverages the diverse linguistic knowledge of large-scale languagemodels, e.g., GPT, to expand a single grounding text to multiplegeometry-consistent descriptions. Meanwhile, in the 3D modality, a transformerfusion module with inter-view attention is introduced to boost the interactionof objects across views. On top of that, we further present a set of learnablemulti-view prototypes, which memorize scene-agnostic knowledge for differentviews, and enhance the framework from two perspectives: a view-guided attentionmodule for more robust text features, and a view-guided scoring strategy duringthe final prediction. With our designed paradigm, ViewRefer achieves superiorperformance on three benchmarks and surpasses the second-best by +2.8%, +1.5%,and +1.35% on Sr3D, Nr3D, and ScanRefer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2303.16894v3 |
| 557 | Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation                                            | Wenhao Chai                    | 2023-03-29     | cs.CV                                   | When applying a pre-trained 2D-to-3D human pose lifting model to a targetunseen dataset, large performance degradation is commonly encountered due todomain shift issues. We observe that the degradation is caused by two factors:1) the large distribution gap over global positions of poses between the sourceand target datasets due to variant camera parameters and settings, and 2) thedeficient diversity of local structures of poses in training. To this end, wecombine \textbf{global adaptation} and \textbf{local generalization} in\textit{PoseDA}, a simple yet effective framework of unsupervised domainadaptation for 3D human pose estimation. Specifically, global adaptation aimsto align global positions of poses from the source domain to the target domainwith a proposed global position alignment (GPA) module. And localgeneralization is designed to enhance the diversity of 2D-3D pose mapping witha local pose augmentation (LPA) module. These modules bring significantperformance improvement without introducing additional learnable parameters. Inaddition, we propose local pose augmentation (LPA) to enhance the diversity of3D poses following an adversarial training scheme consisting of 1) aaugmentation generator that generates the parameters of pre-defined posetransformations and 2) an anchor discriminator to ensure the reality andquality of the augmented data. Our approach can be applicable to almost all2D-3D lifting models. \textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHPunder a cross-dataset evaluation setup, improving upon the previousstate-of-the-art method by 10.2\%.                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2303.16456v2 |
| 558 | SparseNeRF: Distilling Depth Ranking for Few-shot Novel View Synthesis                                                                               | Guangcong Wang                 | 2023-03-28     | cs.CV                                   | Neural Radiance Field (NeRF) significantly degrades when only a limitednumber of views are available. To complement the lack of 3D information,depth-based models, such as DSNeRF and MonoSDF, explicitly assume theavailability of accurate depth maps of multiple views. They linearly scale theaccurate depth maps as supervision to guide the predicted depth of few-shotNeRFs. However, accurate depth maps are difficult and expensive to capture dueto wide-range depth distances in the wild.  In this work, we present a new Sparse-view NeRF (SparseNeRF) framework thatexploits depth priors from real-world inaccurate observations. The inaccuratedepth observations are either from pre-trained depth models or coarse depthmaps of consumer-level depth sensors. Since coarse depth maps are not strictlyscaled to the ground-truth depth maps, we propose a simple yet effectiveconstraint, a local depth ranking method, on NeRFs such that the expected depthranking of the NeRF is consistent with that of the coarse depth maps in localpatches. To preserve the spatial continuity of the estimated depth of NeRF, wefurther propose a spatial continuity constraint to encourage the consistency ofthe expected depth continuity of NeRF with coarse depth maps. Surprisingly,with simple depth ranking constraints, SparseNeRF outperforms allstate-of-the-art few-shot NeRF methods (including depth-based models) onstandard LLFF and DTU datasets. Moreover, we collect a new dataset NVS-RGBDthat contains real-world depth maps from Azure Kinect, ZED 2, and iPhone 13Pro. Extensive experiments on NVS-RGBD dataset also validate the superiorityand generalizability of SparseNeRF. Code and dataset are available athttps://sparsenerf.github.io/.                                                                                                                                                                            | http://arxiv.org/abs/2303.16196v2 |
| 559 | HiLo: Exploiting High Low Frequency Relations for Unbiased Panoptic Scene Graph Generation                                                           | Zijian Zhou                    | 2023-03-28     | cs.CV, cs.AI                            | Panoptic Scene Graph generation (PSG) is a recently proposed task in imagescene understanding that aims to segment the image and extract triplets ofsubjects, objects and their relations to build a scene graph. This task isparticularly challenging for two reasons. First, it suffers from a long-tailproblem in its relation categories, making naive biased methods more inclinedto high-frequency relations. Existing unbiased methods tackle the long-tailproblem by data/loss rebalancing to favor low-frequency relations. Second, asubject-object pair can have two or more semantically overlapping relations.While existing methods favor one over the other, our proposed HiLo frameworklets different network branches specialize on low and high frequency relations,enforce their consistency and fuse the results. To the best of our knowledge weare the first to propose an explicitly unbiased PSG method. In extensiveexperiments we show that our HiLo framework achieves state-of-the-art resultson the PSG task. We also apply our method to the Scene Graph Generation taskthat predicts boxes instead of masks and see improvements over all baselinemethods. Code is available at https://github.com/franciszzj/HiLo.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2303.15994v2 |
| 560 | Bi-Encoder Cascades for Efficient Image Search                                                                                                       | Robert Hönig                   | 2023-03-27     | cs.IR                                   | Modern neural encoders offer unprecedented text-image retrieval (TIR)accuracy, but their high computational cost impedes an adoption to large-scaleimage searches. To lower this cost, model cascades use an expensive encoder torefine the ranking of a cheap encoder. However, existing cascading algorithmsfocus on cross-encoders, which jointly process text-image pairs, but do notconsider cascades of bi-encoders, which separately process texts and images. Weintroduce the small-world search scenario as a realistic setting wherebi-encoder cascades can reduce costs. We then propose a cascading algorithmthat leverages the small-world search scenario to reduce lifetime imageencoding costs of a TIR system. Our experiments show cost reductions by up to6x.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.15595v2 |
| 561 | SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications                                                 | Abdelrahman Shaker             | 2023-03-27     | cs.CV                                   | Self-attention has become a defacto choice for capturing global context invarious vision applications. However, its quadratic computational complexitywith respect to image resolution limits its use in real-time applications,especially for deployment on resource-constrained mobile devices. Althoughhybrid approaches have been proposed to combine the advantages of convolutionsand self-attention for a better speed-accuracy trade-off, the expensive matrixmultiplication operations in self-attention remain a bottleneck. In this work,we introduce a novel efficient additive attention mechanism that effectivelyreplaces the quadratic matrix multiplication operations with linearelement-wise multiplications. Our design shows that the key-value interactioncan be replaced with a linear layer without sacrificing any accuracy. Unlikeprevious state-of-the-art methods, our efficient formulation of self-attentionenables its usage at all stages of the network. Using our proposed efficientadditive attention, we build a series of models called "SwiftFormer" whichachieves state-of-the-art performance in terms of both accuracy and mobileinference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracywith only 0.8 ms latency on iPhone 14, which is more accurate and 2x fastercompared to MobileViT-v2. Code: https://github.com/Amshaker/SwiftFormer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2303.15446v2 |
| 562 | The Stable Signature: Rooting Watermarks in Latent Diffusion Models                                                                                  | Pierre Fernandez               | 2023-03-27     | cs.CV, cs.AI                            | Generative image modeling enables a wide range of applications but raisesethical concerns about responsible deployment. This paper introduces an activestrategy combining image watermarking and Latent Diffusion Models. The goal isfor all generated images to conceal an invisible watermark allowing for futuredetection and/or identification. The method quickly fine-tunes the latentdecoder of the image generator, conditioned on a binary signature. Apre-trained watermark extractor recovers the hidden signature from anygenerated image and a statistical test then determines whether it comes fromthe generative model. We evaluate the invisibility and robustness of thewatermarks on a variety of generation tasks, showing that Stable Signatureworks even after the images are modified. For instance, it detects the originof an image generated from a text prompt, then cropped to keep $10\%$ of thecontent, with $90$+$\%$ accuracy at a false positive rate below 10$^{-6}$.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2303.15435v2 |
| 563 | DiffTAD: Temporal Action Detection with Proposal Denoising Diffusion                                                                                 | Sauradip Nag                   | 2023-03-27     | cs.CV, cs.AI, cs.LG, cs.MM              | We propose a new formulation of temporal action detection (TAD) withdenoising diffusion, DiffTAD in short. Taking as input random temporalproposals, it can yield action proposals accurately given an untrimmed longvideo. This presents a generative modeling perspective, against previousdiscriminative learning manners. This capability is achieved by first diffusingthe ground-truth proposals to random ones (i.e., the forward/noising process)and then learning to reverse the noising process (i.e., the backward/denoisingprocess). Concretely, we establish the denoising process in the Transformerdecoder (e.g., DETR) by introducing a temporal location query design withfaster convergence in training. We further propose a cross-step selectiveconditioning algorithm for inference acceleration. Extensive evaluations onActivityNet and THUMOS show that our DiffTAD achieves top performance comparedto previous art alternatives. The code will be made available athttps://github.com/sauradip/DiffusionTAD.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2303.14863v2 |
| 564 | Sat2Density: Faithful Density Learning from Satellite-Ground Image Pairs                                                                             | Ming Qian                      | 2023-03-26     | cs.CV                                   | This paper aims to develop an accurate 3D geometry representation ofsatellite images using satellite-ground image pairs. Our focus is on thechallenging problem of 3D-aware ground-views synthesis from a satellite image.We draw inspiration from the density field representation used in volumetricneural rendering and propose a new approach, called Sat2Density. Our methodutilizes the properties of ground-view panoramas for the sky and non-skyregions to learn faithful density fields of 3D scenes in a geometricperspective. Unlike other methods that require extra depth information duringtraining, our Sat2Density can automatically learn accurate and faithful 3Dgeometry via density representation without depth supervision. This advancementsignificantly improves the ground-view panorama synthesis task. Additionally,our study provides a new geometric perspective to understand the relationshipbetween satellite and ground-view images in 3D space.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2303.14672v2 |
| 565 | Human Preference Score: Better Aligning Text-to-Image Models with Human Preference                                                                   | Xiaoshi Wu                     | 2023-03-25     | cs.CV, cs.AI                            | Recent years have witnessed a rapid growth of deep generative models, withtext-to-image models gaining significant attention from the public. However,existing models often generate images that do not align well with humanpreferences, such as awkward combinations of limbs and facial expressions. Toaddress this issue, we collect a dataset of human choices on generated imagesfrom the Stable Foundation Discord channel. Our experiments demonstrate thatcurrent evaluation metrics for generative models do not correlate well withhuman choices. Thus, we train a human preference classifier with the collecteddataset and derive a Human Preference Score (HPS) based on the classifier.Using HPS, we propose a simple yet effective method to adapt Stable Diffusionto better align with human preferences. Our experiments show that HPSoutperforms CLIP in predicting human choices and has good generalizationcapability toward images generated from other models. By tuning StableDiffusion with the guidance of HPS, the adapted model is able to generateimages that are more preferred by human users. The project page is availablehere: https://tgxs002.github.io/align_sd_web/ .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2303.14420v2 |
| 566 | FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization                                                                        | Pavan Kumar Anasosalu Vasu     | 2023-03-24     | cs.CV                                   | The recent amalgamation of transformer and convolutional designs has led tosteady improvements in accuracy and efficiency of the models. In this work, weintroduce FastViT, a hybrid vision transformer architecture that obtains thestate-of-the-art latency-accuracy trade-off. To this end, we introduce a noveltoken mixing operator, RepMixer, a building block of FastViT, that usesstructural reparameterization to lower the memory access cost by removingskip-connections in the network. We further apply train-timeoverparametrization and large kernel convolutions to boost accuracy andempirically show that these choices have minimal effect on latency. We showthat - our model is 3.5x faster than CMT, a recent state-of-the-art hybridtransformer architecture, 4.9x faster than EfficientNet, and 1.9x faster thanConvNeXt on a mobile device for the same accuracy on the ImageNet dataset. Atsimilar latency, our model obtains 4.2% better Top-1 accuracy on ImageNet thanMobileOne. Our model consistently outperforms competing architectures acrossseveral tasks -- image classification, detection, segmentation and 3D meshregression with significant improvement in latency on both a mobile device anda desktop GPU. Furthermore, our model is highly robust to out-of-distributionsamples and corruptions, improving over competing robust models. Code andmodels are available at https://github.com/apple/ml-fastvit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2303.14189v2 |
| 567 | Ablating Concepts in Text-to-Image Diffusion Models                                                                                                  | Nupur Kumari                   | 2023-03-23     | cs.CV, cs.GR, cs.LG                     | Large-scale text-to-image diffusion models can generate high-fidelity imageswith powerful compositional ability. However, these models are typicallytrained on an enormous amount of Internet data, often containing copyrightedmaterial, licensed images, and personal photos. Furthermore, they have beenfound to replicate the style of various living artists or memorize exacttraining samples. How can we remove such copyrighted concepts or images withoutretraining the model from scratch? To achieve this goal, we propose anefficient method of ablating concepts in the pretrained model, i.e., preventingthe generation of a target concept. Our algorithm learns to match the imagedistribution for a target style, instance, or text prompt we wish to ablate tothe distribution corresponding to an anchor concept. This prevents the modelfrom generating target concepts given its text condition. Extensive experimentsshow that our method can successfully prevent the generation of the ablatedconcept while preserving closely related concepts in the model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2303.13516v3 |
| 568 | A Large-scale Study of Spatiotemporal Representation Learning with a New Benchmark on Action Recognition                                             | Andong Deng                    | 2023-03-23     | cs.CV                                   | The goal of building a benchmark (suite of datasets) is to provide a unifiedprotocol for fair evaluation and thus facilitate the evolution of a specificarea. Nonetheless, we point out that existing protocols of action recognitioncould yield partial evaluations due to several limitations. To comprehensivelyprobe the effectiveness of spatiotemporal representation learning, we introduceBEAR, a new BEnchmark on video Action Recognition. BEAR is a collection of 18video datasets grouped into 5 categories (anomaly, gesture, daily, sports, andinstructional), which covers a diverse set of real-world applications. WithBEAR, we thoroughly evaluate 6 common spatiotemporal models pre-trained by bothsupervised and self-supervised learning. We also report transfer performancevia standard finetuning, few-shot finetuning, and unsupervised domainadaptation. Our observation suggests that current state-of-the-art cannotsolidly guarantee high performance on datasets close to real-worldapplications, and we hope BEAR can serve as a fair and challenging evaluationbenchmark to gain insights on building next-generation spatiotemporal learners.Our dataset, code, and models are released at:https://github.com/AndongDeng/BEAR                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2303.13505v2 |
| 569 | Chordal Averaging on Flag Manifolds and Its Applications                                                                                             | Nathan Mankovich               | 2023-03-23     | cs.CV, cs.LG, math.DG, math.OC, stat.ML | This paper presents a new, provably-convergent algorithm for computing theflag-mean and flag-median of a set of points on a flag manifold under thechordal metric. The flag manifold is a mathematical space consisting of flags,which are sequences of nested subspaces of a vector space that increase indimension. The flag manifold is a superset of a wide range of known matrixspaces, including Stiefel and Grassmanians, making it a general object that isuseful in a wide variety computer vision problems.  To tackle the challenge of computing first order flag statistics, we firsttransform the problem into one that involves auxiliary variables constrained tothe Stiefel manifold. The Stiefel manifold is a space of orthogonal frames, andleveraging the numerical stability and efficiency of Stiefel-manifoldoptimization enables us to compute the flag-mean effectively. Through a seriesof experiments, we show the competence of our method in Grassmann and rotationaveraging, as well as principal component analysis. We release our source codeunder https://github.com/nmank/FlagAveraging.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.13501v2 |
| 570 | Visually-Prompted Language Model for Fine-Grained Scene Graph Generation in an Open World                                                            | Qifan Yu                       | 2023-03-23     | cs.CV                                   | Scene Graph Generation (SGG) aims to extract <subject, predicate, object>relationships in images for vision understanding. Although recent works havemade steady progress on SGG, they still suffer long-tail distribution issuesthat tail-predicates are more costly to train and hard to distinguish due to asmall amount of annotated data compared to frequent predicates. Existingre-balancing strategies try to handle it via prior rules but are still confinedto pre-defined conditions, which are not scalable for various models anddatasets. In this paper, we propose a Cross-modal prediCate boosting (CaCao)framework, where a visually-prompted language model is learned to generatediverse fine-grained predicates in a low-resource way. The proposed CaCao canbe applied in a plug-and-play fashion and automatically strengthen existing SGGto tackle the long-tailed problem. Based on that, we further introduce a novelEntangled cross-modal prompt approach for open-world predicate scene graphgeneration (Epic), where models can generalize to unseen predicates in azero-shot manner. Comprehensive experiments on three benchmark datasets showthat CaCao consistently boosts the performance of multiple scene graphgeneration models in a model-agnostic way. Moreover, our Epic achievescompetitive performance on open-world predicate prediction. The data and codefor this paper are publicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.13233v2 |
| 571 | MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models                                                                | Jing Zhao                      | 2023-03-23     | cs.CV, cs.AI                            | The advent of open-source AI communities has produced a cornucopia ofpowerful text-guided diffusion models that are trained on various datasets.While few explorations have been conducted on ensembling such models to combinetheir strengths. In this work, we propose a simple yet effective method calledSaliency-aware Noise Blending (SNB) that can empower the fused text-guideddiffusion models to achieve more controllable generation. Specifically, weexperimentally find that the responses of classifier-free guidance are highlyrelated to the saliency of generated images. Thus we propose to trust differentmodels in their areas of expertise by blending the predicted noises of twodiffusion models in a saliency-aware manner. SNB is training-free and can becompleted within a DDIM sampling process. Additionally, it can automaticallyalign the semantics of two noise spaces without requiring additionalannotations such as masks. Extensive experiments show the impressiveeffectiveness of SNB in various applications. Project page is available athttps://magicfusion.github.io/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.13126v3 |
| 572 | From Knowledge Distillation to Self-Knowledge Distillation: A Unified Approach with Normalized Loss and Customized Soft Labels                       | Zhendong Yang                  | 2023-03-23     | cs.CV                                   | Knowledge Distillation (KD) uses the teacher's prediction logits as softlabels to guide the student, while self-KD does not need a real teacher torequire the soft labels. This work unifies the formulations of the two tasks bydecomposing and reorganizing the generic KD loss into a Normalized KD (NKD)loss and customized soft labels for both target class (image's category) andnon-target classes named Universal Self-Knowledge Distillation (USKD). Wedecompose the KD loss and find the non-target loss from it forces the student'snon-target logits to match the teacher's, but the sum of the two non-targetlogits is different, preventing them from being identical. NKD normalizes thenon-target logits to equalize their sum. It can be generally used for KD andself-KD to better use the soft labels for distillation loss. USKD generatescustomized soft labels for both target and non-target classes without ateacher. It smooths the target logit of the student as the soft target labeland uses the rank of the intermediate feature to generate the soft non-targetlabels with Zipf's law. For KD with teachers, our NKD achieves state-of-the-artperformance on CIFAR-100 and ImageNet datasets, boosting the ImageNet Top-1accuracy of ResNet18 from 69.90% to 71.96% with a ResNet-34 teacher. Forself-KD without teachers, USKD is the first self-KD method that can beeffectively applied to both CNN and ViT models with negligible additional timeand memory cost, resulting in new state-of-the-art results, such as 1.17% and0.55% accuracy gains on ImageNet for MobileNet and DeiT-Tiny, respectively. Ourcodes are available at https://github.com/yzd-v/cls_KD.                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2303.13005v2 |
| 573 | Tube-Link: A Flexible Cross Tube Framework for Universal Video Segmentation                                                                          | Xiangtai Li                    | 2023-03-22     | cs.CV                                   | Video segmentation aims to segment and track every pixel in diverse scenariosaccurately. In this paper, we present Tube-Link, a versatile framework thataddresses multiple core tasks of video segmentation with a unifiedarchitecture. Our framework is a near-online approach that takes a shortsubclip as input and outputs the corresponding spatial-temporal tube masks. Toenhance the modeling of cross-tube relationships, we propose an effective wayto perform tube-level linking via attention along the queries. In addition, weintroduce temporal contrastive learning to instance-wise discriminativefeatures for tube-level association. Our approach offers flexibility andefficiency for both short and long video inputs, as the length of each subclipcan be varied according to the needs of datasets or scenarios. Tube-Linkoutperforms existing specialized architectures by a significant margin on fivevideo segmentation datasets. Specifically, it achieves almost 13% relativeimprovements on VIPSeg and 4% improvements on KITTI-STEP over the strongbaseline Video K-Net. When using a ResNet50 backbone on Youtube-VIS-2019 and2021, Tube-Link boosts IDOL by 3% and 4%, respectively.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2303.12782v3 |
| 574 | NLOS-NeuS: Non-line-of-sight Neural Implicit Surface                                                                                                 | Yuki Fujimura                  | 2023-03-22     | cs.CV, eess.IV                          | Non-line-of-sight (NLOS) imaging is conducted to infer invisible scenes fromindirect light on visible objects. The neural transient field (NeTF) wasproposed for representing scenes as neural radiance fields in NLOS scenes. Wepropose NLOS neural implicit surface (NLOS-NeuS), which extends the NeTF toneural implicit surfaces with a signed distance function (SDF) forreconstructing three-dimensional surfaces in NLOS scenes. We introduce twoconstraints as loss functions for correctly learning an SDF to avoid non-zerolevel-set surfaces. We also introduce a lower bound constraint of an SDF basedon the geometry of the first-returning photons. The experimental resultsindicate that these constraints are essential for learning a correct SDF inNLOS scenes. Compared with previous methods with discretized representation,NLOS-NeuS with the neural continuous representation enables us to reconstructsmooth surfaces while preserving fine details in NLOS scenes. To the best ofour knowledge, this is the first study on neural implicit surfaces with volumerendering in NLOS scenes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2303.12280v2 |
| 575 | Exploring the Benefits of Visual Prompting in Differential Privacy                                                                                   | Yizhe Li                       | 2023-03-22     | cs.CV, cs.CR, cs.LG                     | Visual Prompting (VP) is an emerging and powerful technique that allowssample-efficient adaptation to downstream tasks by engineering a well-trainedfrozen source model. In this work, we explore the benefits of VP inconstructing compelling neural network classifiers with differential privacy(DP). We explore and integrate VP into canonical DP training methods anddemonstrate its simplicity and efficiency. In particular, we discover that VPin tandem with PATE, a state-of-the-art DP training method that leverages theknowledge transfer from an ensemble of teachers, achieves the state-of-the-artprivacy-utility trade-off with minimum expenditure of privacy budget. Moreover,we conduct additional experiments on cross-domain image classification with asufficient domain gap to further unveil the advantage of VP in DP. Lastly, wealso conduct extensive ablation studies to validate the effectiveness andcontribution of VP under DP consideration. Our code is available at(https://github.com/EzzzLi/Prompt-PATE).                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.12247v2 |
| 576 | VAD: Vectorized Scene Representation for Efficient Autonomous Driving                                                                                | Bo Jiang                       | 2023-03-21     | cs.RO, cs.CV                            | Autonomous driving requires a comprehensive understanding of the surroundingenvironment for reliable trajectory planning. Previous works rely on denserasterized scene representation (e.g., agent occupancy and semantic map) toperform planning, which is computationally intensive and misses theinstance-level structure information. In this paper, we propose VAD, anend-to-end vectorized paradigm for autonomous driving, which models the drivingscene as a fully vectorized representation. The proposed vectorized paradigmhas two significant advantages. On one hand, VAD exploits the vectorized agentmotion and map elements as explicit instance-level planning constraints whicheffectively improves planning safety. On the other hand, VAD runs much fasterthan previous end-to-end planning methods by getting rid ofcomputation-intensive rasterized representation and hand-designedpost-processing steps. VAD achieves state-of-the-art end-to-end planningperformance on the nuScenes dataset, outperforming the previous best method bya large margin. Our base model, VAD-Base, greatly reduces the average collisionrate by 29.0% and runs 2.5x faster. Besides, a lightweight variant, VAD-Tiny,greatly improves the inference speed (up to 9.3x) while achieving comparableplanning performance. We believe the excellent performance and the highefficiency of VAD are critical for the real-world deployment of an autonomousdriving system. Code and models are available at https://github.com/hustvl/VADfor facilitating future research.                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.12077v3 |
| 577 | TIFA: Accurate and Interpretable Text-to-Image Faithfulness Evaluation with Question Answering                                                       | Yushi Hu                       | 2023-03-21     | cs.CV                                   | Despite thousands of researchers, engineers, and artists actively working onimproving text-to-image generation models, systems often fail to produce imagesthat accurately align with the text inputs. We introduce TIFA (Text-to-ImageFaithfulness evaluation with question Answering), an automatic evaluationmetric that measures the faithfulness of a generated image to its text inputvia visual question answering (VQA). Specifically, given a text input, weautomatically generate several question-answer pairs using a language model. Wecalculate image faithfulness by checking whether existing VQA models can answerthese questions using the generated image. TIFA is a reference-free metric thatallows for fine-grained and interpretable evaluations of generated images. TIFAalso has better correlations with human judgments than existing metrics. Basedon this approach, we introduce TIFA v1.0, a benchmark consisting of 4K diversetext inputs and 25K questions across 12 categories (object, counting, etc.). Wepresent a comprehensive evaluation of existing text-to-image models using TIFAv1.0 and highlight the limitations and challenges of current models. Forinstance, we find that current text-to-image models, despite doing well oncolor and material, still struggle in counting, spatial relations, andcomposing multiple objects. We hope our benchmark will help carefully measurethe research progress in text-to-image synthesis and provide valuable insightsfor further research.                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2303.11897v3 |
| 578 | DiffuMask: Synthesizing Images with Pixel-level Annotations for Semantic Segmentation Using Diffusion Models                                         | Weijia Wu                      | 2023-03-21     | cs.CV                                   | Collecting and annotating images with pixel-wise labels is time-consuming andlaborious. In contrast, synthetic data can be freely available using agenerative model (e.g., DALL-E, Stable Diffusion). In this paper, we show thatit is possible to automatically obtain accurate semantic masks of syntheticimages generated by the Off-the-shelf Stable Diffusion model, which uses onlytext-image pairs during training. Our approach, called DiffuMask, exploits thepotential of the cross-attention map between text and image, which is naturaland seamless to extend the text-driven image synthesis to semantic maskgeneration. DiffuMask uses text-guided cross-attention information to localizeclass/word-specific regions, which are combined with practical techniques tocreate a novel high-resolution and class-discriminative pixel-wise mask. Themethods help to reduce data collection and annotation costs obviously.Experiments demonstrate that the existing segmentation methods trained onsynthetic data of DiffuMask can achieve a competitive performance over thecounterpart of real data (VOC 2012, Cityscapes). For some classes (e.g., bird),DiffuMask presents promising performance, close to the stateof-the-art resultof real data (within 3% mIoU gap). Moreover, in the open-vocabularysegmentation (zero-shot) setting, DiffuMask achieves a new SOTA result onUnseen class of VOC 2012. The project website can be found athttps://weijiawu.github.io/DiffusionMask/.                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2303.11681v2 |
| 579 | BoxSnake: Polygonal Instance Segmentation with Box Supervision                                                                                       | Rui Yang                       | 2023-03-21     | cs.CV, cs.AI                            | Box-supervised instance segmentation has gained much attention as it requiresonly simple box annotations instead of costly mask or polygon annotations.However, existing box-supervised instance segmentation models mainly focus onmask-based frameworks. We propose a new end-to-end training technique, termedBoxSnake, to achieve effective polygonal instance segmentation using only boxannotations for the first time. Our method consists of two loss functions: (1)a point-based unary loss that constrains the bounding box of predicted polygonsto achieve coarse-grained segmentation; and (2) a distance-aware pairwise lossthat encourages the predicted polygons to fit the object boundaries. Comparedwith the mask-based weakly-supervised methods, BoxSnake further reduces theperformance gap between the predicted segmentation and the bounding box, andshows significant superiority on the Cityscapes dataset. The code has beenavailable publicly.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2303.11630v3 |
| 580 | Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation                                                                           | Wenkang Shan                   | 2023-03-21     | cs.CV                                   | In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method withJoint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposedfor probabilistic 3D human pose estimation. On the one hand, D3DP generatesmultiple possible 3D pose hypotheses for a single 2D observation. It graduallydiffuses the ground truth 3D poses to a random distribution, and learns adenoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses.The proposed D3DP is compatible with existing 3D pose estimators and supportsusers to balance efficiency and accuracy during inference through twocustomizable parameters. On the other hand, JPMA is proposed to assemblemultiple hypotheses generated by D3DP into a single 3D pose for practical use.It reprojects 3D pose hypotheses to the 2D camera plane, selects the besthypothesis joint-by-joint based on the reprojection errors, and combines theselected joints into the final pose. The proposed JPMA conducts aggregation atthe joint level and makes use of the 2D prior information, both of which havebeen overlooked by previous approaches. Extensive experiments on Human3.6M andMPI-INF-3DHP datasets show that our method outperforms the state-of-the-artdeterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Codeis available at https://github.com/paTRICK-swk/D3DP.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.11579v2 |
| 581 | Texture Learning Domain Randomization for Domain Generalized Segmentation                                                                            | Sunghwan Kim                   | 2023-03-21     | cs.CV                                   | Deep Neural Networks (DNNs)-based semantic segmentation models trained on asource domain often struggle to generalize to unseen target domains, i.e., adomain gap problem. Texture often contributes to the domain gap, making DNNsvulnerable to domain shift because they are prone to be texture-biased.Existing Domain Generalized Semantic Segmentation (DGSS) methods havealleviated the domain gap problem by guiding models to prioritize shape overtexture. On the other hand, shape and texture are two prominent andcomplementary cues in semantic segmentation. This paper argues that leveragingtexture is crucial for improving performance in DGSS. Specifically, we proposea novel framework, coined Texture Learning Domain Randomization (TLDR). TLDRincludes two novel losses to effectively enhance texture learning in DGSS: (1)a texture regularization loss to prevent overfitting to source domain texturesby using texture features from an ImageNet pre-trained model and (2) a texturegeneralization loss that utilizes random style images to learn diverse texturerepresentations in a self-supervised manner. Extensive experimental resultsdemonstrate the superiority of the proposed TLDR; e.g., TLDR achieves 46.5 mIoUon GTA-to-Cityscapes using ResNet-50, which improves the prior state-of-the-artmethod by 1.9 mIoU. The source code is available athttps://github.com/ssssshwan/TLDR.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2303.11546v2 |
| 582 | eP-ALM: Efficient Perceptual Augmentation of Language Models                                                                                         | Mustafa Shukor                 | 2023-03-20     | cs.CV, cs.CL, cs.LG                     | Large Language Models (LLMs) have so far impressed the world, withunprecedented capabilities that emerge in models at large scales. On the visionside, transformer models (i.e., ViT) are following the same trend, achievingthe best performance on challenging benchmarks. With the abundance of suchunimodal models, a natural question arises; do we need also to follow thistrend to tackle multimodal tasks? In this work, we propose to rather directeffort to efficient adaptations of existing models, and propose to augmentLanguage Models with perception. Existing approaches for adapting pretrainedmodels for vision-language tasks still rely on several key components thathinder their efficiency. In particular, they still train a large number ofparameters, rely on large multimodal pretraining, use encoders (e.g., CLIP)trained on huge image-text datasets, and add significant inference overhead. Inaddition, most of these approaches have focused on Zero-Shot and In ContextLearning, with little to no effort on direct finetuning. We investigate theminimal computational effort needed to adapt unimodal models for multimodaltasks and propose a new challenging setup, alongside different approaches, thatefficiently adapts unimodal pretrained models. We show that by freezing morethan 99% of total parameters, training only one linear projection layer, andprepending only one trainable token, our approach (dubbed eP-ALM) significantlyoutperforms other baselines on VQA and Captioning across Image, Video, andAudio modalities, following the proposed setup. The code is available here:https://github.com/mshukor/eP-ALM.                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2303.11403v3 |
| 583 | Sound Localization from Motion: Jointly Learning Sound Direction and Camera Rotation                                                                 | Ziyang Chen                    | 2023-03-20     | cs.CV, cs.SD, eess.AS                   | The images and sounds that we perceive undergo subtle but geometricallyconsistent changes as we rotate our heads. In this paper, we use these cues tosolve a problem we call Sound Localization from Motion (SLfM): jointlyestimating camera rotation and localizing sound sources. We learn to solvethese tasks solely through self-supervision. A visual model predicts camerarotation from a pair of images, while an audio model predicts the direction ofsound sources from binaural sounds. We train these models to generatepredictions that agree with one another. At test time, the models can bedeployed independently. To obtain a feature representation that is well-suitedto solving this challenging problem, we also propose a method for learning anaudio-visual representation through cross-view binauralization: estimatingbinaural sound from one view, given images and sound from another. Our modelcan successfully estimate accurate rotations on both real and synthetic scenes,and localize sound sources with accuracy competitive with state-of-the-artself-supervised approaches. Project site: https://ificl.github.io/SLfM/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2303.11329v2 |
| 584 | Localizing Object-level Shape Variations with Text-to-Image Diffusion Models                                                                         | Or Patashnik                   | 2023-03-20     | cs.CV, cs.GR, cs.LG                     | Text-to-image models give rise to workflows which often begin with anexploration step, where users sift through a large collection of generatedimages. The global nature of the text-to-image generation process preventsusers from narrowing their exploration to a particular object in the image. Inthis paper, we present a technique to generate a collection of images thatdepicts variations in the shape of a specific object, enabling an object-levelshape exploration process. Creating plausible variations is challenging as itrequires control over the shape of the generated object while respecting itssemantics. A particular challenge when generating object variations isaccurately localizing the manipulation applied over the object's shape. Weintroduce a prompt-mixing technique that switches between prompts along thedenoising process to attain a variety of shape choices. To localize theimage-space operation, we present two techniques that use the self-attentionlayers in conjunction with the cross-attention layers. Moreover, we show thatthese localization techniques are general and effective beyond the scope ofgenerating object variations. Extensive results and comparisons demonstrate theeffectiveness of our method in generating object variations, and the competenceof our localization techniques.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.11306v2 |
| 585 | HiFace: High-Fidelity 3D Face Reconstruction by Learning Static and Dynamic Details                                                                  | Zenghao Chai                   | 2023-03-20     | cs.CV, cs.GR                            | 3D Morphable Models (3DMMs) demonstrate great potential for reconstructingfaithful and animatable 3D facial surfaces from a single image. The facialsurface is influenced by the coarse shape, as well as the static detail (e,g.,person-specific appearance) and dynamic detail (e.g., expression-drivenwrinkles). Previous work struggles to decouple the static and dynamic detailsthrough image-level supervision, leading to reconstructions that are notrealistic. In this paper, we aim at high-fidelity 3D face reconstruction andpropose HiFace to explicitly model the static and dynamic details.Specifically, the static detail is modeled as the linear combination of adisplacement basis, while the dynamic detail is modeled as the linearinterpolation of two displacement maps with polarized expressions. We exploitseveral loss functions to jointly learn the coarse shape and fine details withboth synthetic and real-world datasets, which enable HiFace to reconstructhigh-fidelity 3D shapes with animatable details. Extensive quantitative andqualitative experiments demonstrate that HiFace presents state-of-the-artreconstruction quality and faithfully recovers both the static and dynamicdetails. Our project page can be found at https://project-hiface.github.io.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2303.11225v2 |
| 586 | EmoTalk: Speech-Driven Emotional Disentanglement for 3D Face Animation                                                                               | Ziqiao Peng                    | 2023-03-20     | cs.CV, cs.SD, eess.AS                   | Speech-driven 3D face animation aims to generate realistic facial expressionsthat match the speech content and emotion. However, existing methods oftenneglect emotional facial expressions or fail to disentangle them from speechcontent. To address this issue, this paper proposes an end-to-end neuralnetwork to disentangle different emotions in speech so as to generate rich 3Dfacial expressions. Specifically, we introduce the emotion disentanglingencoder (EDE) to disentangle the emotion and content in the speech bycross-reconstructed speech signals with different emotion labels. Then anemotion-guided feature fusion decoder is employed to generate a 3D talking facewith enhanced emotion. The decoder is driven by the disentangled identity,emotional, and content embeddings so as to generate controllable personal andemotional styles. Finally, considering the scarcity of the 3D emotional talkingface data, we resort to the supervision of facial blendshapes, which enablesthe reconstruction of plausible 3D faces from 2D emotional data, and contributea large-scale 3D emotional talking face dataset (3D-ETF) to train the network.Our experiments and user studies demonstrate that our approach outperformsstate-of-the-art methods and exhibits more diverse facial movements. Werecommend watching the supplementary video:https://ziqiaopeng.github.io/emotalk                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2303.11089v2 |
| 587 | Pluralistic Aging Diffusion Autoencoder                                                                                                              | Peipei Li                      | 2023-03-20     | cs.CV, cs.AI                            | Face aging is an ill-posed problem because multiple plausible aging patternsmay correspond to a given input. Most existing methods often produce onedeterministic estimation. This paper proposes a novel CLIP-driven PluralisticAging Diffusion Autoencoder (PADA) to enhance the diversity of aging patterns.First, we employ diffusion models to generate diverse low-level aging detailsvia a sequential denoising reverse process. Second, we present ProbabilisticAging Embedding (PAE) to capture diverse high-level aging patterns, whichrepresents age information as probabilistic distributions in the common CLIPlatent space. A text-guided KL-divergence loss is designed to guide thislearning. Our method can achieve pluralistic face aging conditioned onopen-world aging texts and arbitrary unseen face images. Qualitative andquantitative experiments demonstrate that our method can generate more diverseand high-quality plausible aging results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2303.11086v2 |
| 588 | Learning Foresightful Dense Visual Affordance for Deformable Object Manipulation                                                                     | Ruihai Wu                      | 2023-03-20     | cs.CV, cs.RO                            | Understanding and manipulating deformable objects (e.g., ropes and fabrics)is an essential yet challenging task with broad applications. Difficulties comefrom complex states and dynamics, diverse configurations and high-dimensionalaction space of deformable objects. Besides, the manipulation tasks usuallyrequire multiple steps to accomplish, and greedy policies may easily lead tolocal optimal states. Existing studies usually tackle this problem usingreinforcement learning or imitating expert demonstrations, with limitations inmodeling complex states or requiring hand-crafted expert policies. In thispaper, we study deformable object manipulation using dense visual affordance,with generalization towards diverse states, and propose a novel kind offoresightful dense affordance, which avoids local optima by estimating states'values for long-term manipulation. We propose a framework for learning thisrepresentation, with novel designs such as multi-stage stable learning andefficient self-supervised data collection without experts. Experimentsdemonstrate the superiority of our proposed foresightful dense affordance.Project page: https://hyperplane-lab.github.io/DeformableAffordance                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2303.11057v3 |
| 589 | Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection                                           | Wenhang Ge                     | 2023-03-20     | cs.CV, cs.LG                            | Neural implicit surface learning has shown significant progress in multi-view3D reconstruction, where an object is represented by multilayer perceptronsthat provide continuous implicit surface representation and view-dependentradiance. However, current methods often fail to accurately reconstructreflective surfaces, leading to severe ambiguity. To overcome this issue, wepropose Ref-NeuS, which aims to reduce ambiguity by attenuating the effect ofreflective surfaces. Specifically, we utilize an anomaly detector to estimatean explicit reflection score with the guidance of multi-view context tolocalize reflective surfaces. Afterward, we design a reflection-awarephotometric loss that adaptively reduces ambiguity by modeling rendered coloras a Gaussian distribution, with the reflection score representing thevariance. We show that together with a reflection direction-dependent radiance,our model achieves high-quality surface reconstruction on reflective surfacesand outperforms the state-of-the-arts by a large margin. Besides, our model isalso comparable on general surfaces.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2303.10840v2 |
| 590 | Spatial-Aware Token for Weakly Supervised Object Localization                                                                                        | Pingyu Wu                      | 2023-03-18     | cs.CV                                   | Weakly supervised object localization (WSOL) is a challenging task aiming tolocalize objects with only image-level supervision. Recent works apply visualtransformer to WSOL and achieve significant success by exploiting thelong-range feature dependency in self-attention mechanism. However, existingtransformer-based methods synthesize the classification feature maps as thelocalization map, which leads to optimization conflicts between classificationand localization tasks. To address this problem, we propose to learn atask-specific spatial-aware token (SAT) to condition localization in a weaklysupervised manner. Specifically, a spatial token is first introduced in theinput space to aggregate representations for localization task. Then a spatialaware attention module is constructed, which allows spatial token to generateforeground probabilities of different patches by querying and to extractlocalization knowledge from the classification task. Besides, for the problemof sparse and unbalanced pixel-level supervision obtained from the image-levellabel, two spatial constraints, including batch area loss and normalizationloss, are designed to compensate and enhance this supervision. Experiments showthat the proposed SAT achieves state-of-the-art performance on both CUB-200 andImageNet, with 98.45% and 73.13% GT-known Loc, respectively. Even under theextreme setting of using only 1 image per class from ImageNet for training, SATalready exceeds the SOTA method by 2.1% GT-known Loc. Code and models areavailable at https://github.com/wpy1999/SAT.                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2303.10438v2 |
| 591 | A Unified Continual Learning Framework with General Parameter-Efficient Tuning                                                                       | Qiankun Gao                    | 2023-03-17     | cs.CV                                   | The "pre-training $\rightarrow$ downstream adaptation" presents both newopportunities and challenges for Continual Learning (CL). Although the recentstate-of-the-art in CL is achieved through Parameter-Efficient-Tuning (PET)adaptation paradigm, only prompt has been explored, limiting its application toTransformers only. In this paper, we position prompting as one instantiation ofPET, and propose a unified CL framework with general PET, dubbed asLearning-Accumulation-Ensemble (LAE). PET, e.g., using Adapter, LoRA, orPrefix, can adapt a pre-trained model to downstream tasks with fewer parametersand resources. Given a PET method, our LAE framework incorporates it for CLwith three novel designs. 1) Learning: the pre-trained model adapts to the newtask by tuning an online PET module, along with our adaptation speedcalibration to align different PET modules, 2) Accumulation: the task-specificknowledge learned by the online PET module is accumulated into an offline PETmodule through momentum update, 3) Ensemble: During inference, we respectivelyconstruct two experts with online/offline PET modules (which are favored by thenovel/historical tasks) for prediction ensemble. We show that LAE is compatiblewith a battery of PET methods and gains strong CL capability. For example, LAEwith Adaptor PET surpasses the prior state-of-the-art by 1.3% and 3.6% inlast-incremental accuracy on CIFAR100 and ImageNet-R datasets, respectively.Code is available at \url{https://github.com/gqk/LAE}.                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2303.10070v2 |
| 592 | No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier                                        | Zexi Li                        | 2023-03-17     | cs.LG, cs.CV                            | Data heterogeneity is an inherent challenge that hinders the performance offederated learning (FL). Recent studies have identified the biased classifiersof local models as the key bottleneck. Previous attempts have used classifiercalibration after FL training, but this approach falls short in improving thepoor feature representations caused by training-time classifier biases.Resolving the classifier bias dilemma in FL requires a full understanding ofthe mechanisms behind the classifier. Recent advances in neural collapse haveshown that the classifiers and feature prototypes under perfect trainingscenarios collapse into an optimal structure called simplex equiangular tightframe (ETF). Building on this neural collapse insight, we propose a solution tothe FL's classifier bias problem by utilizing a synthetic and fixed ETFclassifier during training. The optimal classifier structure enables allclients to learn unified and optimal feature representations even underextremely heterogeneous data. We devise several effective modules to betteradapt the ETF structure in FL, achieving both high generalization andpersonalization. Extensive experiments demonstrate that our method achievesstate-of-the-art performances on CIFAR-10, CIFAR-100, and Tiny-ImageNet.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.10058v2 |
| 593 | Leaping Into Memories: Space-Time Deep Feature Synthesis                                                                                             | Alexandros Stergiou            | 2023-03-17     | cs.CV                                   | The success of deep learning models has led to their adaptation and adoptionby prominent video understanding methods. The majority of these approachesencode features in a joint space-time modality for which the inner workings andlearned representations are difficult to visually interpret. We propose LEArnedPreconscious Synthesis (LEAPS), an architecture-independent method forsynthesizing videos from the internal spatiotemporal representations of models.Using a stimulus video and a target class, we prime a fixed space-time modeland iteratively optimize a video initialized with random noise. Additionalregularizers are used to improve the feature diversity of the synthesizedvideos alongside the cross-frame temporal coherence of motions. Wequantitatively and qualitatively evaluate the applicability of LEAPS byinverting a range of spatiotemporal convolutional and attention-basedarchitectures trained on Kinetics-400, which to the best of our knowledge hasnot been previously accomplished.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2303.09941v4 |
| 594 | DiffusionRet: Generative Text-Video Retrieval with Diffusion Model                                                                                   | Peng Jin                       | 2023-03-17     | cs.CV                                   | Existing text-video retrieval solutions are, in essence, discriminant modelsfocused on maximizing the conditional likelihood, i.e., p(candidates|query).While straightforward, this de facto paradigm overlooks the underlying datadistribution p(query), which makes it challenging to identifyout-of-distribution data. To address this limitation, we creatively tackle thistask from a generative viewpoint and model the correlation between the text andthe video as their joint probability p(candidates,query). This is accomplishedthrough a diffusion-based text-video retrieval framework (DiffusionRet), whichmodels the retrieval task as a process of gradually generating jointdistribution from noise. During training, DiffusionRet is optimized from boththe generation and discrimination perspectives, with the generator beingoptimized by generation loss and the feature extractor trained with contrastiveloss. In this way, DiffusionRet cleverly leverages the strengths of bothgenerative and discriminative methods. Extensive experiments on five commonlyused text-video retrieval benchmarks, including MSRVTT, LSMDC, MSVD,ActivityNet Captions, and DiDeMo, with superior performances, justify theefficacy of our method. More encouragingly, without any modification,DiffusionRet even performs well in out-domain retrieval settings. We believethis work brings fundamental insights into the related fields. Code isavailable at https://github.com/jpthu17/DiffusionRet.                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.09867v2 |
| 595 | Denoising Diffusion Autoencoders are Unified Self-supervised Learners                                                                                | Weilai Xiang                   | 2023-03-17     | cs.CV, cs.LG                            | Inspired by recent advances in diffusion models, which are reminiscent ofdenoising autoencoders, we investigate whether they can acquire discriminativerepresentations for classification via generative pre-training. This papershows that the networks in diffusion models, namely denoising diffusionautoencoders (DDAE), are unified self-supervised learners: by pre-training onunconditional image generation, DDAE has already learned stronglylinear-separable representations within its intermediate layers withoutauxiliary encoders, thus making diffusion pre-training emerge as a generalapproach for generative-and-discriminative dual learning. To validate this, weconduct linear probe and fine-tuning evaluations. Our diffusion-based approachachieves 95.9% and 50.0% linear evaluation accuracies on CIFAR-10 andTiny-ImageNet, respectively, and is comparable to contrastive learning andmasked autoencoders for the first time. Transfer learning from ImageNet alsoconfirms the suitability of DDAE for Vision Transformers, suggesting thepotential to scale DDAEs as unified foundation models. Code is available atgithub.com/FutureXiang/ddae.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2303.09769v2 |
| 596 | CHAMPAGNE: Learning Real-world Conversation from Large-Scale Web Videos                                                                              | Seungju Han                    | 2023-03-17     | cs.CL, cs.AI                            | Visual information is central to conversation: body gestures and physicalbehaviour, for example, contribute to meaning that transcends words alone. Todate, however, most neural conversational models are limited to just text. Weintroduce CHAMPAGNE, a generative model of conversations that can account forvisual contexts. To train CHAMPAGNE, we collect and release YTD-18M, alarge-scale corpus of 18M video-based dialogues. YTD-18M is constructed fromweb videos: crucial to our data collection pipeline is a pretrained languagemodel that converts error-prone automatic transcripts to a cleaner dialogueformat while maintaining meaning. Human evaluation reveals that YTD-18M is moresensible and specific than prior resources (MMDialog, 1M dialogues), whilemaintaining visual-groundedness. Experiments demonstrate that 1) CHAMPAGNElearns to conduct conversation from YTD-18M; and 2) when fine-tuned, itachieves state-of-the-art results on four vision-language tasks focused onreal-world conversations. We release data, models, and code.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2303.09713v2 |
| 597 | Unsupervised Self-Driving Attention Prediction via Uncertainty Mining and Knowledge Embedding                                                        | Pengfei Zhu                    | 2023-03-17     | cs.CV                                   | Predicting attention regions of interest is an important yet challenging taskfor self-driving systems. Existing methodologies rely on large-scale labeledtraffic datasets that are labor-intensive to obtain. Besides, the huge domaingap between natural scenes and traffic scenes in current datasets also limitsthe potential for model training. To address these challenges, we are the firstto introduce an unsupervised way to predict self-driving attention byuncertainty modeling and driving knowledge integration. Our approach'sUncertainty Mining Branch (UMB) discovers commonalities and differences frommultiple generated pseudo-labels achieved from models pre-trained on naturalscenes by actively measuring the uncertainty. Meanwhile, our KnowledgeEmbedding Block (KEB) bridges the domain gap by incorporating driving knowledgeto adaptively refine the generated pseudo-labels. Quantitative and qualitativeresults with equivalent or even more impressive performance compared tofully-supervised state-of-the-art approaches across all three public datasetsdemonstrate the effectiveness of the proposed method and the potential of thisdirection. The code will be made publicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2303.09706v3 |
| 598 | Efficient Computation Sharing for Multi-Task Visual Scene Understanding                                                                              | Sara Shoouri                   | 2023-03-16     | cs.CV, eess.IV                          | Solving multiple visual tasks using individual models can beresource-intensive, while multi-task learning can conserve resources by sharingknowledge across different tasks. Despite the benefits of multi-task learning,such techniques can struggle with balancing the loss for each task, leading topotential performance degradation. We present a novel computation- andparameter-sharing framework that balances efficiency and accuracy to performmultiple visual tasks utilizing individually-trained single-task transformers.Our method is motivated by transfer learning schemes to reduce computationaland parameter storage costs while maintaining the desired performance. Ourapproach involves splitting the tasks into a base task and the other sub-tasks,and sharing a significant portion of activations and parameters/weights betweenthe base and sub-tasks to decrease inter-task redundancies and enhanceknowledge sharing. The evaluation conducted on NYUD-v2 and PASCAL-contextdatasets shows that our method is superior to the state-of-the-arttransformer-based multi-task learning techniques with higher accuracy andreduced computational resources. Moreover, our method is extended to videostream inputs, further reducing computational costs by efficiently sharinginformation across the temporal domain as well as the task domain. Our codesand models will be publicly available.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.09663v2 |
| 599 | Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution                                                                               | Jiamian Wang                   | 2023-03-16     | cs.CV                                   | Image super-resolution (SR) has witnessed extensive neural network designsfrom CNN to transformer architectures. However, prevailing SR models sufferfrom prohibitive memory footprint and intensive computations, which limitsfurther deployment on edge devices. This work investigates the potential ofnetwork pruning for super-resolution to take advantage of off-the-shelf networkdesigns and reduce the underlying computational overhead. Two main challengesremain in applying pruning methods for SR. First, the widely-used filterpruning technique reflects limited granularity and restricted adaptability todiverse network structures. Second, existing pruning methods generally operateupon a pre-trained network for the sparse structure determination, hard to getrid of dense model training in the traditional SR paradigm. To address thesechallenges, we adopt unstructured pruning with sparse models directly trainedfrom scratch. Specifically, we propose a novel Iterative SoftShrinkage-Percentage (ISS-P) method by optimizing the sparse structure of arandomly initialized network at each iteration and tweaking unimportant weightswith a small amount proportional to the magnitude scale on-the-fly. We observethat the proposed ISS-P can dynamically learn sparse structures adapting to theoptimization process and preserve the sparse model's trainability by yielding amore regularized gradient throughput. Experiments on benchmark datasetsdemonstrate the effectiveness of the proposed ISS-P over diverse networkarchitectures. Code is available athttps://github.com/Jiamian-Wang/Iterative-Soft-Shrinkage-SR                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.09650v2 |
| 600 | SurroundOcc: Multi-Camera 3D Occupancy Prediction for Autonomous Driving                                                                             | Yi Wei                         | 2023-03-16     | cs.CV                                   | 3D scene understanding plays a vital role in vision-based autonomous driving.While most existing methods focus on 3D object detection, they have difficultydescribing real-world objects of arbitrary shapes and infinite classes. Towardsa more comprehensive perception of a 3D scene, in this paper, we propose aSurroundOcc method to predict the 3D occupancy with multi-camera images. Wefirst extract multi-scale features for each image and adopt spatial 2D-3Dattention to lift them to the 3D volume space. Then we apply 3D convolutions toprogressively upsample the volume features and impose supervision on multiplelevels. To obtain dense occupancy prediction, we design a pipeline to generatedense occupancy ground truth without expansive occupancy annotations.Specifically, we fuse multi-frame LiDAR scans of dynamic objects and staticscenes separately. Then we adopt Poisson Reconstruction to fill the holes andvoxelize the mesh to get dense occupancy labels. Extensive experiments onnuScenes and SemanticKITTI datasets demonstrate the superiority of our method.Code and dataset are available at https://github.com/weiyithu/SurroundOcc                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2303.09551v2 |
| 601 | Among Us: Adversarially Robust Collaborative Perception by Consensus                                                                                 | Yiming Li                      | 2023-03-16     | cs.RO, cs.AI, cs.CV, cs.MA              | Multiple robots could perceive a scene (e.g., detect objects) collaborativelybetter than individuals, although easily suffer from adversarial attacks whenusing deep learning. This could be addressed by the adversarial defense, butits training requires the often-unknown attacking mechanism. Differently, wepropose ROBOSAC, a novel sampling-based defense strategy generalizable tounseen attackers. Our key idea is that collaborative perception should lead toconsensus rather than dissensus in results compared to individual perception.This leads to our hypothesize-and-verify framework: perception results with andwithout collaboration from a random subset of teammates are compared untilreaching a consensus. In such a framework, more teammates in the sampled subsetoften entail better perception performance but require longer sampling time toreject potential attackers. Thus, we derive how many sampling trials are neededto ensure the desired size of an attacker-free subset, or equivalently, themaximum size of such a subset that we can successfully sample within a givennumber of trials. We validate our method on the task of collaborative 3D objectdetection in autonomous driving scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.09495v3 |
| 602 | Robust Evaluation of Diffusion-Based Adversarial Purification                                                                                        | Minjong Lee                    | 2023-03-16     | cs.CV, cs.CR, cs.LG                     | We question the current evaluation practice on diffusion-based purificationmethods. Diffusion-based purification methods aim to remove adversarial effectsfrom an input data point at test time. The approach gains increasing attentionas an alternative to adversarial training due to the disentangling betweentraining and testing. Well-known white-box attacks are often employed tomeasure the robustness of the purification. However, it is unknown whetherthese attacks are the most effective for the diffusion-based purification sincethe attacks are often tailored for adversarial training. We analyze the currentpractices and provide a new guideline for measuring the robustness ofpurification methods against adversarial attacks. Based on our analysis, wefurther propose a new purification strategy improving robustness compared tothe current diffusion-based purification methods.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2303.09051v2 |
| 603 | Hybrid Spectral Denoising Transformer with Guided Attention                                                                                          | Zeqiang Lai                    | 2023-03-16     | cs.CV, eess.IV                          | In this paper, we present a Hybrid Spectral Denoising Transformer (HSDT) forhyperspectral image denoising. Challenges in adapting transformer for HSI arisefrom the capabilities to tackle existing limitations of CNN-based methods incapturing the global and local spatial-spectral correlations while maintainingefficiency and flexibility. To address these issues, we introduce a hybridapproach that combines the advantages of both models with a Spatial-SpectralSeparable Convolution (S3Conv), Guided Spectral Self-Attention (GSSA), andSelf-Modulated Feed-Forward Network (SM-FFN). Our S3Conv works as a lightweightalternative to 3D convolution, which extracts more spatial-spectral correlatedfeatures while keeping the flexibility to tackle HSIs with an arbitrary numberof bands. These features are then adaptively processed by GSSA which per-forms3D self-attention across the spectral bands, guided by a set of learnablequeries that encode the spectral signatures. This not only enriches our modelwith powerful capabilities for identifying global spectral correlations butalso maintains linear complexity. Moreover, our SM-FFN proposes theself-modulation that intensifies the activations of more informative regions,which further strengthens the aggregated features. Extensive experiments areconducted on various datasets under both simulated and real-world noise, and itshows that our HSDT significantly outperforms the existing state-of-the-artmethods while maintaining low computational overhead. Code is at https://github.com/Zeqiang-Lai/HSDT.                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2303.09040v2 |
| 604 | Mimic3D: Thriving 3D-Aware GANs via 3D-to-2D Imitation                                                                                               | Xingyu Chen                    | 2023-03-16     | cs.CV, cs.GR                            | Generating images with both photorealism and multiview 3D consistency iscrucial for 3D-aware GANs, yet existing methods struggle to achieve themsimultaneously. Improving the photorealism via CNN-based 2D super-resolutioncan break the strict 3D consistency, while keeping the 3D consistency bylearning high-resolution 3D representations for direct rendering oftencompromises image quality. In this paper, we propose a novel learning strategy,namely 3D-to-2D imitation, which enables a 3D-aware GAN to generatehigh-quality images while maintaining their strict 3D consistency, by lettingthe images synthesized by the generator's 3D rendering branch to mimic thosegenerated by its 2D super-resolution branch. We also introduce 3D-awareconvolutions into the generator for better 3D representation learning, whichfurther improves the image generation quality. With the above strategies, ourmethod reaches FID scores of 5.4 and 4.3 on FFHQ and AFHQ-v2 Cats,respectively, at 512x512 resolution, largely outperforming existing 3D-awareGANs using direct 3D rendering and coming very close to the previousstate-of-the-art method that leverages 2D super-resolution. Project website:https://seanchenxy.github.io/Mimic3DWeb.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.09036v2 |
| 605 | Unified Visual Relationship Detection with Vision and Language Models                                                                                | Long Zhao                      | 2023-03-16     | cs.CV                                   | This work focuses on training a single visual relationship detectorpredicting over the union of label spaces from multiple datasets. Merginglabels spanning different datasets could be challenging due to inconsistenttaxonomies. The issue is exacerbated in visual relationship detection whensecond-order visual semantics are introduced between pairs of objects. Toaddress this challenge, we propose UniVRD, a novel bottom-up method for UnifiedVisual Relationship Detection by leveraging vision and language models (VLMs).VLMs provide well-aligned image and text embeddings, where similarrelationships are optimized to be close to each other for semantic unification.Our bottom-up design enables the model to enjoy the benefit of training withboth object detection and visual relationship datasets. Empirical results onboth human-object interaction detection and scene-graph generation demonstratethe competitive performance of our model. UniVRD achieves 38.07 mAP onHICO-DET, outperforming the current best bottom-up HOI detector by 14.26 mAP.More importantly, we show that our unified detector performs as well asdataset-specific models in mAP, and achieves further improvements when we scaleup the model. Our code will be made publicly available on GitHub.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2303.08998v2 |
| 606 | Reinforce Data, Multiply Impact: Improved Model Accuracy and Robustness with Dataset Reinforcement                                                   | Fartash Faghri                 | 2023-03-15     | cs.CV, cs.AI, cs.LG                     | We propose Dataset Reinforcement, a strategy to improve a dataset once suchthat the accuracy of any model architecture trained on the reinforced datasetis improved at no additional training cost for users. We propose a DatasetReinforcement strategy based on data augmentation and knowledge distillation.Our generic strategy is designed based on extensive analysis across CNN- andtransformer-based models and performing large-scale study of distillation withstate-of-the-art models with various data augmentations. We create a reinforcedversion of the ImageNet training dataset, called ImageNet+, as well asreinforced datasets CIFAR-100+, Flowers-102+, and Food-101+. Models trainedwith ImageNet+ are more accurate, robust, and calibrated, and transfer well todownstream tasks (e.g., segmentation and detection). As an example, theaccuracy of ResNet-50 improves by 1.7% on the ImageNet validation set, 3.5% onImageNetV2, and 10.0% on ImageNet-R. Expected Calibration Error (ECE) on theImageNet validation set is also reduced by 9.9%. Using this backbone withMask-RCNN for object detection on MS-COCO, the mean average precision improvesby 0.8%. We reach similar gains for MobileNets, ViTs, and Swin-Transformers.For MobileNetV3 and Swin-Tiny, we observe significant improvements onImageNet-R/A/C of up to 20% improved robustness. Models pretrained on ImageNet+and fine-tuned on CIFAR-100+, Flowers-102+, and Food-101+, reach up to 3.4%improved accuracy. The code, datasets, and pretrained models are available athttps://github.com/apple/ml-dr.                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2303.08983v2 |
| 607 | Spherical Space Feature Decomposition for Guided Depth Map Super-Resolution                                                                          | Zixiang Zhao                   | 2023-03-15     | cs.CV                                   | Guided depth map super-resolution (GDSR), as a hot topic in multi-modal imageprocessing, aims to upsample low-resolution (LR) depth maps with additionalinformation involved in high-resolution (HR) RGB images from the same scene.The critical step of this task is to effectively extract domain-shared anddomain-private RGB/depth features. In addition, three detailed issues, namelyblurry edges, noisy surfaces, and over-transferred RGB texture, need to beaddressed. In this paper, we propose the Spherical Space feature DecompositionNetwork (SSDNet) to solve the above issues. To better model cross-modalityfeatures, Restormer block-based RGB/depth encoders are employed for extractinglocal-global features. Then, the extracted features are mapped to the sphericalspace to complete the separation of private features and the alignment ofshared features. Shared features of RGB are fused with the depth features tocomplete the GDSR task. Subsequently, a spherical contrast refinement (SCR)module is proposed to further address the detail issues. Patches that areclassified according to imperfect categories are input into the SCR module,where the patch features are pulled closer to the ground truth and pushed awayfrom the corresponding imperfect samples in the spherical feature space viacontrastive learning. Extensive experiments demonstrate that our method canachieve state-of-the-art results on four test datasets, as well as successfullygeneralize to real-world scenes. The code is available at\url{https://github.com/Zhaozixiang1228/GDSR-SSDNet}.                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2303.08942v2 |
| 608 | MAtch, eXpand and Improve: Unsupervised Finetuning for Zero-Shot Action Recognition with Language Knowledge                                          | Wei Lin                        | 2023-03-15     | cs.CV                                   | Large scale Vision-Language (VL) models have shown tremendous success inaligning representations between visual and text modalities. This enablesremarkable progress in zero-shot recognition, image generation & editing, andmany other exciting tasks. However, VL models tend to over-represent objectswhile paying much less attention to verbs, and require additional tuning onvideo data for best zero-shot action recognition performance. While previouswork relied on large-scale, fully-annotated data, in this work we propose anunsupervised approach. We adapt a VL model for zero-shot and few-shot actionrecognition using a collection of unlabeled videos and an unpaired actiondictionary. Based on that, we leverage Large Language Models and VL models tobuild a text bag for each unlabeled video via matching, text expansion andcaptioning. We use those bags in a Multiple Instance Learning setup to adapt animage-text backbone to video data. Although finetuned on unlabeled video data,our resulting models demonstrate high transferability to numerous unseenzero-shot downstream tasks, improving the base VL model performance by up to14\%, and even comparing favorably to fully-supervised baselines in bothzero-shot and few-shot video recognition transfer. The code will be releasedlater at \url{https://github.com/wlin-at/MAXI}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2303.08914v2 |
| 609 | Stochastic Segmentation with Conditional Categorical Diffusion Models                                                                                | Lukas Zbinden                  | 2023-03-15     | cs.CV                                   | Semantic segmentation has made significant progress in recent years thanks todeep neural networks, but the common objective of generating a singlesegmentation output that accurately matches the image's content may not besuitable for safety-critical domains such as medical diagnostics and autonomousdriving. Instead, multiple possible correct segmentation maps may be requiredto reflect the true distribution of annotation maps. In this context,stochastic semantic segmentation methods must learn to predict conditionaldistributions of labels given the image, but this is challenging due to thetypically multimodal distributions, high-dimensional output spaces, and limitedannotation data. To address these challenges, we propose a conditionalcategorical diffusion model (CCDM) for semantic segmentation based on DenoisingDiffusion Probabilistic Models. Our model is conditioned to the input image,enabling it to generate multiple segmentation label maps that account for thealeatoric uncertainty arising from divergent ground truth annotations. Ourexperimental results show that CCDM achieves state-of-the-art performance onLIDC, a stochastic semantic segmentation dataset, and outperforms establishedbaselines on the classical segmentation dataset Cityscapes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2303.08888v4 |
| 610 | RSFNet: A White-Box Image Retouching Approach using Region-Specific Color Filters                                                                    | Wenqi Ouyang                   | 2023-03-15     | cs.CV                                   | Retouching images is an essential aspect of enhancing the visual appeal ofphotos. Although users often share common aesthetic preferences, theirretouching methods may vary based on their individual preferences. Therefore,there is a need for white-box approaches that produce satisfying results andenable users to conveniently edit their images simultaneously. Recent white-boxretouching methods rely on cascaded global filters that provide image-levelfilter arguments but cannot perform fine-grained retouching. In contrast,colorists typically employ a divide-and-conquer approach, performing a seriesof region-specific fine-grained enhancements when using traditional tools likeDavinci Resolve. We draw on this insight to develop a white-box framework forphoto retouching using parallel region-specific filters, called RSFNet. Ourmodel generates filter arguments (e.g., saturation, contrast, hue) andattention maps of regions for each filter simultaneously. Instead of cascadingfilters, RSFNet employs linear summations of filters, allowing for a morediverse range of filter classes that can be trained more easily. Ourexperiments demonstrate that RSFNet achieves state-of-the-art results, offeringsatisfying aesthetic appeal and increased user convenience for editablewhite-box retouching.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.08682v2 |
| 611 | Manipulate by Seeing: Creating Manipulation Controllers from Pre-Trained Representations                                                             | Jianren Wang                   | 2023-03-14     | cs.RO                                   | The field of visual representation learning has seen explosive growth in thepast years, but its benefits in robotics have been surprisingly limited so far.Prior work uses generic visual representations as a basis to learn(task-specific) robot action policies (e.g., via behavior cloning). While thevisual representations do accelerate learning, they are primarily used toencode visual observations. Thus, action information has to be derived purelyfrom robot data, which is expensive to collect! In this work, we present ascalable alternative where the visual representations can help directly inferrobot actions. We observe that vision encoders express relationships betweenimage observations as distances (e.g., via embedding dot product) that could beused to efficiently plan robot behavior. We operationalize this insight anddevelop a simple algorithm for acquiring a distance function and dynamicspredictor, by fine-tuning a pre-trained representation on human collected videosequences. The final method is able to substantially outperform traditionalrobot learning baselines (e.g., 70% success v.s. 50% for behavior cloning onpick-place) on a suite of diverse real-world manipulation tasks. It can alsogeneralize to novel objects, without using any robot demonstrations duringtrain time. For visualizations of the learned policies please check:https://agi-labs.github.io/manipulate-by-seeing/.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2303.08135v4 |
| 612 | Window-Based Early-Exit Cascades for Uncertainty Estimation: When Deep Ensembles are More Efficient than Single Models                               | Guoxuan Xia                    | 2023-03-14     | cs.LG, cs.AI, cs.CV                     | Deep Ensembles are a simple, reliable, and effective method of improving boththe predictive performance and uncertainty estimates of deep learningapproaches. However, they are widely criticised as being computationallyexpensive, due to the need to deploy multiple independent models. Recent workhas challenged this view, showing that for predictive accuracy, ensembles canbe more computationally efficient (at inference) than scaling single modelswithin an architecture family. This is achieved by cascading ensemble membersvia an early-exit approach. In this work, we investigate extending theseefficiency gains to tasks related to uncertainty estimation. As many suchtasks, e.g. selective classification, are binary classification, our key novelinsight is to only pass samples within a window close to the binary decisionboundary to later cascade stages. Experiments on ImageNet-scale data across anumber of network architectures and uncertainty tasks show that the proposedwindow-based early-exit approach is able to achieve a superioruncertainty-computation trade-off compared to scaling single models. Forexample, a cascaded EfficientNet-B2 ensemble is able to achieve similarcoverage at 5% risk as a single EfficientNet-B4 with <30% the number of MACs.We also find that cascades/ensembles give more reliable improvements on OODdata vs scaling models up. Code for this work is available at:https://github.com/Guoxoug/window-early-exit.                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.08010v2 |
| 613 | ICICLE: Interpretable Class Incremental Continual Learning                                                                                           | Dawid Rymarczyk                | 2023-03-14     | cs.LG, cs.AI, cs.CV                     | Continual learning enables incremental learning of new tasks withoutforgetting those previously learned, resulting in positive knowledge transferthat can enhance performance on both new and old tasks. However, continuallearning poses new challenges for interpretability, as the rationale behindmodel predictions may change over time, leading to interpretability conceptdrift. We address this problem by proposing Interpretable Class-InCrementalLEarning (ICICLE), an exemplar-free approach that adopts a prototypicalpart-based approach. It consists of three crucial novelties: interpretabilityregularization that distills previously learned concepts while preservinguser-friendly positive reasoning; proximity-based prototype initializationstrategy dedicated to the fine-grained setting; and task-recency biascompensation devoted to prototypical parts. Our experimental resultsdemonstrate that ICICLE reduces the interpretability concept drift andoutperforms the existing exemplar-free methods of common class-incrementallearning when applied to concept-based models.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2303.07811v2 |
| 614 | WDiscOOD: Out-of-Distribution Detection via Whitened Linear Discriminant Analysis                                                                    | Yiye Chen                      | 2023-03-14     | cs.CV, cs.AI, cs.LG                     | Deep neural networks are susceptible to generating overconfident yeterroneous predictions when presented with data beyond known concepts. Thischallenge underscores the importance of detecting out-of-distribution (OOD)samples in the open world. In this work, we propose a novel feature-space OODdetection score based on class-specific and class-agnostic information.Specifically, the approach utilizes Whitened Linear Discriminant Analysis toproject features into two subspaces - the discriminative and residual subspaces- for which the in-distribution (ID) classes are maximally separated andclosely clustered, respectively. The OOD score is then determined by combiningthe deviation from the input data to the ID pattern in both subspaces. Theefficacy of our method, named WDiscOOD, is verified on the large-scaleImageNet-1k benchmark, with six OOD datasets that cover a variety ofdistribution shifts. WDiscOOD demonstrates superior performance on deepclassifiers with diverse backbone architectures, including CNN and visiontransformer. Furthermore, we also show that WDiscOOD more effectively detectsnovel concepts in representation spaces trained with contrastive objectives,including supervised contrastive loss and multi-modality contrastive loss.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2303.07543v4 |
| 615 | Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images                                                 | Nitzan Bitton-Guetta           | 2023-03-13     | cs.CV, cs.AI, cs.CL                     | Weird, unusual, and uncanny images pique the curiosity of observers becausethey challenge commonsense. For example, an image released during the 2022world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldoplaying chess, which playfully violates our expectation that their competitionshould occur on the football field. Humans can easily recognize and interpretthese unconventional images, but can AI models do the same? We introduceWHOOPS!, a new dataset and benchmark for visual commonsense. The dataset iscomprised of purposefully commonsense-defying images created by designers usingpublicly-available image generation tools like Midjourney. We consider severaltasks posed over the dataset. In addition to image captioning, cross-modalmatching, and visual question answering, we introduce a difficult explanationgeneration task, where models must identify and explain why a given image isunusual. Our results show that state-of-the-art models such as GPT3 and BLIP2still lag behind human performance on WHOOPS!. We hope our dataset will inspirethe development of AI models with stronger visual commonsense reasoningabilities. Data, models and code are available at the project website:whoops-benchmark.github.io                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2303.07274v4 |
| 616 | Pretrained ViTs Yield Versatile Representations For Medical Images                                                                                   | Christos Matsoukas             | 2023-03-13     | cs.CV                                   | Convolutional Neural Networks (CNNs) have reigned for a decade as the defacto approach to automated medical image diagnosis, pushing thestate-of-the-art in classification, detection and segmentation tasks. Over thelast years, vision transformers (ViTs) have appeared as a competitivealternative to CNNs, yielding impressive levels of performance in the naturalimage domain, while possessing several interesting properties that could provebeneficial for medical imaging tasks. In this work, we explore the benefits anddrawbacks of transformer-based models for medical image classification. Weconduct a series of experiments on several standard 2D medical image benchmarkdatasets and tasks. Our findings show that, while CNNs perform better iftrained from scratch, off-the-shelf vision transformers can perform on par withCNNs when pretrained on ImageNet, both in a supervised and self-supervisedsetting, rendering them as a viable alternative to CNNs.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2303.07034v2 |
| 617 | TARGET: Federated Class-Continual Learning via Exemplar-Free Distillation                                                                            | Jie Zhang                      | 2023-03-13     | cs.LG, cs.AI                            | This paper focuses on an under-explored yet important problem: FederatedClass-Continual Learning (FCCL), where new classes are dynamically added infederated learning. Existing FCCL works suffer from various limitations, suchas requiring additional datasets or storing the private data from previoustasks. In response, we first demonstrate that non-IID data exacerbatescatastrophic forgetting issue in FL. Then we propose a novel method calledTARGET (federat\textbf{T}ed cl\textbf{A}ss-continual lea\textbf{R}nin\textbf{G}via \textbf{E}xemplar-free dis\textbf{T}illation), which alleviatescatastrophic forgetting in FCCL while preserving client data privacy. Ourproposed method leverages the previously trained global model to transferknowledge of old tasks to the current task at the model level. Moreover, agenerator is trained to produce synthetic data to simulate the globaldistribution of data on each client at the data level. Compared to previousFCCL methods, TARGET does not require any additional datasets or storing realdata from previous tasks, which makes it ideal for data-sensitive scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2303.06937v3 |
| 618 | DDFM: Denoising Diffusion Model for Multi-Modality Image Fusion                                                                                      | Zixiang Zhao                   | 2023-03-13     | cs.CV                                   | Multi-modality image fusion aims to combine different modalities to producefused images that retain the complementary features of each modality, such asfunctional highlights and texture details. To leverage strong generative priorsand address challenges such as unstable training and lack of interpretabilityfor GAN-based generative methods, we propose a novel fusion algorithm based onthe denoising diffusion probabilistic model (DDPM). The fusion task isformulated as a conditional generation problem under the DDPM samplingframework, which is further divided into an unconditional generation subproblemand a maximum likelihood subproblem. The latter is modeled in a hierarchicalBayesian manner with latent variables and inferred by theexpectation-maximization (EM) algorithm. By integrating the inference solutioninto the diffusion sampling iteration, our method can generate high-qualityfused images with natural image generative priors and cross-modalityinformation from source images. Note that all we required is an unconditionalpre-trained generative model, and no fine-tuning is needed. Our extensiveexperiments indicate that our approach yields promising fusion results ininfrared-visible image fusion and medical image fusion. The code is availableat \url{https://github.com/Zhaozixiang1228/MMIF-DDFM}.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2303.06840v2 |
| 619 | Retinexformer: One-stage Retinex-based Transformer for Low-light Image Enhancement                                                                   | Yuanhao Cai                    | 2023-03-12     | cs.CV                                   | When enhancing low-light images, many deep learning algorithms are based onthe Retinex theory. However, the Retinex model does not consider thecorruptions hidden in the dark or introduced by the light-up process. Besides,these methods usually require a tedious multi-stage training pipeline and relyon convolutional neural networks, showing limitations in capturing long-rangedependencies. In this paper, we formulate a simple yet principled One-stageRetinex-based Framework (ORF). ORF first estimates the illumination informationto light up the low-light image and then restores the corruption to produce theenhanced image. We design an Illumination-Guided Transformer (IGT) thatutilizes illumination representations to direct the modeling of non-localinteractions of regions with different lighting conditions. By plugging IGTinto ORF, we obtain our algorithm, Retinexformer. Comprehensive quantitativeand qualitative experiments demonstrate that our Retinexformer significantlyoutperforms state-of-the-art methods on thirteen benchmarks. The user study andapplication on low-light object detection also reveal the latent practicalvalues of our method. Code, models, and results are available athttps://github.com/caiyuanhao1998/Retinexformer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | http://arxiv.org/abs/2303.06705v2 |
| 620 | Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models                                                            | Zangwei Zheng                  | 2023-03-12     | cs.CV, cs.LG                            | Continual learning (CL) can help pre-trained vision-language modelsefficiently adapt to new or under-trained data distributions withoutre-training. Nevertheless, during the continual training of the ContrastiveLanguage-Image Pre-training (CLIP) model, we observe that the model's zero-shottransfer ability significantly degrades due to catastrophic forgetting.Existing CL methods can mitigate forgetting by replaying previous data.However, since the CLIP dataset is private, replay methods cannot access thepre-training dataset. In addition, replaying data of previously learneddownstream tasks can enhance their performance but comes at the cost ofsacrificing zero-shot performance. To address this challenge, we propose anovel method ZSCL to prevent zero-shot transfer degradation in the continuallearning of vision-language models in both feature and parameter space. In thefeature space, a reference dataset is introduced for distillation between thecurrent and initial models. The reference dataset should have semanticdiversity but no need to be labeled, seen in pre-training, or matchedimage-text pairs. In parameter space, we prevent a large parameter shift byaveraging weights during the training. We propose a more challengingMulti-domain Task Incremental Learning (MTIL) benchmark to evaluate differentmethods, where tasks are from various domains instead of class-separated in asingle dataset. Our method outperforms other methods in the traditionalclass-incremental learning setting and the MTIL by 9.7% average score. Our codelocates at https://github.com/Thunderbeee/ZSCL.                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2303.06628v2 |
| 621 | Multi-metrics adaptively identifies backdoors in Federated learning                                                                                  | Siquan Huang                   | 2023-03-12     | cs.CR, cs.CV, cs.LG                     | The decentralized and privacy-preserving nature of federated learning (FL)makes it vulnerable to backdoor attacks aiming to manipulate the behavior ofthe resulting model on specific adversary-chosen inputs. However, most existingdefenses based on statistical differences take effect only against specificattacks, especially when the malicious gradients are similar to benign ones orthe data are highly non-independent and identically distributed (non-IID). Inthis paper, we revisit the distance-based defense methods and discover that i)Euclidean distance becomes meaningless in high dimensions and ii) maliciousgradients with diverse characteristics cannot be identified by a single metric.To this end, we present a simple yet effective defense strategy withmulti-metrics and dynamic weighting to identify backdoors adaptively.Furthermore, our novel defense has no reliance on predefined assumptions overattack settings or data distributions and little impact on benign performance.To evaluate the effectiveness of our approach, we conduct comprehensiveexperiments on different datasets under various attack settings, where ourmethod achieves the best defensive performance. For instance, we achieve thelowest backdoor accuracy of 3.06% under the difficult Edge-case PGD, showingsignificant superiority over previous defenses. The results also demonstratethat our method can be well-adapted to a wide range of non-IID degrees withoutsacrificing the benign performance.                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2303.06601v2 |
| 622 | Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models                                                                     | Juncheng Li                    | 2023-03-12     | cs.CV                                   | Prompt tuning, a recently emerging paradigm, enables the powerfulvision-language pre-training models to adapt to downstream tasks in a parameter-- and data -- efficient way, by learning the ``soft prompts'' to conditionfrozen pre-training models. Though effective, it is particularly problematic inthe few-shot scenario, where prompt tuning performance is sensitive to theinitialization and requires a time-consuming process to find a goodinitialization, thus restricting the fast adaptation ability of thepre-training models. In addition, prompt tuning could undermine thegeneralizability of the pre-training models, because the learnable prompttokens are easy to overfit to the limited training samples. To address theseissues, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM)framework that jointly meta-learns an efficient soft prompt initialization forbetter adaptation and a lightweight gradient regulating function for strongcross-domain generalizability in a meta-learning paradigm using only theunlabeled image-text pre-training data. Rather than designing a specific prompttuning method, our GRAM can be easily incorporated into various prompt tuningmethods in a model-agnostic way, and comprehensive experiments show that GRAMbrings about consistent improvement for them in several settings (i.e.,few-shot learning, cross-domain generalization, cross-dataset generalization,etc.) over 11 datasets. Further, experiments show that GRAM enables theorthogonal methods of textual and visual prompt tuning to work in amutually-enhanced way, offering better generalizability beyond the uni-modalprompt tuning methods.                                                                                                                                                                                                                                                       | http://arxiv.org/abs/2303.06571v2 |
| 623 | SemARFlow: Injecting Semantics into Unsupervised Optical Flow Estimation for Autonomous Driving                                                      | Shuai Yuan                     | 2023-03-10     | cs.CV, cs.RO                            | Unsupervised optical flow estimation is especially hard near occlusions andmotion boundaries and in low-texture regions. We show that additionalinformation such as semantics and domain knowledge can help better constrainthis problem. We introduce SemARFlow, an unsupervised optical flow networkdesigned for autonomous driving data that takes estimated semantic segmentationmasks as additional inputs. This additional information is injected into theencoder and into a learned upsampler that refines the flow output. In addition,a simple yet effective semantic augmentation module provides self-supervisionwhen learning flow and its boundaries for vehicles, poles, and sky. Together,these injections of semantic information improve the KITTI-2015 optical flowtest error rate from 11.80% to 8.38%. We also show visible improvements aroundobject boundaries as well as a greater ability to generalize across datasets.Code is available athttps://github.com/duke-vision/semantic-unsup-flow-release.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.06209v2 |
| 624 | Overwriting Pretrained Bias with Finetuning Data                                                                                                     | Angelina Wang                  | 2023-03-10     | cs.CV, cs.CY, cs.LG                     | Transfer learning is beneficial by allowing the expressive features of modelspretrained on large-scale datasets to be finetuned for the target task ofsmaller, more domain-specific datasets. However, there is a concern that thesepretrained models may come with their own biases which would propagate into thefinetuned model. In this work, we investigate bias when conceptualized as bothspurious correlations between the target task and a sensitive attribute as wellas underrepresentation of a particular group in the dataset. Under both notionsof bias, we find that (1) models finetuned on top of pretrained models canindeed inherit their biases, but (2) this bias can be corrected for throughrelatively minor interventions to the finetuning dataset, and often with anegligible impact to performance. Our findings imply that careful curation ofthe finetuning dataset is important for reducing biases on a downstream task,and doing so can even compensate for bias in the pretrained model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2303.06167v2 |
| 625 | StyleGANEX: StyleGAN-Based Manipulation Beyond Cropped Aligned Faces                                                                                 | Shuai Yang                     | 2023-03-10     | cs.CV, cs.LG                            | Recent advances in face manipulation using StyleGAN have produced impressiveresults. However, StyleGAN is inherently limited to cropped aligned faces at afixed image resolution it is pre-trained on. In this paper, we propose a simpleand effective solution to this limitation by using dilated convolutions torescale the receptive fields of shallow layers in StyleGAN, without alteringany model parameters. This allows fixed-size small features at shallow layersto be extended into larger ones that can accommodate variable resolutions,making them more robust in characterizing unaligned faces. To enable real faceinversion and manipulation, we introduce a corresponding encoder that providesthe first-layer feature of the extended StyleGAN in addition to the latentstyle code. We validate the effectiveness of our method using unaligned faceinputs of various resolutions in a diverse set of face manipulation tasks,including facial attribute editing, super-resolution, sketch/mask-to-facetranslation, and face toonification.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               | http://arxiv.org/abs/2303.06146v2 |
| 626 | GameFormer: Game-theoretic Modeling and Learning of Transformer-based Interactive Prediction and Planning for Autonomous Driving                     | Zhiyu Huang                    | 2023-03-10     | cs.RO                                   | Autonomous vehicles operating in complex real-world environments requireaccurate predictions of interactive behaviors between traffic participants.This paper tackles the interaction prediction problem by formulating it withhierarchical game theory and proposing the GameFormer model for itsimplementation. The model incorporates a Transformer encoder, which effectivelymodels the relationships between scene elements, alongside a novel hierarchicalTransformer decoder structure. At each decoding level, the decoder utilizes theprediction outcomes from the previous level, in addition to the sharedenvironmental context, to iteratively refine the interaction process. Moreover,we propose a learning process that regulates an agent's behavior at the currentlevel to respond to other agents' behaviors from the preceding level. Throughcomprehensive experiments on large-scale real-world driving datasets, wedemonstrate the state-of-the-art accuracy of our model on the Waymo interactionprediction task. Additionally, we validate the model's capacity to jointlyreason about the motion plan of the ego agent and the behaviors of multipleagents in both open-loop and closed-loop planning tests, outperforming variousbaseline methods. Furthermore, we evaluate the efficacy of our model on thenuPlan planning benchmark, where it achieves leading performance.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2303.05760v2 |
| 627 | Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity                                                              | Tong Liang                     | 2023-03-10     | cs.CV, cs.AI                            | There is a recently discovered and intriguing phenomenon called NeuralCollapse: at the terminal phase of training a deep neural network forclassification, the within-class penultimate feature means and the associatedclassifier vectors of all flat classes collapse to the vertices of a simplexEquiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenonby fixing the related classifier weights to a pre-computed ETF to induce neuralcollapse and maximize the separation of the learned features when training withimbalanced data. In this work, we propose to fix the linear classifier of adeep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF,and use a cosine similarity-based auxiliary loss to learn hierarchy-awarepenultimate features that collapse to the HAFrame. We demonstrate that ourapproach reduces the mistake severity of the model's predictions whilemaintaining its top-1 accuracy on several datasets of varying scales withhierarchies of heights ranging from 3 to 12. Code:https://github.com/ltong1130ztr/HAFrame                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           | http://arxiv.org/abs/2303.05689v2 |
| 628 | GPGait: Generalized Pose-based Gait Recognition                                                                                                      | Yang Fu                        | 2023-03-09     | cs.CV                                   | Recent works on pose-based gait recognition have demonstrated the potentialof using such simple information to achieve results comparable tosilhouette-based methods. However, the generalization ability of pose-basedmethods on different datasets is undesirably inferior to that ofsilhouette-based ones, which has received little attention but hinders theapplication of these methods in real-world scenarios. To improve thegeneralization ability of pose-based methods across datasets, we propose a\textbf{G}eneralized \textbf{P}ose-based \textbf{Gait} recognition(\textbf{GPGait}) framework. First, a Human-Oriented Transformation (HOT) and aseries of Human-Oriented Descriptors (HOD) are proposed to obtain a unifiedpose representation with discriminative multi-features. Then, given the slightvariations in the unified representation after HOT and HOD, it becomes crucialfor the network to extract local-global relationships between the keypoints. Tothis end, a Part-Aware Graph Convolutional Network (PAGCN) is proposed toenable efficient graph partition and local-global spatial feature extraction.Experiments on four public gait recognition datasets, CASIA-B, OUMVLP-Pose,Gait3D and GREW, show that our model demonstrates better and more stablecross-domain capabilities compared to existing skeleton-based methods,achieving comparable recognition results to silhouette-based ones. Code isavailable at https://github.com/BNU-IVC/FastPoseGait.                                                                                                                                                                                                                                                                                                                                                                                                                                                          | http://arxiv.org/abs/2303.05234v2 |
| 629 | Contrastive Model Adaptation for Cross-Condition Robustness in Semantic Segmentation                                                                 | David Bruggemann               | 2023-03-09     | cs.CV                                   | Standard unsupervised domain adaptation methods adapt models from a source toa target domain using labeled source data and unlabeled target data jointly. Inmodel adaptation, on the other hand, access to the labeled source data isprohibited, i.e., only the source-trained model and unlabeled target data areavailable. We investigate normal-to-adverse condition model adaptation forsemantic segmentation, whereby image-level correspondences are available in thetarget domain. The target set consists of unlabeled pairs of adverse- andnormal-condition street images taken at GPS-matched locations. Our method --CMA -- leverages such image pairs to learn condition-invariant features viacontrastive learning. In particular, CMA encourages features in the embeddingspace to be grouped according to their condition-invariant semantic content andnot according to the condition under which respective inputs are captured. Toobtain accurate cross-domain semantic correspondences, we warp the normal imageto the viewpoint of the adverse image and leverage warp-confidence scores tocreate robust, aggregated features. With this approach, we achievestate-of-the-art semantic segmentation performance for model adaptation onseveral normal-to-adverse adaptation benchmarks, such as ACDC and Dark Zurich.We also evaluate CMA on a newly procured adverse-condition generalizationbenchmark and report favorable results compared to standard unsupervised domainadaptation methods, despite the comparative handicap of CMA due to source datainaccessibility. Code is available at https://github.com/brdav/cma.                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2303.05194v3 |
| 630 | SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model                                                           | Gengwei Zhang                  | 2023-03-09     | cs.CV, cs.AI, cs.LG                     | The goal of continual learning is to improve the performance of recognitionmodels in learning sequentially arrived data. Although most existing works areestablished on the premise of learning from scratch, growing efforts have beendevoted to incorporating the benefits of pre-training. However, how toadaptively exploit the pre-trained knowledge for each incremental task whilemaintaining its generalizability remains an open question. In this work, wepresent an extensive analysis for continual learning on a pre-trained model(CLPM), and attribute the key challenge to a progressive overfitting problem.Observing that selectively reducing the learning rate can almost resolve thisissue in the representation layer, we propose a simple but extremely effectiveapproach named Slow Learner with Classifier Alignment (SLCA), which furtherimproves the classification layer by modeling the class-wise distributions andaligning the classification layers in a post-hoc fashion. Across a variety ofscenarios, our proposal provides substantial improvements for CLPM (e.g., up to49.76%, 50.05%, 44.69% and 40.16% on Split CIFAR-100, Split ImageNet-R, SplitCUB-200 and Split Cars-196, respectively), and thus outperformsstate-of-the-art approaches by a large margin. Based on such a strong baseline,critical factors and promising directions are analyzed in-depth to facilitatesubsequent research. Code has been made available at:https://github.com/GengDavid/SLCA.                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2303.05118v3 |
| 631 | ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction                                               | Jiabang He                     | 2023-03-09     | cs.CL                                   | Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstratedremarkable results in various natural language processing (NLP) tasks within-context learning, which involves inference based on a few demonstrationexamples. Despite their successes in NLP tasks, no investigation has beenconducted to assess the ability of LLMs to perform document informationextraction (DIE) using in-context learning. Applying LLMs to DIE poses twochallenges: the modality and task gap. To this end, we propose a simple buteffective in-context learning framework called ICL-D3IE, which enables LLMs toperform DIE with different types of demonstration examples. Specifically, weextract the most difficult and distinct segments from hard training documentsas hard demonstrations for benefiting all test instances. We designdemonstrations describing relationships that enable LLMs to understandpositional relationships. We introduce formatting demonstrations for easyanswer extraction. Additionally, the framework improves diverse demonstrationsby updating them iteratively. Our experiments on three widely used benchmarkdatasets demonstrate that the ICL-D3IE framework enables Davinci-003/ChatGPT toachieve superior performance when compared to previous pre-trained methodsfine-tuned with full training in both the in-distribution (ID) setting and inthe out-of-distribution (OOD) setting. Code is available athttps://github.com/MAEHCM/ICL-D3IE.                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | http://arxiv.org/abs/2303.05063v4 |
| 632 | Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation                                                                                 | Qichen Fu                      | 2023-03-09     | cs.CV                                   | Accurately estimating 3D hand pose is crucial for understanding how humansinteract with the world. Despite remarkable progress, existing methods oftenstruggle to generate plausible hand poses when the hand is heavily occluded orblurred. In videos, the movements of the hand allow us to observe various partsof the hand that may be occluded or blurred in a single frame. To adaptivelyleverage the visual clue before and after the occlusion or blurring for robusthand pose estimation, we propose the Deformer: a framework that implicitlyreasons about the relationship between hand parts within the same image(spatial dimension) and different timesteps (temporal dimension). We show thata naive application of the transformer self-attention mechanism is notsufficient because motion blur or occlusions in certain frames can lead toheavily distorted hand features and generate imprecise keys and queries. Toaddress this challenge, we incorporate a Dynamic Fusion Module into Deformer,which predicts the deformation of the hand and warps the hand mesh predictionsfrom nearby frames to explicitly support the current frame estimation.Furthermore, we have observed that errors are unevenly distributed acrossdifferent hand parts, with vertices around fingertips having disproportionatelyhigher errors than those around the palm. We mitigate this issue by introducinga new loss function called maxMSE that automatically adjusts the weight ofevery vertex to focus the model on critical hand parts. Extensive experimentsshow that our method significantly outperforms state-of-the-art methods by 10%,and is more robust to occlusions (over 14%).                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2303.04991v2 |
| 633 | CROSSFIRE: Camera Relocalization On Self-Supervised Features from an Implicit Representation                                                         | Arthur Moreau                  | 2023-03-08     | cs.CV                                   | Beyond novel view synthesis, Neural Radiance Fields are useful forapplications that interact with the real world. In this paper, we use them asan implicit map of a given scene and propose a camera relocalization algorithmtailored for this representation. The proposed method enables to compute inreal-time the precise position of a device using a single RGB camera, duringits navigation. In contrast with previous work, we do not rely on poseregression or photometric alignment but rather use dense local featuresobtained through volumetric rendering which are specialized on the scene with aself-supervised objective. As a result, our algorithm is more accurate thancompetitors, able to operate in dynamic outdoor environments with changinglightning conditions and can be readily integrated in any volumetric neuralrenderer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2303.04869v2 |
| 634 | CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning                                                                      | Hritik Bansal                  | 2023-03-06     | cs.CV, cs.AI, cs.CR, cs.LG              | Multimodal contrastive pretraining has been used to train multimodalrepresentation models, such as CLIP, on large amounts of paired image-textdata. However, previous studies have revealed that such models are vulnerableto backdoor attacks. Specifically, when trained on backdoored examples, CLIPlearns spurious correlations between the embedded backdoor trigger and thetarget label, aligning their representations in the joint embedding space.Injecting even a small number of poisoned examples, such as 75 examples in 3million pretraining data, can significantly manipulate the model's behavior,making it difficult to detect or unlearn such correlations. To address thisissue, we propose CleanCLIP, a finetuning framework that weakens the learnedspurious associations introduced by backdoor attacks by independentlyre-aligning the representations for individual modalities. We demonstrate thatunsupervised finetuning using a combination of multimodal contrastive andunimodal self-supervised objectives for individual modalities can significantlyreduce the impact of the backdoor attack. Additionally, we show that supervisedfinetuning on task-specific labeled image data removes the backdoor triggerfrom the CLIP vision encoder. We show empirically that CleanCLIP maintainsmodel performance on benign examples while erasing a range of backdoor attackson multimodal contrastive learning. The code and checkpoints are available athttps://github.com/nishadsinghi/CleanCLIP.                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2303.03323v3 |
| 635 | Delicate Textured Mesh Recovery from NeRF via Adaptive Surface Refinement                                                                            | Jiaxiang Tang                  | 2023-03-03     | cs.CV                                   | Neural Radiance Fields (NeRF) have constituted a remarkable breakthrough inimage-based 3D reconstruction. However, their implicit volumetricrepresentations differ significantly from the widely-adopted polygonal meshesand lack support from common 3D software and hardware, making their renderingand manipulation inefficient. To overcome this limitation, we present a novelframework that generates textured surface meshes from images. Our approachbegins by efficiently initializing the geometry and view-dependency decomposedappearance with a NeRF. Subsequently, a coarse mesh is extracted, and aniterative surface refining algorithm is developed to adaptively adjust bothvertex positions and face density based on re-projected rendering errors. Wejointly refine the appearance with geometry and bake it into texture images forreal-time rendering. Extensive experiments demonstrate that our method achievessuperior mesh quality and competitive rendering quality.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2303.02091v2 |
| 636 | ABAW: Valence-Arousal Estimation, Expression Recognition, Action Unit Detection & Emotional Reaction Intensity Estimation Challenges                 | Dimitrios Kollias              | 2023-03-02     | cs.CV, cs.LG                            | The fifth Affective Behavior Analysis in-the-wild (ABAW) Competition is partof the respective ABAW Workshop which will be held in conjunction with IEEEComputer Vision and Pattern Recognition Conference (CVPR), 2023. The 5th ABAWCompetition is a continuation of the Competitions held at ECCV 2022, IEEE CVPR2022, ICCV 2021, IEEE FG 2020 and CVPR 2017 Conferences, and is dedicated atautomatically analyzing affect. For this year's Competition, we feature twocorpora: i) an extended version of the Aff-Wild2 database and ii) theHume-Reaction dataset. The former database is an audiovisual one of around 600videos of around 3M frames and is annotated with respect to:a) two continuousaffect dimensions -valence (how positive/negative a person is) and arousal (howactive/passive a person is)-; b) basic expressions (e.g. happiness, sadness,neutral state); and c) atomic facial muscle actions (i.e., action units). Thelatter dataset is an audiovisual one in which reactions of individuals toemotional stimuli have been annotated with respect to seven emotionalexpression intensities. Thus the 5th ABAW Competition encompasses fourChallenges: i) uni-task Valence-Arousal Estimation, ii) uni-task ExpressionClassification, iii) uni-task Action Unit Detection, and iv) Emotional ReactionIntensity Estimation. In this paper, we present these Challenges, along withtheir corpora, we outline the evaluation metrics, we present the baselinesystems and illustrate their obtained performance.                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2303.01498v3 |
| 637 | HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation                                                           | Kai Zhai                       | 2023-02-28     | cs.CV                                   | 2D-to-3D human pose lifting is fundamental for 3D human pose estimation(HPE), for which graph convolutional networks (GCNs) have proven inherentlysuitable for modeling the human skeletal topology. However, the currentGCN-based 3D HPE methods update the node features by aggregating theirneighbors' information without considering the interaction of joints indifferent joint synergies. Although some studies have proposed importing limbinformation to learn the movement patterns, the latent synergies among joints,such as maintaining balance are seldom investigated. We propose the Hop-wiseGraphFormer with Intragroup Joint Refinement (HopFIR) architecture to tacklethe 3D HPE problem. HopFIR mainly consists of a novel hop-wise GraphFormer(HGF) module and an intragroup joint refinement (IJR) module. The HGF modulegroups the joints by k-hop neighbors and applies a hopwise transformer-likeattention mechanism to these groups to discover latent joint synergies. The IJRmodule leverages the prior limb information for peripheral joint refinement.Extensive experimental results show that HopFIR outperforms the SOTA methods bya large margin, with a mean per-joint position error (MPJPE) on the Human3.6Mdataset of 32.67 mm. We also demonstrate that the state-of-the-art GCN-basedmethods can benefit from the proposed hop-wise attention mechanism with asignificant improvement in performance: SemGCN and MGCN are improved by 8.9%and 4.5%, respectively.                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2302.14581v3 |
| 638 | BEVPlace: Learning LiDAR-based Place Recognition using Bird's Eye View Images                                                                        | Lun Luo                        | 2023-02-28     | cs.CV, cs.RO                            | Place recognition is a key module for long-term SLAM systems. CurrentLiDAR-based place recognition methods usually use representations of pointclouds such as unordered points or range images. These methods achieve highrecall rates of retrieval, but their performance may degrade in the case ofview variation or scene changes. In this work, we explore the potential of adifferent representation in place recognition, i.e. bird's eye view (BEV)images. We observe that the structural contents of BEV images are lessinfluenced by rotations and translations of point clouds. We validate that,without any delicate design, a simple VGGNet trained on BEV images achievescomparable performance with the state-of-the-art place recognition methods inscenes of slight viewpoint changes. For more robust place recognition, wedesign a rotation-invariant network called BEVPlace. We use group convolutionto extract rotation-equivariant local features from the images and NetVLAD forglobal feature aggregation. In addition, we observe that the distance betweenBEV features is correlated with the geometry distance of point clouds. Based onthe observation, we develop a method to estimate the position of the querycloud, extending the usage of place recognition. The experiments conducted onlarge-scale public datasets show that our method 1) achieves state-of-the-artperformance in terms of recall rates, 2) is robust to view changes, 3) showsstrong generalization ability, and 4) can estimate the positions of query pointclouds. Source codes are publicly available athttps://github.com/zjuluolun/BEVPlace.                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2302.14325v3 |
| 639 | Towards Memory- and Time-Efficient Backpropagation for Training Spiking Neural Networks                                                              | Qingyan Meng                   | 2023-02-28     | cs.NE, cs.LG                            | Spiking Neural Networks (SNNs) are promising energy-efficient models forneuromorphic computing. For training the non-differentiable SNN models, thebackpropagation through time (BPTT) with surrogate gradients (SG) method hasachieved high performance. However, this method suffers from considerablememory cost and training time during training. In this paper, we propose theSpatial Learning Through Time (SLTT) method that can achieve high performancewhile greatly improving training efficiency compared with BPTT. First, we showthat the backpropagation of SNNs through the temporal domain contributes just alittle to the final calculated gradients. Thus, we propose to ignore theunimportant routes in the computational graph during backpropagation. Theproposed method reduces the number of scalar multiplications and achieves asmall memory occupation that is independent of the total time steps.Furthermore, we propose a variant of SLTT, called SLTT-K, that allowsbackpropagation only at K time steps, then the required number of scalarmultiplications is further reduced and is independent of the total time steps.Experiments on both static and neuromorphic datasets demonstrate superiortraining efficiency and performance of our SLTT. In particular, our methodachieves state-of-the-art accuracy on ImageNet, while the memory cost andtraining time are reduced by more than 70% and 50%, respectively, compared withBPTT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2302.14311v3 |
| 640 | GLOW: Global Layout Aware Attacks on Object Detection                                                                                                | Buyu Liu                       | 2023-02-27     | cs.CV                                   | Adversarial attacks aim to perturb images such that a predictor outputsincorrect results. Due to the limited research in structured attacks, imposingconsistency checks on natural multi-object scenes is a promising yet practicaldefense against conventional adversarial attacks. More desired attacks, to thisend, should be able to fool defenses with such consistency checks. Therefore,we present the first approach GLOW that copes with various attack requests bygenerating global layout-aware adversarial attacks, in which both categoricaland geometric layout constraints are explicitly established. Specifically, wefocus on object detection task and given a victim image, GLOW first localizesvictim objects according to target labels. And then it generates multipleattack plans, together with their context-consistency scores. Our proposedGLOW, on the one hand, is capable of handling various types of requests,including single or multiple victim objects, with or without specified victimobjects. On the other hand, it produces a consistency score for each attackplan, reflecting the overall contextual consistency that both semantic categoryand global scene layout are considered. In experiment, we design multiple typesof attack requests and validate our ideas on MS COCO and Pascal. Extensiveexperimental results demonstrate that we can achieve about 30$\%$ averagerelative improvement compared to state-of-the-art methods in conventionalsingle object attack request; Moreover, our method outperforms SOTAssignificantly on more generic attack requests by about 20$\%$ in average;Finally, our method produces superior performance under challenging zero-queryblack-box setting, or 20$\%$ better than SOTAs. Our code, model and attackrequests would be made available.                                                                                                                        | http://arxiv.org/abs/2302.14166v2 |
| 641 | UMIFormer: Mining the Correlations between Similar Tokens for Multi-View 3D Reconstruction                                                           | Zhenwei Zhu                    | 2023-02-27     | cs.CV                                   | In recent years, many video tasks have achieved breakthroughs by utilizingthe vision transformer and establishing spatial-temporal decoupling for featureextraction. Although multi-view 3D reconstruction also faces multiple images asinput, it cannot immediately inherit their success due to completely ambiguousassociations between unstructured views. There is not usable priorrelationship, which is similar to the temporally-coherence property in a video.To solve this problem, we propose a novel transformer network for UnstructuredMultiple Images (UMIFormer). It exploits transformer blocks for decoupledintra-view encoding and designed blocks for token rectification that mine thecorrelation between similar tokens from different views to achieve decoupledinter-view encoding. Afterward, all tokens acquired from various branches arecompressed into a fixed-size compact representation while preserving richinformation for reconstruction by leveraging the similarities between tokens.We empirically demonstrate on ShapeNet and confirm that our decoupled learningmethod is adaptable for unstructured multiple images. Meanwhile, theexperiments also verify our model outperforms existing SOTA methods by a largemargin. Code will be available at https://github.com/GaryZhu1996/UMIFormer.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | http://arxiv.org/abs/2302.13987v2 |
| 642 | ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation                                                      | Yuxiang Wei                    | 2023-02-27     | cs.CV                                   | In addition to the unprecedented ability in imaginary creation, largetext-to-image models are expected to take customized concepts in imagegeneration. Existing works generally learn such concepts in anoptimization-based manner, yet bringing excessive computation or memory burden.In this paper, we instead propose a learning-based encoder, which consists of aglobal and a local mapping networks for fast and accurate customizedtext-to-image generation. In specific, the global mapping network projects thehierarchical features of a given image into multiple new words in the textualword embedding space, i.e., one primary word for well-editable concept andother auxiliary words to exclude irrelevant disturbances (e.g., background). Inthe meantime, a local mapping network injects the encoded patch features intocross attention layers to provide omitted details, without sacrificing theeditability of primary concepts. We compare our method with existingoptimization-based approaches on a variety of user-defined concepts, anddemonstrate that our method enables high-fidelity inversion and more robusteditability with a significantly faster encoding process. Our code is publiclyavailable at https://github.com/csyxwei/ELITE.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2302.13848v2 |
| 643 | Learning Non-Local Spatial-Angular Correlation for Light Field Image Super-Resolution                                                                | Zhengyu Liang                  | 2023-02-16     | cs.CV                                   | Exploiting spatial-angular correlation is crucial to light field (LF) imagesuper-resolution (SR), but is highly challenging due to its non-local propertycaused by the disparities among LF images. Although many deep neural networks(DNNs) have been developed for LF image SR and achieved continuously improvedperformance, existing methods cannot well leverage the long-rangespatial-angular correlation and thus suffer a significant performance drop whenhandling scenes with large disparity variations. In this paper, we propose asimple yet effective method to learn the non-local spatial-angular correlationfor LF image SR. In our method, we adopt the epipolar plane image (EPI)representation to project the 4D spatial-angular correlation onto multiple 2DEPI planes, and then develop a Transformer network with repetitiveself-attention operations to learn the spatial-angular correlation by modelingthe dependencies between each pair of EPI pixels. Our method can fullyincorporate the information from all angular views while achieving a globalreceptive field along the epipolar line. We conduct extensive experiments withinsightful visualizations to validate the effectiveness of our method.Comparative results on five public datasets show that our method not onlyachieves state-of-the-art SR performance, but also performs robust to disparityvariations. Code is publicly available athttps://github.com/ZhengyuLiang24/EPIT.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                | http://arxiv.org/abs/2302.08058v3 |
| 644 | LiveHand: Real-time and Photorealistic Neural Hand Rendering                                                                                         | Akshay Mundra                  | 2023-02-15     | cs.GR                                   | The human hand is the main medium through which we interact with oursurroundings, making its digitization an important problem. While there areseveral works modeling the geometry of hands, little attention has been paid tocapturing photo-realistic appearance. Moreover, for applications in extendedreality and gaming, real-time rendering is critical. We present the firstneural-implicit approach to photo-realistically render hands in real-time. Thisis a challenging problem as hands are textured and undergo strong articulationswith pose-dependent effects. However, we show that this aim is achievablethrough our carefully designed method. This includes training on alow-resolution rendering of a neural radiance field, together with a3D-consistent super-resolution module and mesh-guided sampling and spacecanonicalization. We demonstrate a novel application of perceptual loss on theimage space, which is critical for learning details accurately. We also show alive demo where we photo-realistically render the human hand in real-time forthe first time, while also modeling pose- and view-dependent appearanceeffects. We ablate all our design choices and show that they optimize forrendering speed and quality. Video results and our code can be accessed fromhttps://vcai.mpi-inf.mpg.de/projects/LiveHand/                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2302.07672v3 |
| 645 | 3D-aware Blending with Generative NeRFs                                                                                                              | Hyunsu Kim                     | 2023-02-13     | cs.CV, cs.GR, cs.LG                     | Image blending aims to combine multiple images seamlessly. It remainschallenging for existing 2D-based methods, especially when input images aremisaligned due to differences in 3D camera poses and object shapes. To tacklethese issues, we propose a 3D-aware blending method using generative NeuralRadiance Fields (NeRF), including two key components: 3D-aware alignment and3D-aware blending. For 3D-aware alignment, we first estimate the camera pose ofthe reference image with respect to generative NeRFs and then perform 3D localalignment for each part. To further leverage 3D information of the generativeNeRF, we propose 3D-aware blending that directly blends images on the NeRF'slatent representation space, rather than raw pixel space. Collectively, ourmethod outperforms existing 2D baselines, as validated by extensivequantitative and qualitative evaluations with FFHQ and AFHQ-Cat.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      | http://arxiv.org/abs/2302.06608v3 |
| 646 | Anatomical Invariance Modeling and Semantic Alignment for Self-supervised Learning in 3D Medical Image Analysis                                      | Yankai Jiang                   | 2023-02-11     | cs.CV, cs.AI                            | Self-supervised learning (SSL) has recently achieved promising performancefor 3D medical image analysis tasks. Most current methods follow existing SSLparadigm originally designed for photographic or natural images, which cannotexplicitly and thoroughly exploit the intrinsic similar anatomical structuresacross varying medical images. This may in fact degrade the quality of learneddeep representations by maximizing the similarity among features containingspatial misalignment information and different anatomical semantics. In thiswork, we propose a new self-supervised learning framework, namely Alice, thatexplicitly fulfills Anatomical invariance modeling and semantic alignment viaelaborately combining discriminative and generative objectives. Aliceintroduces a new contrastive learning strategy which encourages the similaritybetween views that are diversely mined but with consistent high-levelsemantics, in order to learn invariant anatomical features. Moreover, we designa conditional anatomical feature alignment module to complement corruptedembeddings with globally matched semantics and inter-patch topologyinformation, conditioned by the distribution of local image content, whichpermits to create better contrastive pairs. Our extensive quantitativeexperiments on three 3D medical image analysis tasks demonstrate and validatethe performance superiority of Alice, surpassing the previous best SSLcounterpart methods and showing promising ability for united representationlearning. Codes are available at https://github.com/alibaba-damo-academy/alice.                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2302.05615v3 |
| 647 | Enhancing Modality-Agnostic Representations via Meta-Learning for Brain Tumor Segmentation                                                           | Aishik Konwer                  | 2023-02-08     | cs.CV                                   | In medical vision, different imaging modalities provide complementaryinformation. However, in practice, not all modalities may be available duringinference or even training. Previous approaches, e.g., knowledge distillationor image synthesis, often assume the availability of full modalities for allpatients during training; this is unrealistic and impractical due to thevariability in data collection across sites. We propose a novel approach tolearn enhanced modality-agnostic representations by employing a meta-learningstrategy in training, even when only limited full modality samples areavailable. Meta-learning enhances partial modality representations to fullmodality representations by meta-training on partial modality data andmeta-testing on limited full modality samples. Additionally, we co-supervisethis feature enrichment by introducing an auxiliary adversarial learningbranch. More specifically, a missing modality detector is used as adiscriminator to mimic the full modality setting. Our segmentation frameworksignificantly outperforms state-of-the-art brain tumor segmentation techniquesin missing modality scenarios.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2302.04308v2 |
| 648 | HumanMAC: Masked Motion Completion for Human Motion Prediction                                                                                       | Ling-Hao Chen                  | 2023-02-07     | cs.CV, cs.AI                            | Human motion prediction is a classical problem in computer vision andcomputer graphics, which has a wide range of practical applications. Previouseffects achieve great empirical performance based on an encoding-decodingstyle. The methods of this style work by first encoding previous motions tolatent representations and then decoding the latent representations intopredicted motions. However, in practice, they are still unsatisfactory due toseveral issues, including complicated loss constraints, cumbersome trainingprocesses, and scarce switch of different categories of motions in prediction.In this paper, to address the above issues, we jump out of the foregoing styleand propose a novel framework from a new perspective. Specifically, ourframework works in a masked completion fashion. In the training stage, we learna motion diffusion model that generates motions from random noise. In theinference stage, with a denoising procedure, we make motion predictionconditioning on observed motions to output more continuous and controllablepredictions. The proposed framework enjoys promising algorithmic properties,which only needs one loss in optimization and is trained in an end-to-endmanner. Additionally, it accomplishes the switch of different categories ofmotions effectively, which is significant in realistic tasks, e.g., theanimation task. Comprehensive experiments on benchmarks confirm the superiorityof the proposed framework. The project page is available athttps://lhchen.top/Human-MAC.                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2302.03665v4 |
| 649 | Decoupled Iterative Refinement Framework for Interacting Hands Reconstruction from a Single RGB Image                                                | Pengfei Ren                    | 2023-02-05     | cs.CV, cs.AI                            | Reconstructing interacting hands from a single RGB image is a verychallenging task. On the one hand, severe mutual occlusion and similar localappearance between two hands confuse the extraction of visual features,resulting in the misalignment of estimated hand meshes and the image. On theother hand, there are complex spatial relationship between interacting hands,which significantly increases the solution space of hand poses and increasesthe difficulty of network learning. In this paper, we propose a decouplediterative refinement framework to achieve pixel-alignment hand reconstructionwhile efficiently modeling the spatial relationship between hands.Specifically, we define two feature spaces with different characteristics,namely 2D visual feature space and 3D joint feature space. First, we obtainjoint-wise features from the visual feature map and utilize a graph convolutionnetwork and a transformer to perform intra- and inter-hand informationinteraction in the 3D joint feature space, respectively. Then, we project thejoint features with global information back into the 2D visual feature space inan obfuscation-free manner and utilize the 2D convolution for pixel-wiseenhancement. By performing multiple alternate enhancements in the two featurespaces, our method can achieve an accurate and robust reconstruction ofinteracting hands. Our method outperforms all existing two-hand reconstructionmethods by a large margin on the InterHand2.6M dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2302.02410v2 |
| 650 | Get3DHuman: Lifting StyleGAN-Human into a 3D Generative Model using Pixel-aligned Reconstruction Priors                                              | Zhangyang Xiong                | 2023-02-02     | cs.CV                                   | Fast generation of high-quality 3D digital humans is important to a vastnumber of applications ranging from entertainment to professional concerns.Recent advances in differentiable rendering have enabled the training of 3Dgenerative models without requiring 3D ground truths. However, the quality ofthe generated 3D humans still has much room to improve in terms of bothfidelity and diversity. In this paper, we present Get3DHuman, a novel 3D humanframework that can significantly boost the realism and diversity of thegenerated outcomes by only using a limited budget of 3D ground-truth data. Ourkey observation is that the 3D generator can profit from human-related priorslearned through 2D human generators and 3D reconstructors. Specifically, webridge the latent space of Get3DHuman with that of StyleGAN-Human via aspecially-designed prior network, where the input latent code is mapped to theshape and texture feature volumes spanned by the pixel-aligned 3Dreconstructor. The outcomes of the prior network are then leveraged as thesupervisory signals for the main generator network. To ensure effectivetraining, we further propose three tailored losses applied to the generatedfeature volumes and the intermediate feature maps. Extensive experimentsdemonstrate that Get3DHuman greatly outperforms the other state-of-the-artapproaches and can support a wide range of applications including shapeinterpolation, shape re-texturing, and single-view reconstruction throughlatent inversion.                                                                                                                                                                                                                                                                                                                                                                                                            | http://arxiv.org/abs/2302.01162v5 |
| 651 | HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning                                                                    | Xiaozheng Zheng                | 2023-02-02     | cs.CV                                   | Recent advancements in 3D hand pose estimation have shown promising results,but its effectiveness has primarily relied on the availability of large-scaleannotated datasets, the creation of which is a laborious and costly process. Toalleviate the label-hungry limitation, we propose a self-supervised learningframework, HaMuCo, that learns a single-view hand pose estimator frommulti-view pseudo 2D labels. However, one of the main challenges ofself-supervised learning is the presence of noisy labels and the ``groupthink''effect from multiple views. To overcome these issues, we introduce a cross-viewinteraction network that distills the single-view estimator by utilizing thecross-view correlated features and enforcing multi-view consistency to achievecollaborative learning. Both the single-view estimator and the cross-viewinteraction network are trained jointly in an end-to-end manner. Extensiveexperiments show that our method can achieve state-of-the-art performance onmulti-view self-supervised hand pose estimation. Furthermore, the proposedcross-view interaction network can also be applied to hand pose estimation frommulti-view input and outperforms previous methods under the same settings.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2302.00988v2 |
| 652 | Learning Good Features to Transfer Across Tasks and Domains                                                                                          | Pierluigi Zama Ramirez         | 2023-01-26     | cs.CV                                   | Availability of labelled data is the major obstacle to the deployment of deeplearning algorithms for computer vision tasks in new domains. The fact thatmany frameworks adopted to solve different tasks share the same architecturesuggests that there should be a way of reusing the knowledge learned in aspecific setting to solve novel tasks with limited or no additionalsupervision. In this work, we first show that such knowledge can be sharedacross tasks by learning a mapping between task-specific deep features in agiven domain. Then, we show that this mapping function, implemented by a neuralnetwork, is able to generalize to novel unseen domains. Besides, we propose aset of strategies to constrain the learned feature spaces, to ease learning andincrease the generalization capability of the mapping network, therebyconsiderably improving the final performance of our framework. Our proposalobtains compelling results in challenging synthetic-to-real adaptationscenarios by transferring knowledge between monocular depth estimation andsemantic segmentation tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2301.11310v1 |
| 653 | BallGAN: 3D-aware Image Synthesis with a Spherical Background                                                                                        | Minjung Shin                   | 2023-01-22     | cs.CV, cs.AI, cs.LG                     | 3D-aware GANs aim to synthesize realistic 3D scenes such that they can berendered in arbitrary perspectives to produce images. Although previous methodsproduce realistic images, they suffer from unstable training or degeneratesolutions where the 3D geometry is unnatural. We hypothesize that the 3Dgeometry is underdetermined due to the insufficient constraint, i.e., beingclassified as real image to the discriminator is not enough. To solve thisproblem, we propose to approximate the background as a spherical surface andrepresent a scene as a union of the foreground placed in the sphere and thethin spherical background. It reduces the degree of freedom in the backgroundfield. Accordingly, we modify the volume rendering equation and incorporatededicated constraints to design a novel 3D-aware GAN framework named BallGAN.BallGAN has multiple advantages as follows. 1) It produces more reasonable 3Dgeometry; the images of a scene across different viewpoints have betterphotometric consistency and fidelity than the state-of-the-art methods. 2) Thetraining becomes much more stable. 3) The foreground can be separately renderedon top of different arbitrary backgrounds.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        | http://arxiv.org/abs/2301.09091v3 |
| 654 | CLIPTER: Looking at the Bigger Picture in Scene Text Recognition                                                                                     | Aviad Aberdam                  | 2023-01-18     | cs.CV, cs.LG                            | Reading text in real-world scenarios often requires understanding the contextsurrounding it, especially when dealing with poor-quality text. However,current scene text recognizers are unaware of the bigger picture as theyoperate on cropped text images. In this study, we harness the representativecapabilities of modern vision-language models, such as CLIP, to providescene-level information to the crop-based recognizer. We achieve this by fusinga rich representation of the entire image, obtained from the vision-languagemodel, with the recognizer word-level features via a gated cross-attentionmechanism. This component gradually shifts to the context-enhancedrepresentation, allowing for stable fine-tuning of a pretrained recognizer. Wedemonstrate the effectiveness of our model-agnostic framework, CLIPTER (CLIPTExt Recognition), on leading text recognition architectures and achievestate-of-the-art results across multiple benchmarks. Furthermore, our analysishighlights improved robustness to out-of-vocabulary words and enhancedgeneralization in low-data regimes.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              | http://arxiv.org/abs/2301.07464v2 |
| 655 | FemtoDet: An Object Detection Baseline for Energy Versus Performance Tradeoffs                                                                       | Peng Tu                        | 2023-01-17     | cs.CV                                   | Efficient detectors for edge devices are often optimized for parameters orspeed count metrics, which remain in weak correlation with the energy ofdetectors.  However, some vision applications of convolutional neural networks, such asalways-on surveillance cameras, are critical for energy constraints.  This paper aims to serve as a baseline by designing detectors to reachtradeoffs between energy and performance from two perspectives:  1) We extensively analyze various CNNs to identify low-energy architectures,including selecting activation functions, convolutions operators, and featurefusion structures on necks. These underappreciated details in past workseriously affect the energy consumption of detectors;  2) To break through the dilemmatic energy-performance problem, we propose abalanced detector driven by energy using discovered low-energy components named\textit{FemtoDet}.  In addition to the novel construction, we improve FemtoDet by consideringconvolutions and training strategy optimizations.  Specifically, we develop a new instance boundary enhancement (IBE) module forconvolution optimization to overcome the contradiction between the limitedcapacity of CNNs and detection tasks in diverse spatial representations, andpropose a recursive warm-restart (RecWR) for optimizing training strategy toescape the sub-optimization of light-weight detectors by considering the datashift produced in popular augmentations.  As a result, FemtoDet with only 68.77k parameters achieves a competitivescore of 46.3 AP50 on PASCAL VOC and 1.11 W $\&$ 64.47 FPS on QualcommSnapdragon 865 CPU platforms.  Extensive experiments on COCO and TJU-DHD datasets indicate that the proposedmethod achieves competitive results in diverse scenes.                                                                                                                                                 | http://arxiv.org/abs/2301.06719v5 |
| 656 | AdaPoinTr: Diverse Point Cloud Completion with Adaptive Geometry-Aware Transformers                                                                  | Xumin Yu                       | 2023-01-11     | cs.CV, cs.AI                            | In this paper, we present a new method that reformulates point cloudcompletion as a set-to-set translation problem and design a new model, calledPoinTr, which adopts a Transformer encoder-decoder architecture for point cloudcompletion. By representing the point cloud as a set of unordered groups ofpoints with position embeddings, we convert the input data to a sequence ofpoint proxies and employ the Transformers for generation. To facilitateTransformers to better leverage the inductive bias about 3D geometricstructures of point clouds, we further devise a geometry-aware block thatmodels the local geometric relationships explicitly. The migration ofTransformers enables our model to better learn structural knowledge andpreserve detailed information for point cloud completion. Taking a step towardsmore complicated and diverse situations, we further propose AdaPoinTr bydeveloping an adaptive query generation mechanism and designing a noveldenoising task during completing a point cloud. Coupling these two techniquesenables us to train the model efficiently and effectively: we reduce trainingtime (by 15x or more) and improve completion performance (over 20%). We alsoshow our method can be extended to the scene-level point cloud completionscenario by designing a new geometry-enhanced semantic scene completionframework. Extensive experiments on the existing and newly-proposed datasetsdemonstrate the effectiveness of our method, which attains 6.53 CD on PCN, 0.81CD on ShapeNet-55 and 0.392 MMD on real-world KITTI, surpassing other work by alarge margin and establishing new state-of-the-arts on various benchmarks. Mostnotably, AdaPoinTr can achieve such promising performance with higherthroughputs and fewer FLOPs compared with the previous best methods inpractice. The code and datasets are available athttps://github.com/yuxumin/PoinTr                              | http://arxiv.org/abs/2301.04545v1 |
| 657 | Learning Support and Trivial Prototypes for Interpretable Image Classification                                                                       | Chong Wang                     | 2023-01-08     | cs.CV                                   | Prototypical part network (ProtoPNet) methods have been designed to achieveinterpretable classification by associating predictions with a set of trainingprototypes, which we refer to as trivial prototypes because they are trained tolie far from the classification boundary in the feature space. Note that it ispossible to make an analogy between ProtoPNet and support vector machine (SVM)given that the classification from both methods relies on computing similaritywith a set of training points (i.e., trivial prototypes in ProtoPNet, andsupport vectors in SVM). However, while trivial prototypes are located far fromthe classification boundary, support vectors are located close to thisboundary, and we argue that this discrepancy with the well-established SVMtheory can result in ProtoPNet models with inferior classification accuracy. Inthis paper, we aim to improve the classification of ProtoPNet with a new methodto learn support prototypes that lie near the classification boundary in thefeature space, as suggested by the SVM theory. In addition, we target theimprovement of classification results with a new model, named ST-ProtoPNet,which exploits our support prototypes and the trivial prototypes to providemore effective classification. Experimental results on CUB-200-2011, StanfordCars, and Stanford Dogs datasets demonstrate that ST-ProtoPNet achievesstate-of-the-art classification accuracy and interpretability results. We alsoshow that the proposed support prototypes tend to be better localised in theobject of interest rather than in the background region.                                                                                                                                                                                                                                                                                                                     | http://arxiv.org/abs/2301.04011v3 |
| 658 | Learning by Sorting: Self-supervised Learning with Group Ordering Constraints                                                                        | Nina Shvetsova                 | 2023-01-05     | cs.CV                                   | Contrastive learning has become an important tool in learning representationsfrom unlabeled data mainly relying on the idea of minimizing distance betweenpositive data pairs, e.g., views from the same images, and maximizing distancebetween negative data pairs, e.g., views from different images. This paperproposes a new variation of the contrastive learning objective, Group OrderingConstraints (GroCo), that leverages the idea of sorting the distances ofpositive and negative pairs and computing the respective loss based on how manypositive pairs have a larger distance than the negative pairs, and thus are notordered correctly. To this end, the GroCo loss is based on differentiablesorting networks, which enable training with sorting supervision by matching adifferentiable permutation matrix, which is produced by sorting a given set ofscores, to a respective ground truth permutation matrix. Applying this idea togroupwise pre-ordered inputs of multiple positive and negative pairs allowsintroducing the GroCo loss with implicit emphasis on strong positives andnegatives, leading to better optimization of the local neighborhood. Weevaluate the proposed formulation on various self-supervised learningbenchmarks and show that it not only leads to improved results compared tovanilla contrastive learning but also shows competitive performance tocomparable methods in linear probing and outperforms current methods in k-NNperformance.                                                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2301.02009v2 |
| 659 | Event Camera Data Pre-training                                                                                                                       | Yan Yang                       | 2023-01-05     | cs.CV                                   | This paper proposes a pre-trained neural network for handling event cameradata. Our model is a self-supervised learning framework, and uses paired eventcamera data and natural RGB images for training.  Our method contains three modules connected in a sequence: i) a family ofevent data augmentations, generating meaningful event images forself-supervised training; ii) a conditional masking strategy to sampleinformative event patches from event images, encouraging our model to capturethe spatial layout of a scene and accelerating training; iii) a contrastivelearning approach, enforcing the similarity of embeddings between matchingevent images, and between paired event and RGB images. An embedding projectionloss is proposed to avoid the model collapse when enforcing the event imageembedding similarities. A probability distribution alignment loss is proposedto encourage the event image to be consistent with its paired RGB image in thefeature space.  Transfer learning performance on downstream tasks shows the superiority ofour method over state-of-the-art methods. For example, we achieve top-1accuracy at 64.83% on the N-ImageNet dataset.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | http://arxiv.org/abs/2301.01928v3 |
| 660 | Betrayed by Captions: Joint Caption Grounding and Generation for Open Vocabulary Instance Segmentation                                               | Jianzong Wu                    | 2023-01-02     | cs.CV                                   | In this work, we focus on open vocabulary instance segmentation to expand asegmentation model to classify and segment instance-level novel categories.Previous approaches have relied on massive caption datasets and complexpipelines to establish one-to-one mappings between image regions and words incaptions. However, such methods build noisy supervision by matching non-visiblewords to image regions, such as adjectives and verbs. Meanwhile, context wordsare also important for inferring the existence of novel objects as they showhigh inter-correlations with novel categories. To overcome these limitations,we devise a joint \textbf{Caption Grounding and Generation (CGG)} framework,which incorporates a novel grounding loss that only focuses on matching objectnouns to improve learning efficiency. We also introduce a caption generationhead that enables additional supervision and contextual modeling as acomplementation to the grounding loss. Our analysis and results demonstratethat grounding and generation components complement each other, significantlyenhancing the segmentation performance for novel classes. Experiments on theCOCO dataset with two settings: Open Vocabulary Instance Segmentation (OVIS)and Open Set Panoptic Segmentation (OSPS) demonstrate the superiority of theCGG. Specifically, CGG achieves a substantial improvement of 6.8% mAP for novelclasses without extra data on the OVIS task and 15% PQ improvements for novelclasses on the OSPS benchmark.                                                                                                                                                                                                                                                                                                                                                                                                                             | http://arxiv.org/abs/2301.00805v2 |
| 661 | CLIP-Driven Universal Model for Organ Segmentation and Tumor Detection                                                                               | Jie Liu                        | 2023-01-02     | eess.IV, cs.CV, cs.LG                   | An increasing number of public datasets have shown a marked impact onautomated organ segmentation and tumor detection. However, due to the smallsize and partially labeled problem of each dataset, as well as a limitedinvestigation of diverse types of tumors, the resulting models are oftenlimited to segmenting specific organs/tumors and ignore the semantics ofanatomical structures, nor can they be extended to novel domains. To addressthese issues, we propose the CLIP-Driven Universal Model, which incorporatestext embedding learned from Contrastive Language-Image Pre-training (CLIP) tosegmentation models. This CLIP-based label encoding captures anatomicalrelationships, enabling the model to learn a structured feature embedding andsegment 25 organs and 6 types of tumors. The proposed model is developed froman assembly of 14 datasets, using a total of 3,410 CT scans for training andthen evaluated on 6,162 external CT scans from 3 additional datasets. We rankfirst on the Medical Segmentation Decathlon (MSD) public leaderboard andachieve state-of-the-art results on Beyond The Cranial Vault (BTCV).Additionally, the Universal Model is computationally more efficient (6x faster)compared with dataset-specific models, generalized better to CT scans fromvarying sites, and shows stronger transfer learning performance on novel tasks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   | http://arxiv.org/abs/2301.00785v5 |